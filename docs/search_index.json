[
["index.html", "On the Last Day of Your Life, Don’t Forget to Die", " On the Last Day of Your Life, Don’t Forget to Die Some practical thoughts for those wanting to go further with their academic research Michael Clark 2018-01-07 "],
["preface.html", "Preface Outline Goal", " Preface I see you. I recognize you. I acknowledge your existence. Let’s talk… You want to do quality research using sound statistical and other practices, but your training is minimal, and it might have even been a couple years since you last did anything substantial. And while you want to do good analysis, any decent statistical effort will require a notable amount of programming, and you may have little to no programming experience. Furthermore, you may have been taught by people with no formal statistical, programming, data or other relevant training, which only compounds the problem. So you don’t know what you’re doing, but you don’t want anyone to notice. In fact, you’d like to bluff your way to the point of giving presentations, publishing papers etc., all with the polish of a master. It’s okay. Most of the others are in the same boat. These are some thoughts, particularly for graduate students, but really any researcher in applied disciplines, on what they can do regarding statistical and programming basics to make their research efforts more efficient, more in-depth, and in the end, hopefully more fun too. The suggestions will have a strong eye toward balance. I realize that if you wanted to be a statistician or computer programmer, you wouldn’t be getting a degree in sociology, chemistry, ecology or whatever. Note that the following is oriented toward the vast majority of those who do research that will involve some form of statistical analysis. While I know for a fact that some of it could even help those doing other kinds of research, it will have less to say to them. Outline General First we begin with some general things that can apply to statistics, programming, and possibly well beyond. Concepts will be introduced like compartmentalization, reproducibility, and so forth, many of which will not require any particular expertise, only practice. Even so, their application can go a long way toward making research and statistical computing easier. Basic analysis and programming After that we’ll cover things that I think anyone doing research could benefit from, both statistically and in terms of programming. For example, this will include iterative programming, coding style, and problems with common statistical practice. Some might seem a little specific, but they generally won’t take much effort to implement. Intermediate analysis and programming The last part will regard some suggestions for those that get a taste for analysis and computing and want to go further. This section will be for those that want to do even more advanced programming and statistics, as well as more to present themselves and their work to others. Goal The hope here is to basically inform about those things that you won’t be told about or spend much time on in usual training circumstances. You may get a sense of some of it, but, for example, probably most applied statistics courses don’t even cover visualization, basic programming principles (i.e. beyond getting a statistical result), or how to organize one’s approach. This means that many dislike the enterprise, because so much time is wasted doing tedious things no one would like to do, instead of exploring ideas through data. Incorporating just a few of these ideas and practices in your workflow can go a long way to make statistical computing a lot more fun. "],
["part-i-universal-truths-cycles.html", "Part I: Universal Truths &amp; Cycles Face the facts and accept them Get Organized Open and free is the way to be Compartmentalization Reproducibility Get a book Complexify later Much of your brain is devoted to visual processing Embrace uncertainty but avoid relativism Flexibility Back up Know the basics of how computers/OS work Your work isn’t that important Magic Summary of Part I", " Part I: Universal Truths &amp; Cycles This first part will cover some basic things that can apply to statistics, programming, and possibly well beyond. None of the following really requires any specific background or experience to understand and begin implementing. Face the facts and accept them You have chosen to get, or have obtained, an advanced degree in a discipline that requires scientific output. Among many things, science involves putting a theory into a mathematical form that allows for some form of analytical chicanery to be applied, in an effort to weigh evidence, in the form of data, for or against that theory. This can be very simple, and it doesn’t require causal formulation. For example, the following: \\[Y = f(X) + \\epsilon\\] may just mean something like life satisfaction (\\(Y\\)) is related to some set of other things like health, education, income, (i.e. all the stuff in \\(X\\)), but not exactly (\\(\\epsilon\\)). How you choose to investigate your theories will depend on your discipline, for better or worse. Practically speaking, it means you will have to do things like statistical analysis, collect data, do some programming, perhaps engage in a lot of writing, etc. If this isn’t for you, that’s perfectly fine, it means you’re normal, or at least among the vast majority of humanity. And if it isn’t your bag, it would be nice if you didn’t waste the time and resources of faculty, students, and others who do want to engage in those activities, not to mention taxpayer money that’s used to fund such pursuits. Seriously, don’t be a jerk. One more thing to plan on - everything, I mean everything, will take longer than you expect. That may even include your time-in-training. You may, for example, plan on 4-5 years for a Ph.D., or, as faculty, 1 year to complete data collection and submit an article, but, as the old saying goes, shit happens. You’ll have to learn to deal with it. Get Organized The movies would like you to believe that science is just a series of eureka moments had by oddball geniuses who can’t seem to do anything right most of the time, but will intuit the perfect solution to any problem with just a moment’s reflection. As anyone who’s actually engaged in scientific research knows, this is a bunch of BS. On the contrary, if you’re doing good science it’s going to take a lot of hard work. However, a few simple steps can be taken that will avoid a lot of wasted time. There are a few things you can control in your research efforts, and being organized is one of them, so do it. For starters, a very simple way to get organized is to create a project folder. Many IDEs (e.g. Spyder for Python, RStudio for R) seem to do this now, which makes it easy. But for any project you have, make a project folder that will be the home for what you’re working on, e.g. a specific article or dissertation. This will serve as the working directory for that project1. In addition, it should have some organization to it, e.g. sub-folders for data, code, documents etc. There are even R packages that do this for you, but I advise against using someone else’s organization, as it likely will not work for you2. Note that your first attempt at this sort of organization will also probably not work, but that’s okay, you’ll eventually find one that does the trick for most of your projects. As an example, I have a batch script that will create something similar to the following folder structure: Project/ Data_Files/ raw/ other/ Code/ Analysis/ Data_Preparation/ Descriptives/ Functions/ Related_Docs/ Misc_Notes_Etc/ Very simple and straightforward. Also, note the underscores rather than spaces. We’ll talk about naming conventions later, but the gist is that you never want to name any file or folder with a space. People tell me it’s 2018 as I write this, but somehow spaces will still screw things up sometimes, or simply must be converted to something else at some point. On the programming side, the other nice thing about projects is that whenever you open or switch to a project, all your scripts will be there just as you left them. This allows you to pick things up right where you left off. Little efficiencies gained just by being even slightly more organized can save weeks over the course of say, a dissertation. Other things we will cover, e.g. doing things in a reproducible fashion, moving from simpler things to more complex, compartmentalization, version control, etc. are all little steps in organization that go a long way towards efficiency. Open and free is the way to be The researchers in the academic world who have kept up with the times generally are not using things like SPSS, MS Excel and MS Word to do their work. They are also publishing in open outlets that allow for rapid dissemination and easy access to their work, and some are even boycotting traditional outlets altogether. Even if their primary tools are proprietary, such tools are using something open and/or free under the hood. In fact, many of the most common tools of the trade are open source and/or freely available, e.g. C++, Python, R, \\(\\LaTeX\\), Markdown, etc. At least one reason for this is because open source means the code is available to be inspected, the licenses for the code, if they apply, are far less restrictive3, and often anyone could potentially contribute to the development process. This in turn produces more rapid development and extension. Furthermore, not everywhere in the world where people might want to do science is financially well off enough to throw thousands of dollars at software that doesn’t do anything special. This is why R, Python and similar rapidly gained and took over ground after years of dominance by SAS, SPSS, and similar4. On the proprietary side, take for example, Mplus, a widely used software for structural equation modeling. At the time I started writing this, they just released version 8, five years after version 7 and a year and a half since the last point release (7.4). This in and of itself is no big deal, given that the new features are few. However, one of the things they mentioned were ‘corrections to minor problems that have been found’. They don’t mention what the bugs were, but evidently they were allowed to exist at least for over a year, and we don’t know if they existed prior to that as well. Other statistical software, like SPSS, have a ‘full release’ every year, but one would be hard pressed to know what’s changed over the past 10. Nonetheless, they expect you and or your organization to pay for it each and every year. This sort of approach to software development in science is not acceptable in this day and age, especially when comparable or even better tools are available to do the same task5. Before I go on a rant, practically speaking, the best statistical, data science, and related tools currently are found with R and Python. For writing, it’s far easier to use Rmarkdown for scientific publishing (even if you use Python, but feel free to try Jupyter), and if you really must have MS Word, it will generate a .docx document for you. Otherwise, \\(\\LaTeX\\) is still quite commonly used and will produce a much better document than MS Word, especially if using mathematical formulas, tables, graphics etc. I’m not saying you have to go as far as Richard Stallman, the guy pictured, who only uses free, as in both beer and speech, software. But you have already long used and benefited from open source software, so all I’m saying is to continue doing so in your own research. Compartmentalization Now for a lesson from computer science, though more generally, it can be taken simply as divide and conquer. Break your activities into smaller, more manageable chunks. This makes it easy to pick up where you left off, and get things accomplished even if you don’t have a lot of time. It has the added benefit that when things go wrong, it’s much easier to isolate the problem, and probably keep working on the other aspects of the project that are still functional. Analysis For analysis, break activities into something like the following: Initial import and inspection: This is just making sure you’re reading the data in correctly, and that a quick glance to see that it looks like it should. Maybe you’ll do some filtering to get rid of things you know you’ll not need. Data processing: This is all the stuff you should be doing before you ever look at even a mean or standard deviation. Calculating composites, data debugging, merging, etc. would be included. Descriptive analysis: This includes just getting a feel for the data via simple summaries, but you will likely find some data bugs you missed previously. Analysis: This will include things like trimming the data just to those variables necessary for the models, as well as running the models themselves. You may yet still find data bugs at this stage. Visualization: This can be quite an undertaking itself, even to just get one visualization just right. Note that everything after the first step may not be a sequential process, and that’s okay. Programming Your code can generally follow the plan you had for analysis, but it can easily be even further compartmentalized. For example, scripts can be specific to one goal, however trivial. In the analysis stage, you might be investigating several outcomes of interest. In that case, have a script specific to each one. One might think that having a lot of scripts would be a problem, but generally, and if you’re just starting out, the converse will be far more of an issue. You can always create a master file that will call the scripts in a desired order. I have a not-so-hard-and-fast rule of thumb that if I get to 100 lines, I should have some justification for not breaking some of it out into a more manageable chunk6. This will also apply to functions for those that get further into programming. Having one massive function that does everything is a PITA to debug, or have users inspect. Having smaller functions allows one to isolate problems and make for more rapid debugging. Other The idea of compartmentalization applies to other endeavors as well. For example, when writing your paper, give each section its own document (child) to be called by the parent document or at compilation. This is easily done with tools like \\(\\LaTeX\\) and Rmarkdown. As a specific example, my document introducing Bayesian analysis has separate a separate Rmarkdown (.Rmd) file for each major section. Reproducibility Assume that at some point you’ll have to leave your analysis, code, document etc. behind. Perhaps another project comes to forefront, maybe it’s just time off, maybe it’s a health issue, who knows? But think about when you come back and have to complete your mission. Will you know what you’ve done? This is only the beginning, but if you can’t reproduce your own work it’s certain no one else will. As a beginning framework, could someone take the data and your code and get the same results? You’d be surprised at how much published work would fail what should be a simple test. Some concepts you’ll want some sense of include the following: Reproducible research Replicability Literate statistical programming Dynamic Report Generation Article pre-registration and others. The differences in terminology regard the focus: analysis results, code, data archiving, report generation and so forth. Exact replication is the one thing that, no matter how hard we might try, simply can’t happen. Different times, different people involved, etc. all work against that being possible. However, we can get close, or define it in some acceptable way that can be done. Practically speaking, just thinking about this topic will likely make you take steps to do better than what you might have otherwise. But one can start by just thinking about yourself, could you do it, again? Get a book If you decide on a particular analytical route for your thesis, dissertation, or paper, get a book on the technique7, or at least one that covers the topic extensively. Buy it, check it out, whatever you need to, but have a handy reference for the thing you’re about to spend an inordinate time on, possibly with not having ever dealt with it before. It just makes sense. Even if it’s a little more technical, remember that you’re about to spend a lot of time in this area, and that’s the sort of thing you’ll need. The one thing you don’t want to do is wing it based on what other applied researchers in your field are doing in the articles you’re reading. They may very well know what they’re doing, but you may not have the skill set to distinguish that. You can train a dog to bark when p &lt; .05. Complexify later A very old approach that has withstood the test of time in many domains- start with the simple, and build in complexity as you go8. Even then, elegance and parsimony should be the goal, not complexity. Analysis This really comes into play with analysis- always start with simpler models. I personally use some of the simplest models for debugging purposes, because some data issues are difficult to discover until the implementation of analysis. But even before modeling, you should be doing basic data checks and descriptives. The following provides an example where the final model is a mixed model with some other complexities added. Basic data descriptives. Know the range for numeric variables, and general frequency distribution for categorical, of all the variables going into your model. Mean center or scale numeric variables. Possibly collapse very infrequent categories (if there is an issue). Run a regression. Check some diagnostics. Run a standard mixed model with just random intercepts. Run a standard mixed model with random slopes added. Decide whether the complexity is warranted. Add interaction/nonlinear terms. Taking the above approach allows one to feel more comfortable when they get to the more complex stage, and confident that the effects they see are actually reflective of what the data is attempting to express. Programming In programming, this probably is best illustrated when writing functions. Your first iteration of a function probably will only have a one or two arguments. However, once you lay the (solid) foundation, you can build upon it later. This suggestion has roots in the Knuth quote: Premature optimization is the root of all evil It’s okay to write some code that simply works, and add details and generalizations later. You’ll often be on some completely arbitrary time crunch, so the main thing is to write good code at the beginning, then work on making it more optimal or handle more complex situations later. Much of your brain is devoted to visual processing First, assume others’ brains are not so different from yours, then take advantage of it. One of my pet peeves is when I hear the phrase ‘I’m a visual learner.’ Really? So you have a brain? Everyone with working eyeballs is a visual learner. Some can also learn via other means, e.g. text and sound, but that is secondary. The gist is that you should always be thinking how to display the products of your research visually, possibly even interactively. Unfortunately you will find that some journals are still acting as if it’s 1950, and that people only peruse articles via print. If you can believe it, they actually will charge extra for color9. Journals that still have the black and white, print first mindset in this day and age should be avoided at all costs, unless you don’t like people reading your research. With regard to interesting visualization, too many people I consult/work with have literally told me the equivalent of “All these Ph.D.s and M.D.s that read my article or see my presentation won’t be able to understand it.” Utterly ridiculous. Maybe you will need to work on explaining the visualization to make it easier to digest, but I can assure you that folks in academia, as well as those beyond, can handle it. If they were that dumb, which I refuse to believe, why exactly would you care if they even read your article? Are you anxious to be cited by nincompoops? Some other things to note: Visualization may be the most fun you’ll have with your document, spend some time with it. Color matters, and if you’re just picking ones you like, you’re doing it wrong. Bar plots of any kind have limited utility at best. Pie charts and some others are just a form of bar chart. Most cartograms are about as useful as bar plots. If your map could be interpreted as reflecting little more than population, you should try something else, regardless of how cool it looks. Infographics have little if any role to play in an academic setting. Do not waste people’s time. Try to convert your categorical information to quantitative so that you’ll have more options to play with. Use data visualizations from 1936 and the 1800s as inspiration10: Embrace uncertainty but avoid relativism A lot of people living today think their opinion is valid merely by having it. This is not true, no matter what their parents told them, nor what all politicians seem to believe. You do not fly to a destination thinking it would be just as viable to flap your arms or click your heels together. You don’t cross the street thinking that the theory that ‘all cars will stop for you, just for you’ is a viable one. Try telling your neighbor that the clear sky at noon is chartreuse. I’m sure they’ll look at you funny. I could go on, but even a moment of reflection suggests that every ounce of human behavior goes against the idea that all ideas are equally valid. Evidence not only matters, it is what we live by as a species, and what got us this far. There are definitely areas where it is difficult to tell whether one theory is better than another, especially in interesting and new areas of research, but this does not mean the evidence is rubbish, nor that it may not be used to suggest one path over another. Evidence often may even lean very strongly in one particular direction, so much that one would be hard-pressed to even try to suggest an alternative. It may also be found decades later to be utterly false. That’s okay too11. That’s just the way things are, and anyone that thinks otherwise will have a very difficult time in life. Pretending things are certain just simply doesn’t make them so. Your theory, whatever you’re researching, resides somewhere between completely false to mostly correct given current knowledge. Regardless, it is incomplete, and possibly grossly inaccurate in places, and without the progress of science we’ll never know. This is why, despite journal reviewers’ comments, null results are not only useful, they are interesting in and of themselves, and as vital as any ‘statistically significant’ result. We need to know that a treatment doesn’t work, that one social group is not terribly different than another on some matter, that temperatures in the ‘x’ range will result in a cascading series of events ultimately leading to the space craft tearing itself apart before it breaks atmosphere. Things like interval estimates and prediction error give us a sense of how much work we have left to do. Rely on them heavily. The goal of research is not to give a definitive answer, but to reduce uncertainty, and that reduction is usually from a whole lot to very slightly less so. If you’re not okay with the fuzzy, science is probably not for you. Flexibility Empty your mind, be formless. Shapeless, like water. If you put water into a cup, it becomes the cup. You put water into a bottle and it becomes the bottle. You put it in a teapot, it becomes the teapot. Now, water can flow… or it can crash. Be water, my friend. ~ Bruce Lee The author of the one-inch-punch algorithm should not be taken lightly for obvious reasons12. Things happen, and you will need to be able to roll with the punches of data collection, analysis, collaboration, life etc. Assume that at some point, something in the research process will not go your way, and deal with it as best you can. There is literally nothing else you can do. Back up Everything. You should never lose more than a day’s work under the worst of circumstances, and if you’re doing things even with the bare modicum of responsibility, you probably won’t lose more than a few minutes13. Thankfully tools like Box, Dropbox, Google’s Backup etc., though with their drawbacks, make it easy to always have that backup. Sometimes, you may even want to go to a previous state for replicability purposes, or to fix a bug that found it’s way through your system, and there are tools available to help with this as well, such as Git. Know the basics of how computers/OS work If you’re wondering about what specifically I might be referring to in this section, it may mean you don’t know the basics of how computers work. As a result, you may be wasting a tremendous amount of your own and very likely others’ time. Note that I’m not even remotely suggesting things that would be regarded as ‘technical’ by anyone besides a news reporter. I’ll provide a single example. Probably the biggest time sink I experience in consulting is a Mac user looking for a file. Thankfully, your primary computer doesn’t work like your phone (yet), so don’t expect it to, or treat it as if it did (e.g. leaving dozens of applications running eating up all your resources). If you’re doing analysis on your machine you should know how many cores and how much RAM you have, and why that matters. You should also be able to check what resources are available at a given time. However, your computer may not have enough hardware to do sophisticated analysis with the data you are interested in, and so you may have have to work on other machines. While this is getting much easier, and will be trivial in time, at this stage of things you may have to gain some technical know-how to use, for example, a computing cluster environment of some kind. At some point everything will be cloud-based, and applied researchers can remain completely ignorant of basic aspects of computing, but we’re not quite there yet. Beyond hardware, you should know the basic structure of your operating system, what are and how to change defaults (e.g. where files are downloaded), some simple shortcuts to help navigation, how to install programs efficiently etc. I have been in workshops where people did not know the difference between an installer and the actual program it was meant to install. Just note that knowing this does not qualify you for being ‘tech savvy’. Your work isn’t that important To put some things in perspective, say an article you publish is cited by 1000 unique researchers, which would be a hell of a lot in practically any discipline. That suggests that at least 1e-07% of the population of the world knows about it. Congrats. A couple billion more and you’ll be on par with a YouTube video of a cat from three months ago that everyone’s already forgotten about. Don’t forget to take some time every once in a while to do something else. See some live music, get some exercise, go to a museum, get drunk, whatever you need to do to not think about this esoteric part of intellectual endeavor that in the end \\(99.\\bar{9}\\%\\) of the world couldn’t care less about. The world will keep spinning I assure you. By the same token, don’t be a jerk to your collaborators or the people you need to do your work (e.g. IT, administrators, etc.). ‘First author’ is not equivalent to ‘alone’. Magic Research takes a lot of effort. It does not happen by magic. If some aspect about your work seems magical to you, you can be sure that a reviewer, other researcher, conference attendee will ask you about it. For applied researchers doing quantitative work, this means being able to say more than ‘I clicked and this happened!’, or throwing the grad student under the bus when pressed on something you don’t know. Analysis Assuming the data has been prepared, running a Bayesian quantile regression with a complicated random effects structure takes one line of code in R, same as a t-test. For most applied researchers, the former would be magic, and that’s okay, if they don’t intend to use it. Despite what many seem to think though, you simply cannot correctly interpret a magical result. As mentioned previously, you can train a dog to bark when p &lt; .05. Likewise researchers can mimic language they see in journal articles, sufficiently passing off what seems to be knowledge of the techniques employed. This is part of what has contributed to numerous issues, retractions, etc. currently on display in various disciplines. The time will come when you’ll need to move beyond the basic stats you were formally taught, and you’ll have to teach yourself and/or get some help to learn these new techniques, or they will remain magic to you. However, learning new things is part of the definition of being an academic (and human for that matter)14. You don’t necessarily have to be an expert, but saying something is statistically significant is not enough to demonstrate knowledge of a technique. Be prepared to do some work. Programming In programming, sharing and reusing code is simply the way things are done. And that’s a good thing. However, if you’re copying and pasting others’ work and don’t know what some parts of it are doing, or why it’s written one way and not another, then that part of the code is indistinguishable from magic to you. It may be that in order for that part to work properly, your data will need to be prepped a certain way. It may also be that the code doesn’t apply to your situation, and using it will produce problematic results. Magical code is also often behind errors that are uninterpretable to the researcher. As elsewhere, know what’s going on in your research, and you’ll avoid any number of problems. Summary of Part I Suck it up, dive in, and don’t waste your breath. The first line of a programming script shouldn’t have to be to set the working directory.↩ Most of the folks I’ve come across that suggest ways to organize your directory typically call for a structure that would be notable overkill for applied researchers. A lot of people could probably get away with just a couple folders.↩ Typically the restrictions are things like, cite the authors, and don’t try to sell it.↩ It’s also the reason SAS and SPSS have integration with R and Python, and finally started offering free student versions. Too little, way too late.↩ I honestly can’t understand why almost all of these proprietary statistical packages still exist. Some I know are still accepting payments even though development ended years ago. Others served a need at one time that’s long been met and surpassed by open source tools. Some practically have zero presence outside of a particular discipline, but anyone brought up in that discipline may be unaware of that fact.↩ This is not a rule that would apply e.g. in software development, but we’re not talking about that… yet.↩ And you can judge them by their cover. One of the most useful books out there had some of the most sage advice ever written on its cover: Don’t Panic.↩ Complexify is not a word, but I like it better than complicate. Complexity may be good, not sure about complication.↩ They probably also have page limit restrictions. Who will think of the printers?!!↩ Please don’t actually do the Florence Nightengale type of plot, commonly referred to as wind rose plots. While hers in particular is most beautiful, in the end they are glorified pie charts, and do a poor job of conveying what otherwise would be simple information to digest.↩ A useful text to peruse is Laudan (2012).↩ This document is written in alternating voices of the narrator of the Hitchhiker’s Guide to the Galaxy and Bruce Lee in that clip.↩ I personally don’t adhere to this as strongly as you should because I don’t have deadlines/graduation looming, and I am fond of the Buddhist notion of impermanence. I won’t go so far as blowing my awesome sand mandala away as soon as I’ve created it, but I often don’t mind starting over.↩ I have actually heard the words come out of the mouth of one person as follows: “I’m faculty, I don’t want to learn anything new!”. This person also never learned that he was a joke that no one, not even in his own department, took seriously, though many explicit hints had been provided that was the case. There are people in academia that have a goal of getting tenure and doing the bare minimum from there on out. They should be avoided at all costs.↩ "],
["part-ii-perfect-sound-forever.html", "Part II: Perfect Sound Forever Analytical Programming Other Summary of Part II", " Part II: Perfect Sound Forever The goal here is not expertise, but primarily conceptual awareness and understanding. For example, you don’t have to know much about machine learning approaches other than why you might use them. You can then learn more as needed when the time comes, if it even does. For programming, it literally could be two lines of code to import your data and run a statistical model. But the data is never ready, and data processing will likely be where you spend the vast majority of your time. However, knowing even a few things about basic programming will help you spend a tremendous amount less regarding what you have to do, but probably more of things you want to do. Some motivation. Here are some things I’ve seen: A social science grad student first learning R/programming at the beginning of the semester, that within a few months was a keyboard shortcut whiz, and was taking some Python workshops to boot. Another social science student using efficient programming practices to analyze big data on a high performance computing cluster. Architects, English Lit faculty, Staff librarians. I have seen them all do basic programming and statistics to even some advanced stuff. I have seen 50+ years old SPSS folks get into machine learning. It obviously isn’t easy, but anyone can do it. You’ll likely be surprised at just what eventually does becomes easy, but be prepared for regular stumbles along the way, no matter what level you’re at. As you improve, you’ll just be at another level where the same amount of stuff will trip you up, only more advanced. Case in point, here is the most common phrase I utter when programming: ~ You’ve got to be @%*^!ing kidding me!! Once you get good at some things, you’ll still hit roadblocks, be surprised, or just make stupid mistakes. It’s just part of the journey, and mostly just means you’re doing something interesting. If it helps, think of it as an RPG where you’re leveling up, only, you’ll probably be the only one to tell you that you’ve leveled up15. Analytical Explanation and Prediction: a Starting Point A narrative of startling interest!! One way to look at the analytical endeavor is in terms of two goals, not mutually exclusive. On the one hand we are interested in theory confirmation and comparison. Theory suggests certain constructs have relations to other constructs We set up mathematical models that convey the theory in a testable form, and collect data corresponding to those variables. Via a a particular framework, e.g. null hypothesis testing or Bayesian, we can say something about the statistical notability of those relationships. The ultimate goal is to tell a story about the results and their relation to the theory, possibly suggesting exceptions and modifications. In this approach, explanation is the focus. On the other side, we need something that is practically viable. The goal is not so much to tell a story as it is to get the best possible prediction from our models. Techniques may be used that will tell us little about which predictors are best, why some predictors work better, or what is even the primary feature of the data being learned. However, not only does the result work well, such approaches almost always outperform the far more explanatory ones in practice. To make things a little more concrete, we can consider linear regression vs. deep learning. With the former we can obtain highly interpretable coefficients, statistical probabilities, interval estimates, etc. It is extremely easy to tell a story with the results. With deep learning on the other hand, it’s difficult to tell what’s being learned, or what features are being latched on to. However, assuming it’s applied appropriately16, it works extremely well! Most applied disciplines seem to focus exclusively on explanation in their statistical education. You might think this is not too big of an issue, but to see the consequences of this, one only has to look at the current ‘replication crisis’17. You’ve now had nearly a century of results interpreted solely in terms of statistical significance in the null hypothesis testing framework, with typically little to no focus on whether the damn model even works. As a specific example, you can have statistically significant predictors in a linear regression and an adjusted R2, which would tell you how much variance in the outcome you can explain with the predictors, of negative value! The following demonstrates this. I construct completely random data, i.e. none of the \\(X\\) has any relation to \\(y\\), and use it in the model. N = 250 X = matrix(rnorm(N*10), ncol=10) y = rnorm(N) ## summary(lm(y ~ ., data.frame(X))) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.112 0.059 1.916 0.057 X1 0.056 0.062 0.896 0.371 X2 -0.019 0.058 -0.333 0.74 X3 0.021 0.063 0.334 0.739 X4 0.028 0.055 0.503 0.615 X5 0.016 0.06 0.274 0.785 X6 -0.02 0.06 -0.33 0.742 X7 0.048 0.055 0.872 0.384 X8 0.054 0.057 0.957 0.34 X9 0.015 0.06 0.256 0.798 X10 0.125 0.06 2.095 0.037 Fitting linear model: y ~ . Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 250 0.9158 0.02702 -0.01369 One term has a p-value of 0.037, but we know this is meaningless. Yet you will find entire papers written about that one coefficient, with p-value slightly less than .0518, while in terms of prediction the model literally does worse than guessing the mean would with new data. Quite simply, this sort of practice has got to stop. The key idea is balance. For most applied disciplines explanation is not only required, but to be focused on. This is fine. Some people do research where collecting one observation might take a few days of effort. This too, is okay. However, complicated models with little data require something more than statistical significance- the results generally will have too much variability associated with them. But for any modeling endeavor, predictive validity is also important. You simply cannot focus on the statistical (significance) story while entirely ignoring the practical value of the model. A Word about Effect Size Some disciplines, e.g. psychology, try to get their adherents to focus on effect size. Reviewers regularly request it, but also are regularly non-specific about the metric, or if they do mention it, make clear they have no idea what they’re talking about. For example, one of the more popular effect sizes is Cohen’s d, which is a standardized difference in two group means (e.g. control vs. treatment). It’s applicable in cases where the primary goal is a single group comparison, e.g. via a t-test. However, if you are doing a t-test as your primary model and haven’t gone back in time, then I can assure you no one is going to be citing your work regardless of your effect size. Here are just some of the issues. Without a corresponding interval estimate the effect size is of little value. That’s right, the effect size has associated uncertainty with it, just like everything else. As soon as you get into any complication, e.g. repeated measures, interactions, etc., there is little standard or possibly even a viable way to define it. Small data and large effects mean nothing other than that you need more data. It is definitely not the case that the effect is ‘more real’ because you happened to see it with such a small sample. In fact, your first move after seeing a large effect in a small sample should probably be to go back to the data processing stage to make sure no errors were introduced. Another common ‘effect size’ is the standardized regression coefficient. You don’t see this much outside of disciplines where SPSS is used a lot. However, even in the only setting where it’s easily calculated, it still doesn’t make effects of categorical vs. effects of numeric predictors easily comparable. And as soon as you move beyond the standard linear model, which is where most models should be these days, it’s pretty much not defined. For example, what’s the effect size here? If you’re dealing with interesting data and modeling, there is little chance there is an easily identified ‘effect size’. Instead, you’ll need to know your measures well and think hard about what you’re seeing. If you know the target variable well, effects in terms of predicted values under a variety of data settings will be far more enlightening. The Standard Linear Model Everything starts with the standard linear regression model. To paraphrase Shalizi (2017), it’s relatively simple19, it can do a decent enough job in a lot of situations, and it’s a standard to the point it’s essentially a tool of communication. One of the more important aspects of it is that it serves as the foundation for everything else. Simply put, you need to get to a point where practically nothing about what you get from it in standard settings should be a mystery to you. Coefficients, standard errors, residuals, fitted values, prediction, how categorical predictor variables are used, residual standard error, interpreting interactions, model comparison. It’s also good to know what sorts of things are important (how you specify the model), and not so much (e.g. normality assumption). If you don’t understand standard regression, you’ll be hard pressed to understand little else methodologically. One additional benefit is that knowing it will take care of the special cases like ANOVA and t-tests, so you won’t have to spend time learning those unless you just want to. Classification The second most commonly used model after the standard linear model is probably logistic regression, where one predicts a binary target (e.g. yes-no). Standard logistic regression, like the standard linear model, is a special case of the generalized linear model. This gets you beyond ordinary least squares methods and into maximum likelihood estimation explicitly. We’ll return to this later. In addition to standard fare like coefficients and their standard errors, you’ll get two types of predictions, probability of being in one of the categories, or the actual classification of being in one category or the other. One can look at many metrics of performance with logistic regression, for example, accuracy, sensitivity, and specificity. Most only look at accuracy, which is not a good way to do things in many scenarios. Actually, many don’t even do that. You’ll often see papers going on and on about the statistical significance of predictors even though their actual prediction is no better than guessing the most common category. Again, there is explanation and then there is prediction. Even if you don’t think you’ll use it for your current work you’ll come across papers that use it, and you will use it at some point if you do enough statistical modeling. Furthermore, getting used to logistic regression can serve as a first step to other GLM models (e.g. poisson regression), and other categorical models, e.g. ordinal and multinomial. If you do use it, consider interpreting it in terms of actual predictions at key values of the predictors20 rather than statistical significance and odds ratios. Not only are predicted probabilities easy to interpret, it’s pretty much the only way to interpret interactions and other nonlinear effects that might be included in the model. Unsupervised Methods Along with basic regression and logistic regression, you’ll need some conceptual understanding of what are typically referred to as unsupervised learning, dimension reduction, or matrix factorization. Here, we are exploring structure more than predicting outcomes (though we could do that too). Probably the most common among these are principal components analysis (PCA), factor analysis, and k-means cluster analysis. I would suggest just getting a handle on why you’d use them, and be prepared for needing to implement them or similar variants for your work, because they are used a lot. Some of these are taught in classes on multivariate analysis, which is an utterly useless name, but those classes also often include things like MANOVA, standard linear discriminant analysis, canonical correlation and others that have little place in modern methodology21. Don’t waste your time with those. Loss functions and maximum likelihood estimation In statistical modeling we are interested in estimated parameters. For example, these might be regression coefficients or variances. But how do we do it? We certainly don’t just guess, and despite what the press would have you believe about AI, we definitely don’t use magic. So where do the numbers come from? The answer is that we pick values that bring about the best result. But how do we define best? Well there is no one way to do it, and how you might do it depends on the situation. However, there are a couple common approaches you should understand conceptually very well. Let’s begin with ordinary least squares (OLS). If this is our optimizing approach, then we are trying to minimize the squared residuals, or errors, in prediction. For classification, it could simply be that we want to minimize mis-classification errors. Probably the most common method in standard statistical analysis goes the other way. Instead of minimizing errors in prediction, we seek parameters that maximize the likelihood of seeing the observed data. This method, maximum likelihood estimation, is the most common way we estimate parameters in statistical modeling, and has equal importance in the Bayesian framework too. Sometimes the estimates that might minimize the loss function may also be the maximum likelihood estimate. This is the case in standard linear regression, but it would not be in other settings. The primary point is, knowing the objective function for your analysis will help it not seem so mysterious. There are many optimization algorithms, and while you don’t need to know the details of them, do know that you may get errors because their default settings don’t work for your analytical situation. Many applied statistical courses will spend time almost exclusively on ordinary least squares techniques, for which they might not even talk about estimation at all, leaving the students to basically believe in magic when they do statistical analysis. Some will even report that they used maximum likelihood, but this is only because they know that is the default. For your own sake, don’t bluff this. With R or Python, you can literally write your own regression function via least squares or maximum likelihood in one or two lines of code. I strongly suggest you do this, even if only once, just to drive the point home. I can also say it is quite satisfiying when something you put together reproduces the output you expect from the statistical program. Problems with NHST I’m sick and tired of talking about the issues with null hypothesis testing, and I think the statistician types in general are. Its problems have been noted since it was first proposed, and people in both methodological and applied disciplines have been raising the issues for almost a century at this point. It’s annoying for me to even think about it, so if you want details, I won’t be rehashing them here, but you won’t have to look far on the web to find them. Assuming you’re even using the approach correctly (almost never the case in applied research), just about every other statistic that falls out of an analysis is more important than the p-value. Yet I have seen people torture data, change their theories, ignore important findings, etc., because of it. Researchers still doing this are not doing good analysis, because almost by definition it is work that likely won’t stand the test of time, probably even the second time. They are wasting their own and others time in general. Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test. J Berkson 1938 “The statistical folkways of a more primitive past continue to dominate the local scene.” Rozeboom 196022 “As a second example, consider significance tests. They are also widely overused and misused.” Cox 1977 “Are the effects of A and B different? They are always different, for some decimal place.” Tukey 1991 That first one is an article title, but otherwise those are some very old quotes, indicating just how long this has been a problem. NHST as a scientific paradigm, in which evidence is weighed to assess a theory’s real-world effectiveness, for a variety of reasons, doesn’t work in practice23. It is difficult to say whether it ever did. Many disciplines are possibly decades behind in their science both because of missed opportunities, and because of trudging through weak results that didn’t hold up, but were adhered to merely because of statistical significance. Statistical analysis will not provide you a hard answer or truth, and claiming something is ‘significant’ doesn’t make it so. The goal in statistical approaches to science is not to say whether a predictor is important or not, but rather whether the data is consistent with a given theory in general, and whether viable predictions can be made with a given theory. Go in with competing models, e.g. simpler ones vs. more complex, or theory-driven vs. purely exploratory. Spend time expressing your data visually. If you do, you’ll regularly have something interesting to talk about regardless of any p-value. Under and Overfitting With four parameters I can fit an elephant, and with five I can make him wiggle his trunk. ~ John Von Neumann Overfitting A common problem in standard statistical modeling and machine learning includes overfitting. Overfitting may be described in a couple ways, but the key idea is one of model comparison. For example, usually we want to compare simpler models vs. more complex models. While more complex models may better mimic what we see in nature, simpler models may perform nearly as well, allow for simpler interpretation, provide computational benefits etc. Ideally we want a parsimonious model, one that is complex, but not overly so. A more complex model can actually make no better or even worse predictions than a simpler model. In this case we are overfitting the data. Take for example the following plot of data. A sufficiently complex model might obtain the following fit. Great fit right? The R2 for the model is 0.88 with mean absolute error (MAE) of 0.9. However, if we predict it on new data from the same underlying data generation process, the MAE is 3, which is notably worse! Conversely, a simple regression with a quadratic term has comparable fit, with even a lower MAE (2.7). For reasons we’re about to see, I would not recommend the standard regression fit either, but the point of overfitting is illustrated. Another way to think about overfitting is in terms of the number of parameters you must estimate with the model vs. the number of data points you have to estimate them. Some machine learning approaches may have thousands of parameters for example, so you better have a lot of data for them to work well. A comon problem I see is with structural equation modeling, where people routinely fit complex models with several dozen to more than a hundred parameters with very little data. Because they also don’t compare models to simpler ones, or even typically discuss prediction at all, they don’t even speak to the notion of overfitting. Sometimes, when their model doesn’t fit as well as they’d like, they’ll even use purely data driven approaches to add even more parameters! For more on overfitting, see this discussion at Andrew Gelman’s blog. Underfitting A converse problem is perhaps just as ubiquitous or even more prevalent in some fields, and that is not incorporating sufficiently complex models, or underfitting. Take for example, the fact that standard linear regression, with no interactions or nonlinear effects, is still the most commonly employed model. It seems suspect that a straight line would be ‘good enough’ to explain most relationships in nature. In many cases the simplification may be worth it, but we are well beyond the time where that should be a so widely used as a default. Consider a generalized additive model (GAM) fit to the previous data. It initially won’t fit as well as the overparameterized model. But it certainly does better than a straight line would. When it comes to new data, it performs the best (MAE = 2.3). My own opinion is that a generalized additive model would be a better default model rather than the standard regression model. It penalizes complexity so that if a linear fit is better, that will likely be the result. In addition, interactions, spatial effects, random effects and myriad approaches are available to add to it. For more on these models, see my intros here and here. The issues around over- and underfitting can be somewhat subtle, but they are pervasive. In general, I think underfitting is perhaps more the problem in the things I see, and with penalized regression approaches, one can incorporate more complex models while still guarding against overfitting. Interpretation of more complex models may become more difficult, but also more rich, and more fun in my opinion. Variable transformations There are some good reasons to transform a variable. For example, mean centering (subtracting the mean), or additionally scaling numerical variables both makes a value of 0 practically meaningful (i.e. it is the mean), and can go a long way toward making the estimation process easier for many algorithms. Some techniques are not even useful unless the variables are similarly scaled. There are also times, especially in the case of categorical outcomes and predictors, when the need arises to collapse categories which have very few observations. Otherwise they can lead a model to fail. Finally, some, especially econometricians, prefer to speak in terms of elasticities, and so will take the log of a numeric variable to do so. That’s pretty much it as far as the reasons to transform a variable- easier estimation or theoretical interpretiability. Here are couple things not to do. If you have a numeric variable, there simply is no justification for discretizing it. You are just making it a less reliable and expressive measure, and there is no benefit to that. In addition, transforming a variable ‘due to normality’ is translated as ‘I don’t actually know what the normality assumption is’. Case in point, the following is a perfectly reasonable target variable that would meet the normality assumption for regression. Why? Because the assumption regards the residuals, not the observed target variable. The above is just what you’d expect if you were dealing with a very strong group effect. The following shows the density plot of the residuals and a test of normality (non-significant means okay). Shapiro-Wilk normality test: residuals(lm(y ~ x)) Test statistic P value 0.9977 0.7342 I’ve seen this assumption so confused that some think it applies to the predictors as well! Some have also tried to apply it to ordinal variables. A moment’s reflection will prove this will not meet with success. Some also apply it to data, such as counts, where we actually expect it to be skewed. And I don’t know anyone that knows how to interpret a variable that has undergone an square root arcsine transformation. If your model is not capturing tails or other aspects of the observed target, it’s likely because the normal distribution is not a viable candidate for the underlying data generating process, or the model simply isn’t capturing a key aspect of the data (e.g. inherent clustering). It makes more sense to change your assumption about the distribution (e.g. use a t or beta distribution if appropriate), or fixing your model (e.g. using a mixed model if appropriate), than trying to torture your data in a misguided attempt to speak to one of the less important assumptions in the standard linear model. You’ll also find that the heteroscedasticity problems magically go away as well. Analytical summary what is needed further for your specific projects learn more about whatever technique your using Programming If you’re doing serious scientific research, these days you’re going to have to learn some programming24. Don’t want to? That’s okay, just do something else besides scientific research. Even if you’re doing relatively simple analysis, a lot of data processing will have to take place beforehand to ensure that whatever results you obtain have integrity. Unless you like to spend several weeks on something that could take a couple hours with even just basic programming skills, you’ll need to spend some time in this area to develop said skills. One of the first things to note with statistical programming is, that once things are set up in terms of the data, it is as easy to run a model in R or Python as it is menu-driven programs like SPSS. It even requires less syntax than you would find with, e.g. SPSS and SAS. Compare the following two approaches, one a menu-driven approach in SPSS and another programming approach in R. SPSS Click File Click Open -- hunt for file on computer -- wait a few seconds for SPSS to open its own file type Click Analyze Click/hover Regression Click Linear Regression Search for and Click your dependent variable (or click, start typing the name, and hope) Search for and Click each &#39;independent&#39; variable Click on various options Click ok R d = haven::read_spss(&#39;filelocation/file.spss&#39;) model = lm(DV ~ x1 + x2 + x3, data=d) There is no debate here, even givng SPSS the head start of having the data in its own format, one approach is vastly more efficient, and simply easier. In fact, with RStudio’s autocompletion, the odds are that the first line probably only required a few keystrokes. You may have to learn that you can use the haven package for importing SPSS files, or the lm function to do regression, but that’s very quickly picked up, and less than you would have had to learn regarding the SPSS menus, which have remained mostly unchanged and in their disorganized state for over 20 years. The other nice thing is that you can redo the above in two steps with Ctrl+A (select all) and Ctrl+Enter (run)25. R &amp; Python When it comes to statistical programming and computation, there are two primary choices these days, but you should be flexible, because the situation will likely change at some point in your career. R and Python are the primary tools of statistics/data science, and have pretty much overtaken both enterprise and academia. They are both easier to program for data processing and analysis than they were even just a few years ago, and notably more so than they were in the early days of their use for modeling. It doesn’t really matter which you use. Python will have more power when it comes to machine learning, potential enterprise applications, natural language processing, and, in general, speed. R is still notably easier to use for applied interactive analysis, and has far more developed packages for a far wider range of techniques. However, these distinctions melt by the wayside more and more everyday. So you’re fine with either. Alternatives Other Programming languages and related Lower level programming languages such as Java, Fortran, and C++ are not required knowledge for the vast majority of applied statistical and related application. They are primarily used when you need to get the most speed you can, and you’ll be able to find someone on campus to help you there, for example, in your research support group. Matlab, Julia, and Mathematica are other tools or languages you might come across that are highly capable, but are either proprietary (i.e. require a renewable license, cost money, and don’t share source code), or don’t yet offer much beyond what you can get with Python and R, and usually far less. Of these I’d recommend Julia as its use appears to be on the rise and is freely available. You will also find languages and frameworks within languages such as Scala, Spark, Hadoop, Keras, Torch, TensorFlow, SQL, Lua and so forth. Most of these have specific purposes, e.g. databases, or don’t need to come into play unless you have massive data (e.g. TensorFlow). Don’t bother unless needed, but don’t be surprised if you do. In addition, people are writing analytical functionality in things like javascript, Ruby, Haskell and other things that weren’t meant for it nor are all that good at it. I don’t know why. Statistical or similar The primary statistical programs in academia are SAS, SPSS and Stata. Unlike the others, Stata use actually appears to be growing, and they also have a great community. The other two are dwindling fast in academia, especially, and thankfully, SPSS. Aside from those, there are relics, money-grabs, and WTFs of all kinds. Seriously, there is some ridiculous software out there that some poor folks in some specific niche of a discipline got suckered into using a long time ago, and for whatever reason still use it. If you’re using non-open source software whose functionality is completely matched and bettered by others that are freely available, you’d better be able to justify it in some way, and I can’t think of anything you could come up with. If you use something like SAS or Stata, you still can’t use the menus for analysis. Aside from being inefficient, it’s not reproducible, and thus not a way to do science. Excel is not an option for scientific work, ever. I would say you could use it for data entry, but it’s very problematic even then. Avoid at all costs. If you don’t want to take my word for it here are some other things to peruse. Link 1 spectrum.ieee.org Link 2 tiobe.com Link 3 r4stats.com Link 4 kdnuggets.com IDEs After installing the basic components for R or Python, you can use them straight away. However, this would be silly, as they don’t come with a way to use them that enables basic functionality or is easy. If your R console comes with retina-bleeding red-colored font, or you’re using the clown-colored IDLE from Python, just shut them down and walk slowly away. You want to use what’s called an integreted development environment, or IDE. For R, the far and away most popular one is RStudio. For Python, I would suggest Anaconda/Spyder for scientific work. There are others, but unless you have a lot more experience, I’d suggest sticking with those26. What these do, practically speaking, is make your coding efforts a lot more easy and efficient. Syntax highlighting, code completion, autospacing, interactivity… this is only the beginning. I’ve written this document, workshops, published articles, and many other things entirely with RStudio. A good IDE is essential. Customize it After you initially start to get the hang of your IDE, customize it. I’m always struck by the fact that people don’t change the basic settings in their software, which are often chosen by people who should not have any say in design whatsoever27. Coding style Coding style matters, but perhaps not in the way that many think. The only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code. ~ Hadley Wickham Wickham is right on the first part, but I’m not so much in agreeement with the second. No one writes perfect code, no one. It takes effort just to write halfway decent code. That’s fine. As you start out, you won’t be writing good code for quite some time. That’s okay too. However, assuming you want others, including yourself at a future date, to be able to use your code, you should feel bad giving them shitty code, because you’ll be wasting their time. Going back to one of our initial suggestions, Don’t be a jerk. Write better code, always. Time and other resources will always be working against you, but do the best you can. The first step is to start with a coding style. For example, both Google and Hadley have provided an R coding style for others to follow, and there is PEP8 for Python. If you only followed those, your code will be great relative to what you would have done. But realize they are only suggestions, some are based on decades of coding practice, others are based on whim, and they don’t tell you which. For example, the Google says using underscores for variable names, e.g. this_is_a_variable, is ‘bad’, while capital first camel casing is ‘good’, e.g. ThisIsAVariable. However, it’s very clear which is more readable, and just as easy to type. Furthermore, with modern IDEs, once the object is created, you only have to type a couple letters to complete the rest. Both Wickham’s and Google’s style guide consistently contradict one another and themseleves, and yet they’re both based on more experience than you’ll ever have, so that should give you some sense that it isn’t exactly easy to figure out what works best. The point is, there is no authority here, but there are better approaches than your nospace, semi;coloned, naming-things-‘data’, everything-in-a-single-script attempts will come up with, I guarantee you. In coding, just trying to do better will make your code better28. Errors as calls for help When you first start programming, you’ll get a lot of errors. Rather than getting frustrated, you should see them as calls for help from the programming environment you’re working in. For example, take the following. sum(z) Error in eval(expr, envir, enclos): object &#39;z&#39; not found This is simply R telling you that it can’t find something named z. It may be that you called it Z, or haven’t yet run the line in your code that starts with z = to create it. But it’s no reason to be frustrated. When running models, errors typically signify that a variable or even the whole data set is not in the correct format. Check the documentation. In other cases, you’re probably spelling it wrong. Warnings are not errors, but you should investigate why they are occurring, rather than ignoring them. A warning telling you that a package was built with a more recent version of R is no big deal. A warning telling you that your model did not converge is serious. As you start out you may not know the difference, so treat them as serious until you understand them. The more programming you do, the fewer errors you’ll see, but when they do occur, you’ll also more likely know what they mean and can more easily deal with them. I/O One of the first things you want to learn in scientific programming is how to get common data formats, such as text files or statistical program formats, in and out of your programming environment. As long as it is difficult for you to even get started, the less likely you are going to want to do any programming in the first place. In the past it might have taken a couple steps to go from one format to another, but this is no longer the case29. And every programming language or statistical package can easily read in common formats, but you’ll have to get used to how to do it initially. Even RStudio provides a point-and-click approach to importing data, but you should be doing things such that coding it will be more efficient. Note that very large data sets may require special needs, but you’ll just cross that bridge when you get to it. Basic object types There are object types that are common to practically all programming languages, and you should be familiar with them. boolean: a binary 0-1, or TRUE/FALSE. These are especially important to be familiar with for indexing purposes. numeric of a kind: specifically ‘floats’ or ‘integers’ but for practical reasons can be anything that isn’t a string variable (and may include boolean). string: text based represenation. For example, a variable may be a boolean called Male, which would indicate that values of 1 or TRUE would indicate an observation is a male, or the variable might be a string called Sex with values of ‘Male’ or ‘Female’. Beyond that, statistically oriented programming environments typically distinguish categorical variables, called factors. You can think of them as labeled integers. For example, we might have a variable called Sex that has values of 1 and 2 but with labels ‘Male’ and ‘Female’. The distinction between factors and strings is less an issue these days, but depending on the package you’re using or analysis being conducted, you may need one or the other. Data structures Beyond basic objects/variables, we also have more complicated structures, which again you’ll want to be aware of, because you’ll use them often. vectors: Vectors contain multiple elements of the same type. matrices/arrays: Matrices in R can be thought of as two-dimensional data structures, and arrays will allow beyond 2-d. In Python, these would be numpy arrays. data frames: Data frames are, at least in R, the fundamental modeling object that has all the variables you want to analyze. They are distinguished from matrices in that they are able to possess variables of different types, e.g. factors and integers. The corresponding Python object is a pandas DataFrame. lists: lists are objects with no restrictions on what the elements may be30. The first element could be the word ‘cat’, the second could be a list of strings pertaining to animal names, the third could be a thousand numeric matrices. They are extremely common in R, as practically every model function returns a list (e.g. including elements of coefficients, fitted values, etc.). Lists are found in Python too, and aditionally one can think of Python dictionaries as named lists (though there is a little more going on than that). Note that R doesn’t care whether you name the elements of the list or not, it’s still just a list. Indexing Once you have a data structure you’ll need to learn how access parts of it. For example, you may want to inspect the first 10 rows, or the fifth column, or all the variables that start with ‘X’. This is what a base R approach might look like. mydata[c(1:2, 6, 8), 5:10] # works for matrices or data frames mydata[mydate$Sex == &#39;Male&#39;, ] # boolean indexing mydata[,&#39;variable_name&#39;] # works for matrices or data frames mydata$variable_name # works for lists/data frames mydata[[&#39;variable_name&#39;]] # works for lists/data frames However, base R indexing for data frames is largely a good way to write ugly and/or bad code. One of the first and lasting rules in programming for scientific work is: Avoid magic numbers. Indexing by numbers goes against reproducibility, because row and columns can change quite dramatically both in data collection and data object creation. The Age column might be the fifth one now, but it may not be later. If you want the Age column, reference it by name. Likewise, for Python dataframes your default should be loc rather than iloc, and similar. The tidyverse approach in R enhances reproducibility by making it easier to use names and by using functions that make it clear what’s going on. Take the following for example. mydata %&gt;% select(Age, contains(&#39;blue&#39;), starts_with(&#39;A&#39;)) You might not have ever worked with the dplyr package, but I’ll be you know what that’s doing. Furthermore, if it’s your code, you’ll know what it means six months from now. Python dataframes have similar functionality31. The gist is, you need to get familiar with how to slice and dice your data structures, and the quicker you will be able to process the data and get to the interesting aspects of it. Aggregation One of the most common data processing procedures beyond slicing rows or columns is the ‘group-by’ operation. As such, you’ll need to get used to using it in your environment as early as possible, such that it becomes second nature. When it comes to R, do not use base R approaches like tapply or aggregate. They are slow, memory-intensive, or just plain annoying in implementation. Again, the tidyverse will help here. mydata %&gt;% group_by(Sex) %&gt;% summarize(Age = mean(Age)) The above would return the mean age for both Males and Females. Very straightforward. Pandas in Python will have similar functionality32. Iterative programming One of the most common activities in programming is doing a specific operation on all rows or columns, or some subset of them. It’s so common, that you’ll need to learn it early on, and use it often. The basic approach for most programming languages, called a for loop is as follows: column_means = list() for (i in 1:10) { column_means[[i]] = mean(mydata[,i]) } The above would calculate the mean of the first 10 columns of some matrix/dataframe x. When you look at the column_means object after running the loop, it will have 10 elements, each being the mean of the associated column in x. It is extremely important to know how to use loops very early on in your programming, or you will be wasting enormous amounts of time, because it always comes up. Once you get the basic idea down, know that it will be conceptually identical and often even look the same in almost every programming language. Here is the same loop in Python, using the numpy module (as np). column_means = [] for i in range(0, 10): column_means.append(np.mean(x[:,i])) However, for data science, there is typically a more efficient approach, in terms of code, computational time, or both. All of the following would be a better way to do the same operation in R. colMeans(x[,1:10]) apply(x[,1:10], 2, mean) sapply(x[,1:10], mean) x %&gt;% summarise_at(1:10, mean) Similarly in Python: np.mean(x, axis=0) Again, you need the concept first, so start by doing explicit loops, then see how to use the tools available so that you almost never have to write an explicit loop. Programming summary Programming is a fundamental part of modern science, and I personally would go so far to say that if you aren’t doing any programming at some point in your data processing and analysis, you aren’t doing science. It formalizes the analytical approach, makes all your data processing efforts more efficient, and tends the entire endeavor toward a more reproducible setting. The better you become at it, the more time you can spend on the other things. Other Visualization There are a couple important things to know when you begin learning about visualization in the scientific context. One of the more essential things to learn right off the bat is that your intuition is almost certainly wrong. You likely don’t know how to visualize well because most visualization in scientific/academic reporting is done poorly, if not terribly, so going by what you see will simply perpetuate bad approaches. For example, no one should be doing 3d pie charts, bar plots with error bars on the top of them33, or plots with unusually scaled axes, yet these are still regularly displayed in journals. On top of this, an applied stats course likely will not spend much time, if any, teaching the basics of visualization. I don’t mean producing plots, at least they usually do that, but actually thinking about the means of conveying scientific information visually. Second, depending on the tool used, default settings may be just as bad as what you would choose with no information. For example in terms of color, whatever you’d pick on your own is not going to do well, but unless the package developer put actual effort into it, you might end up with red-green or other problematic combos34. Lines may be too thin, gridlines may overwhelm the actual plot content, and so forth. With color, issues of emphasis, colorblindness, etc., come in to play, and unless you put some thought into it, your visualization will have problems. A good visualization in a research context will take complex relationships and make them easier to understand, even if it takes some effort on the part of the viewer. So when thinking of a visualization, the goal is going to typically be to work with some complex, but interesting, data story and attempt present it in a way that tells that narrative in a clear fashion. For example, showing the difference in group means is not complex, so a visualization is just going to create a bunch of whitespace to tell you what a single sentence could. The above has multiple problems, some of which include that the differences are perceptually exagerated, most of the plotting area isn’t useful, the colors chosen are poor and draw the eye to versicolor more than the others, the colors add no information, it has a completely unnecesary gridlines, and the only ‘story’ here is simply that ‘Setosa &gt; rest’. When displaying information for others to consume, always think beyond two dimensions. 2d plots such as simple bar and scatter plots are generally a waste of time, beyond initial data exploration (i.e. before modeling), and it’s simply too easy to add more information while still not being overwhelming. For example, with a scatterplot one can use point size to represent another variable, color for some grouping, and transparency for yet another variable. You now have five pieces of information (including the basic x and y dimensions) represented in the same graph. The following represents all the information in the iris data set35 (hovering will show the other variable values), including some summary information (means for x, y axes), and it utilizes all of the plot space. It doesn’t distort the relationship of group means, because we have the context of the all the observations, and the colors chosen don’t draw the eye to one group any more than another. So it takes the previous 2d plot and adds various other information, but without being overwhelming. It has gotten easier over time to pick some decent colors. For example, ggplot2 in R will default to evenly spaced colors, and the viridis module in Python and R will provide evenly spaced, colorblind-, and print-friendly palettes. Colorbrewer.org and the RColorBrewer package is another useful tool. Also note that for scientific communication, infovis is not an option. This isn’t advertising, and what may ‘look cool’ may not be very useful for scientific presentation. Take for example, word clouds. People seem to love them, but they are almost useless. As mentioned in the first part, your audience will be fine with something more than a bar plot. I have been told multiple times that the reason for not doing a particular style of plot is because the audience of Ph.D.s, M.D.s and graduate students won’t be able to figure it out. This is an utterly ridiculous notion with no proof whatsoever. Unless you are thinking interactively first, you probably aren’t even producing a visualization that is as complex (and commonplace) as those commonly seen in news outlets like the New York Times. If someone is actually interested in your work, they will read the caption or associated text and have the cognitive capability to sort out what you’re showing. Reproducibility revisited Data Before we get to analysis, one needs to understand that everything begins and ends with the data. If you don’t collect the data carefully, it doesn’t matter what else you do36. You’ve completely wasted your own time and resources, along with that of everyone else who is affiliated with the project. At this stage in your career, if you’re not willing to get your hands dirty with the data and make sure it’s collected and processed properly, you simply shouldn’t be doing research37. Collecting and processing the data is not as fun as analyzing it, but analyzing it isn’t any fun at all if you can’t trust the data. Furthermore, the more time you spend with the data, the analysis stage will likely be very brief, because there will be less going back and forth from analysis to data checks. Analysis The first step in reproducibility is to be able to duplicate the results of an analysis. We can refer to this as reproducible data analysis. The key idea is whether you could start with the raw data and produce the same results as you publish. In addition, you might apply the idea of convergent validity to the process. Similar analytical techniques should come to the same conclusions. However, this is a very low bar for reproducibility in general. A real first step would involve setting things up so that someone else could reproduce the results you came up with. The best way to achieve this is to engage in literate statistical programming, probably best exemplified with using Rmarkdown for your documents. Using LSP, the document, data, and code are combined into one entity. As an example, one can see this article. The actual files that produced it are a combination of text, R code, HTML, \\(\\LaTeX\\), and more. Not a single number you see in it was typed- there is R code embedded throughout the text producing those values. The figures38 are produced with the document, not cut and paste into it. Same with the statistical results. To reproduce the results one merely has to compile the document with a single click. In fact, it’s impossible to get different results. There is actually no need to write ‘scripts’ anymore. One can, with Rmarkdown, Jupyter Notebook/Lab, and similar tools, take a reproducible manuscript as the default scientific product. Programming There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors. Comments A starting point for reproducibility in your programming is to comment everything. The comments should not be about what the code is doing. This is because if you know the language at all you’ll know what it’s doing, or if you forget, it will be easy to look up what the function or command does. Instead, comment about why you’re doing it, or what the goal of the code execution is. However comments aren’t exactly necessary if your code is embedded in the research product, i.e. the document, itself, though they still might prove useful. Naming things Another thing to think about is naming things. If you are naming anything of importance as ‘data’, you deserve whatever happens to you. Even more if you are naming things ‘data3’ or ‘finaldata7’. However, most of us aren’t taught this, and so are guilty of it at some point or another. But now you know. So don’t. This goes for files, objects, variables, everything. Name things as though you won’t need it for a year, and yet a year later you’d know what it was. With modern IDEs, auto-complete means you can use long names and not even have to type but a couple letters. Take advantage of this. A good rule of thumb is to ask what would you look for six months from now? Projects This was mentioned in the previous getting organized section. IDEs and even some statistical software allow for the creation of projects. These become self-contained folders where all your scripts, documents, visuals, and so forth reside. Basically, if you are hunting for files or have to set your working directory, you’re not doing things in an efficient or reproducible fashion. Creating projects is so simple, there is no reason not to. For any distinct work you’re doing, create an organized and self-contained folder that only pertains to the work done for that project. Documents Writing up scientific results in something like Microsoft Word is a deplorable practice in terms of science. Copying and pasting output from a statistical or other programming environment into a Word document is practically the definition of irreproducibility, opens the door for numerous errors, and, on the practical side, is exceedingly tedious. If any change has to be made to the data, e.g. an error is discovered, analyses re-run etc., one will potentially have to change every number and visualization associated with the document. That alone should be reason enough not to want to engage in such a practice. Many disciplines have used \\(\\LaTeX\\) as their approach and still do. Not only does it produce beautiful documents for print, it eventually became a means for literate programming, simultaneously embedding code and text and providing the means for more reproducible work. Unfortunately, \\(\\LaTeX\\) is inherently a print medium, making it marginally useful these days. If you’ve learned it, don’t fret, you can still use it, e.g. with math formulas, but everything you could have done with it you can now do via other means. Except you won’t have to completely disrupt your train of thought and text messing with floats and tables. Markdown has displaced \\(\\LaTeX\\) for scientific publishing these days. In the right environment, there is nothing you can’t do with it that you couldn’t do with \\(\\LaTeX\\), and if you think otherwise, it just means you haven’t gotten used to it yet. The issue is that, despite what many journals still seem to think, the web is the medium of scientific communication, just as it is with practically everything else. If you’re creating a scientific document with a print-first mentality, you are probably not interested in disseminating your work to the broadest possible audience, and you are going to continually handicap what you might have been able to show with your efforts. Rmarkdown allows one to produce HTML, PDF, MS Word and a whole host of other formats (e.g. slide presentations), and in about as easy a fashion as one could, especially compared to \\(\\LaTeX\\). Other statistical environments such as Matlab and Stata are playing catchup, while others, still haven’t figured it out. In the Python world, you have Jupyter Notebook and eventually Jupyter Lab, though I haven’t seen it used in as extensive fashion as Rmarkdown39, which incidentally would allow you to incorporate Python and some other languages along with your R. The gist is, your document first needs to be connected to the underlying data, and secondly needs to be ready for the web. To get a sense how far one can go, I recently collaborated on a paper entirely in Rmarkdown, in which not a single digit in the final copy was actually typed. All values, tables (which were also formatted via code), figures, and inline text was produced via underlying R code, and with the final output, as much as it pains me to admit, MS Word. For those using R, Rmarkdown makes it such that you don’t even really have to write traditional scripts anymore, you can just use Rmarkdown and let your normal text take the place of comments. For Python, you have Jupyter Notebook that appears to allow much of the same. Summary of Part II Most applied researchers, or anyone engaging in statistical modeling generally, can not only benefit from many of the concepts outlined in Part I and II, but also have plenty of time to do the other things they need and/or want to do. It may seem like spending time with these things might seem like something you don’t want to do, but the alternative includes far more time cutting and pasting, redoing uninteresting analyses, trying to find which file/data goes with what, writing about noisy results, creating ugly visualizations etc. In other words, you don’t have to be a strong programmer, though you will have to do some programming, and take steps to do it better. You don’t have to be a statistician, but you will need to learn well a technique you intend to use. It won’t be easy, but I’ve never seen an instance in which the alternative was easier. Don’t deceive yourself. Do better. Should you need assistance with knowing whether you have leveled up or not, there is someone in the IT department at LSU named Robert Gill III Esq. that can help you. You cannot provide specifics, only ask, ‘Have I leveled?’. He will then respond with a series of questions that will, in one way or another, probe the essence of your being, and provide the only answer you will ever need. Sprinkled with quotes from Buckaroo Bonzai, Goonies, and for matters of import, Forrest Gump.↩ In other words, a ridiculous amount of data, with ‘clean’ data, and a whole host of other things in place.↩ It is current if you’re not aware of statistical history of the past 60+ years. The only thing that’s different from past calls to do better work is people’s ability to complain on social media. Applied researchers will do no more methodologically than what is expected, which seems largely defined by whatever they come across in the few journals they spend time with. This is not necessarily right or wrong, just the way it is.↩ I’m not even going into the fact that they probably finagled the data 20 different ways with umpteen models to get that p-value slightly less than .05, a practice called p-hacking. People who do this engage in the worst type of anti-science.↩ If you’re just starting out, it probably isn’t simple to you, but we’re talking relatively. It’ll be old hat to you as well at some point.↩ Sometimes called marginal effects.↩ However, I can say I do like name dropping canonical correlation, which MANOVA, LDA, and standard regression are special cases of, every once in a while just to befuddle people.↩ You may try, but you will never, ever find a better quote in all of statistics. Also, it’s generally expected that when uttered, those in earshot must snap at least twice and say ‘Right on, daddy-o’.↩ The reason many sciences have progressed is because they don’t use NHST correctly.↩ At least until the ideas of Star Trek become a reality. And don’t let the folks in the Bay Area fool you, it’s still a very long way off. For some perspective, at the turn of the 20th century, there were no cars as we now know them other than demonstrations and prototypes. By World War I, less than 20 years later, cars were a common sight in cities, and airplanes were being used in combat. We’ve now had more than four decades of combined internet and personal computing capabilities, and probably the biggest difference between now and then is that people spend a lot of time staring at tiny screens in their hand. Most of the stuff we’re just now getting to was promised to be right around the corner during the space age of the 1950s and 60s. The point is, a lot of this stuff will come, but don’t hold your breath.↩ I was going to do a Python example too, but I couldn’t find an example that wouldn’t take 5 or more lines not counting the import lines, which would be several more.↩ I do like Atom for Julia and Python, but it’s a general text editor, not a program specific IDE.↩ Just look at Google’s gmail, Facebook, Amazon. They are essentially the anti-Tron of user esteem. Amazon in particular seems to simply vomit out the return of a search that mildly references some of the letters you actually typed into its engine, if they used Google translate on the query first, while assuming the native language was an interpretive dance in Kabuki theater. The ‘art’ of basic drilldown is a completely lost enterprise. Using that website almost makes you want to ask Alexa for the best form of suicide.↩ My main motivation for getting on GitHub was making my code better. If the only reason you use it is that the thought of others looking at your code will make you write better code, then definitely do so.↩ Some of you may still come across a tool like StatTransfer to go from say, SPSS to Stata. This is completely unnecessary and has been for some time.↩ Technically, R data.frames are lists.↩ My document on data processing and visualization has examples in both R and Python. The Python code is available at the repository.↩ In case it wasn’t obvious, the Pandas and tidyverse authors are in cahoots with one another, though the Pandas author is no longer actively involved in its development as far as code is concerned. It should be fairly easy enough to move from one to the other.↩ Or barplots at all in my opinion.↩ Matlab ignored for years calls to get rid of its default ‘jet’ colorscheme. They finally did, and laughably have tried to make their new choice, which is still problematic, proprietary.↩ This is the last time I use this terrible data set.↩ Despite what some in the ‘hard’ sciences seem to think, merely collecting the data and having a good theory is not enough for strong science.↩ One can distinguish researchers from (research) administrators. I have respect for the latter, as they are often the ones who coordinate the teams, acquire the funds, and drive the research in general, though in many situations, I would hesitate to say they are actually doing the research. For graduate students and early career folks for whom this document is intended however, this distinction doesn’t really apply. You should be neck deep in your data.↩ Sadly I largely lost most of the arguments on the visuals.↩ I don’t see many bells and whistles with what people do with Jupyter Notebook - practically all the ones I come across look like the default - which makes me think it isn’t easy to go into deep customization, though I don’t really know (I’ve only dabbled with it myself, so I’m speaking from a place of ignorance). However notebooks in general, including R notebooks generated by RStudio or Jupyter, are very much geared toward interactive coding rather than creating a more or less static scientific document. Though things may change in the future, that has less a role in typical scientific communication, where you wouldn’t want code interfering with the data story you’re trying to convey. It will be interesting to see where Jupyter Lab goes in furthering the development.↩ "],
["part-iii-one-last-caress.html", "Part III: One Last Caress Analytical Programming Other Summary of Part III", " Part III: One Last Caress This section is for those who get the bug, and actually find they enjoy programming and analysis, or simply want to continue expanding their toolbox to do better analysis. It’ll outline some good things to be aware of, or skills to start obtaining. Analytical Beyond GLM After standard regression and classification techniques, there are some common extensions to consider. However, I would suggest you first learn an advanced technique that is commonly used in your discipline. Beyond that, some more common statistical models that take a next step in complexity are those that deal with clustered data (mixed models), nonlinear relationships (e.g. additive models), and those that deal with space or time (possibly both). Be aware that the additional complexity brings with it a fuzziness, such that it becomes difficult to use standard null hypothesis testing. This is not a drawback by any stretch, but it will be difficult for you if you’ve come to rely on p-values as your primary means of interpreting models. Dimension Reduction/Latent variable models In the previous section we noted that one should at least be familiar with why you would use things like PCA and factor analysis. For the more motivated, these should be standard tools in your toolbox. I would specifically suggest PCA, standard factor analysis, latent dirichlet allocation, and model-based clustering methods (i.e. mixture models). Matrix basics There are two reasons to know just a smidge of matrix operations- speeding up your code and understanding methodological presentations. As far as the amount, you probably don’t need any more than is commonly found in the appendices of some statistical texts, and being familiar with those operations in your programming language of choice. With programming, while others are writing verbose loops, you’ll be speedily one-lining the same operations in vectorized fashion. Furthermore, when you’re wanting to learn new techniques, such knowledge will allow for more rapid acquisition. The Bayes Way Bayesian analysis is commonplace, and requires no more justification than standard techniques. If you don’t see it regularly applied in your field, it is your discipline that hasn’t been keeping up with things. You, on the other hand, should feel comfortable using it in lieu of any other analysis you would, starting with regression. A Bayesian regression requires no more effort to explain than a standard regression, because it is just a standard regression, albeit estimated differently. The benefits of the Bayesian approach are many. To begin, you get probabilities and intervals that make sense, and don’t require some notion of ‘infinitely repeated experiments’ and backward logic. Any statistic you can calculate from a model will automatically have an interval estimate. For example, in a standard regression you’d not only get an ‘adjusted’ R2 by default40, you’d also have an interval estimate for it. The Bayesian approach works well in cases where you have small samples, possessing built-in regularization to guard against overfitting, allowing you to explore more fully your data even when you don’t have much of it. For more complicated models, the p-values and intervals are no more difficult to acquire than usual, unlike the traditional setting where it seems that for many models beyond the GLM setting approximations are required (e.g. mixed models, generalized additive models). In general the Bayesian approach will give you more flexibility and better assessment of uncertainty, and can definitely be your default modeling approach. For an introduction to Bayesian analysis, see my document here. Machine Learning In this day and age, any applied discipline that teaches a semester course in statistical analysis should have a week of it devoted to an overview of machine learning. The only reason you may not see it practiced much in your discipline is due to 1. your discipline never teaches it, or 2. people outside of your discipline are doing the interesting things for your discipline, and publishing elsewhere. I can tell you however, that it is useful, you may need to employ it some day, or at the very least, you shouldn’t ignore articles using it because you don’t know what they’re talking about conceptually. Data can be big, complex, or both, and there are techniques to handle it if so. If you don’t know about them, you may be inclined to do some very silly things to coerce the situation to one you know, which is precisely the wrong thing to do. If your discipline doesn’t cover it in any fashion, do a workshop, look online for some exercises, or simply schedule some time to chat with someone who does know about them. With tools like R and Python, the difficulty will not be running the models. Programming You’ll always be able to go further in programming, if for no other reason than the fact that you likely weren’t taught basic programming skills in the first place. The more you know, the easier things will continue to come, the more data you can explore, the better visualizations you can produce, the speed at which you can do practically everything will increase, etc. Keyboard There are things you can do. And there are things you can do because you finished that other stuff an hour ago. One of the main benefits of an IDE are all the keyboard shortcuts that can save you a lot of time compared to point and clicking. Unfortunately, keyboards on many laptops don’t have the type of keyboard those shortcuts assume, and some operating systems seem to not be aware that keyboards can have over 100 keys. Writing functions One of the first things you can do to take your programming further is to write your own functions. One of the tenets of programming is DRY, or don’t repeat yourself. Anytime you’re copying what is essentially the same code violates that principle. Writing your own functions can help you avoid doing so, and can be extremely simple. For example, the following takes two variables, divides one by the other and multiplies by 100 (i.e. creates a percentage). R percent &lt;- function(x, y) { 100*x/y } Python def percent(x, y): return 100*x/y If you want, you can now add additional complexity to make it do more (e.g. add rounding, formatting). Furthermore, you can reuse the function anywhere you like. Once you get the hang of writing functions, you’ll find yourself doing so regularly, because it simply makes things much easier than constantly repeating yourself. In addition, you may find other functions available in packages do almost everything you want, and you’ll be able to tweak their code to create a function that does exactly what you want. Debugging Debugging is the process of finding errors or issues in code. If you are writing your own functions, debugging will allow you to course through the function line by line, inspecting the output created, and fix any errors. Even if it’s not your own function, debugging can allow you to find out why some function is spitting an esoteric error message at you. More generally, the concept of debugging is essential to scientific endeavor. Data, models, and theories come with bugs too, and you should have a similar mindset when approaching them as you would problematic code, inspecting them piece by piece and improving them whenever the opportunity arises. Iterative Programming &amp; Vectorization One of the more popular blogs on R-Bloggers for a long time was ‘Writing your first for loop’. The reason is that iterative programming is a core programming technique, particularly in data science. It is so common that you’ll want to apply some function to, for example, every row or column, that not knowing how to do so in an efficient fashion will severely hamper your coding practice. Some programming languages have even more efficiency to be gained through vectorization, where operations are done on vectors rather than proceeding element by element. For example, in R, between core functions being vectorized, matrix operations, the apply family of functions, and packages like purrr, I almost never have to write an explicit loop, and code runs much faster. With some languages you may find some that suggest vectorization will not produce much speed gain. They are wrong, if only because it will take you less time to write the code. Cleaner and clearer code is always desirable too. High Performance Computing At some point your data or models may require more resources than your computer can provide. Research universities provide cluster computing for such cases, but now anyone can use services such as those provided by Amazon or Microsoft as well. Knowing the basics of parallelization, the cluster computing environment, and related tools will be necessary in such cases. However, even your own computer comes with multiple cores that you can take advantage, so you should be able to use the tools of your language to do that. Any iterative process in which the iterations are independent can potentially benefit from parallelization. When speed becomes an issue, you’ll want to also be familiar with code profiling. Profiling allows you to assess where in your code bottlenecks occur, which you may then rework to produce faster results. However, this comes with the following caveat: premature optimization is the root of all evil. Code can be relatively slow but still good enough, and it’s important to balance computing vs. programming time. Other Visualization &amp; Presentation In a previous section, we talked about how a good visualization will take complex relationships and make them easier to understand, and in general, it will require some effort to pull off a good one. Beyond that, you also want to think interactively. Interactivity is key to visualization in this day and age, and should be a fundamental part of your visualization arsenal, especially since interactive forms of traditional plots are as easy to produce as the traditional ones. It probably won’t be long before it’s an expected part of scientific visualization, assuming the journals get out of the way. The nice thing is, you can always start with an interactive plot, and take a snapshot for the publication, keeping the interactive plot as supplementary material. Adding interactivity can add even more dimensions to a single visualization. For those that really want to go down the rabbit hole regarding visualization, there’s always more to consider, and this document can perhaps serve as a starting point. The text is of a contrast ratio relative to the background that would satisfy some of the more strict web accessibility standards. This includes the color coded text, though those are not quite as stringent. I use my own CSS files to implement these throughout the document. In addition, the colors in the plots are evenly spaced and would be distinguishable for those with even rare forms of color-blindness. You’ll also notice there are no gridlines, which are completely unnecessary when you can obtain the specific values by hovering over the plot41. Engaging the web If you do enough data analysis you’ll eventually need to acquire some data directly from the web, possibly in some raw form (e.g. text) that will require additional processing. Whether this will be easy or difficult unfortunately will not be up to you, but rather to those developing the website(s) that house the data, and many actively make the data difficult to obtain42. However, you can make it easier on yourself by knowing some basic HTML and other aspects of server-client communication, which will also help your web-based presentations. Your browser even comes with tools to help you. Scraping the web will involve using tools in ways you never have, and thus often take a lot of effort, especially the first attempt. However, the payoff is often worth it, especially when you can create a unique data set that no one previously had access to. Version control basics Version control is something you should be aware of even if you don’t use it beyond the things like Box drive. For decades now there have been what are called version control systems (VCS) for software development. And even if you don’t use formal a formal VCS, the concept of thinking of your science as software development will go a long way toward improving your scientific method. Tools like Git and Mercurial allow for complex development involving many developers and iterations of applications to come together in a way that allows one to more rapidly correct mistakes, branch off into exploratory endeavors, add new people to the project, etc. But you’re not a software developer you say. I say so what. Consider a faculty member with several grad students working on different phases of multiple projects. These tools would potentially allow for much more seemless collaboration, where, for example on one project a student could be working on the visualization ‘branch’, another on the conference presentation, another may be correcting some data, and while the faculty is working on the write-up, and eventually all these pieces can be merged into a final product. If there is any issue found later, it is easy to revert back to the correct state and continue from that point. VCS are also typically hosted remotely, e.g. at GitHub, and so automatic backup is in place, and it also provides a potential way to have your work exposed to even more people. The latter is especially nice for students, who will usually want to be marketing themselves and their skills as much as possible. However the real benefit to using something like Git and GitHub, is that if you know others are going to see your code, you will write better code. To get started with version control, I’d suggest creating an account on GitHub and going through the steps to create your first repository. A repository (repo) is merely a collection of scripts and files associated with a specific endeavor. For example, many R packages and Python modules can be found as a repo on GitHub. Your first repository can be used for anything, but at least one of your first should be used just to learn how to use Git. Don’t worry, you won’t be able to break anything. The basic idea is that you will write some code, then push what you have to the repo on the web server (e.g. GitHub). Likewise, if you don’t have the latest update, you can pull whatever is on the repository to your local machine. In its most basic form where there is only one contributor to a repo, i.e. you, using Git can be very easy, especially with an IDE like RStudio that allows you to, once Git is installed, use version control via a couple clicks. Other tools, e.g. GitKraken, can make the process more visual and efficient. In my opinion, one of the key benefits to using a public storehouse for your code is that someone else might see it. For me, this meant writing better code, and that alone is beneficial. For private endeavors there is BitBucket and GitLab, which work very similarly to GitHub. However, note that while they are not public, that doesn’t mean they are secure enough for your data purposes (e.g. HIPAA compliant). However, you can still do the work and just ignore files that are data as far as what you send to the repo. The additional benefits to version control are peace of mind, knowing that you can always and easily revert to a previous state if needed, and the discipline of the practice itself. It requires requires a bit of a different mindset to think of your research projects in a similar way as one would with software development, but is extremely useful in terms of reproducibility. When you start thinking of your science and modeling as software development, one that goes through a potentially continual (and public) process of improvement, you’ll almost automatically avoid many of the problems associated with the so-called ‘crisis’ of reproducibility. People engaged in this aren’t having any crisis at all, their methods are open, their documents are infused with data. Journal outlets, as they are now, are impediments, not facilitators. ‘Articles’, if the term should even apply, should come with titles plus a version number, ready to be extended, improved, or merely replicated. Publishing Assuming you have given up using MS Word and are actually interested in doing reproducible research, the next steps are getting used to using Markdown, CSS, and even basic HTML. Markdown is actually easier to use than MS Word, as you don’t have to hunt for what to click to make something a header, or italics. For example: # Header ## Sub-header *italics* - item 1 - item 2 Sprinkled throughout are chunks of code. ```{r myrchunk, echo=FALSE} x = rnorm(100) ``` This is just simple R code, and the option echo=FALSE means it won’t be visible in the document. However, I can then use the object x anywhere else in the document. The point is not to make a markdown document, but to ultimately convert it to HTML, PDF, or even MS Word (if you must). With RStudio this takes no effort on your part. There are tools for Jupyter notebooks also, and even Stata has finally started down this path. Now consider a code chunk that runs a regression model and a function that will convert the results to nice table. No copy-paste nonsense required, and likely no formatting even. Data changes? So will your table of results. Same goes for visualizations. There, I just saved you hours on your next write-up. Beyond markdown, knowing a bit of CSS will allow you to customize your work. Learning CSS is trivial, the hard part is knowing what to change. For example, if you know a modicum of HTML, you’d know that headers are denoted with H, followed by a level. For example h1 is a top-level header. In CSS, if we want to change something about those we’d do the following in a *.css file. h1, h2, h3, h4 { color: red; font-family: Helvetica; } The above would change both the color and font of the first four levels of headers43. Very easy. The issue is knowing what the names of things are, and what options you can tweak. This will come with time, but as a first step, simply use the web developer tools in your browser to inspect webpages. This will show you what both the tags are and what options are applied to them. Once you get the hang of it, writing a good looking and reproducible documents will be your default. In addition, your document write-up can be part of your project/GitHub repo. You’ll be a better researcher, and those that know you’re using such tools will put greater trust in your results. Collaboration &amp; Sharing Once your work gets going there are lots of ways to share it with your research group and beyond. R, Python, and others allow one to create interactive notebooks. The idea here is that someone else can run the code you give them and see the results within the document itself, and while you can do this with any script, it also is in the same markdown format, so could potentially be published to HTML or other format. In my opinion, these are most useful for instruction, otherwise one can use the standard markdown-esque approach. Git and related tools were created specifically for teams working collaboratively on software projects. However, it’s not restricted to software. Research teams can use the same approach. For example, one could be working on descriptive statistics and writing up the methods section (mostly text, some stats), another working on the intro (only text), and another on the results (mostly stats), and do so all at the same time. At any point any of the individuals could compile the most recent version of the document without affecting others44. If you really get good at programming, you may want to make your own package that covers some functionality not readily available or just does something differently. You can then share this with the world so that they can use the tool you’ve created as well. This has become so easy, the current proliferation of packages, especially in the R world, just continues to skyrocket. However, you may want to create a package for your project/team. It’d contain useful functions and documentation, adding to the reproducibility and reusability of your work. This might seem tedious, but the time saved on revise and resubmits alone would probably make up for it. Summary of Part III See Gelman here and here.↩ Gridlines are unnecessary anyway in my opinion. First, can you really not tell roughly where the values are without them? Second, they often convey a level of precision that is not inherent in the data. Third, and most important, they’re ugly. This goes for their use in tables as well.↩ By intention or incompetence.↩ In writing that bit of CSS, I discovered that you can actually implement CSS via a CSS chunk in Rmarkdown. So I guess you could create a CSS chunk at the beginning of a document rather than having a separate file. I find that sub-optimal though, and have no idea what other consequences there may be.↩ In theory. In my experience you’d want separate files for each section of the paper for this to work smoothly.↩ "],
["resources.html", "Resources Web Texts", " Resources The following resources should be seen not as definitive, but merely potential starting points. Web MOOCs MOOCs can be hit or miss. I’ve done a few, and some were more about fighting the system than learning programming or analytical methods. For example, you can having working, even efficient, code that produces the correct answer, but still have it rejected for completely inconsequential reasons, or have to wait an inordinate time to have it run and checked on the servers etc. Some methods-based courses were more about having to learn more about a programming language than learning techniques. Some assume notably more background knowledge than described, or are taught by people who really aren’t good educators, or aren’t adequately tested/piloted. On the other hand, a good one can be a great way to get a solid background in some topic relatively quickly. If you’re still struggling after the first few topics, don’t continue to waste your time. You can always come back to it, or another might come along that’s a better match for you. Also, don’t pay for them. The odds of it being worth it are slim and none. If you’re willing to pay, you should just take a university course or workshop and get personal attention. Stack Exchange &amp; Stack Overflow Stack Exchange is a network of question and answer sites covering a ridiculous number of topics, and with a rating system so that you don’t have to waste time with answers written by people who don’t know what they’re talking about. For example, Stack Overflow is specifically about coding, and you’ll find practically all your questions answered there. Remember, while things are new to you, they aren’t in general. I can only assume it may have something to do with a filter bubble or something, but many still aren’t aware of this resource or don’t use it nearly as much as they should. Cross Validated is where people ask statistical questions, and is another place you’ll likely spend some time. Github Quick start guide - For R users Happy Git with R by Jenny Bryan Texts Standard modeling and beyond Gelman &amp; Hill Fahrmeier Bayesian Gelman Mclreath Machine Learning elements elements in R Murphy "],
["well-get-back-to-you.html", "We’ll get back to you Sleep on your back", " We’ll get back to you Sleep on your back Some background. I was trained as an experimental psychologist. Statistically speaking, that meant a lot of ANOVA, but not even complicated ANOVA, just some standard factorial mixed design. Continuous data? Discretize it and run ANOVA. Not doing an experiment? ANOVA anyway! The time spent on just t-tests and other overly simple bivariate relationships… just ridiculous. Nary a word mentioned that it was a special case of more common and flexible models, I had to learn that later. I also had to take multivariate and psychometric techniques, which were among some of the least likely methods to be used in experimental psychology, but at least they started to get into something more interesting than ANOVA… like MANOVA!45 There were red books often referenced (Winer, Kirk), ‘modern’ updates to those that still were 90% ANOVA methods… it was like some instructors had ignored all that had gone on around them, and figured everything had statistically been settled around 1984. Don’t get me wrong, 1984 wasn’t all bad. In fact it was a most excellent time in some ways46. Sure, there was some of the best entertainment humanity would ever offer, like Buckaroo Bonzai, Red Dawn, Knight Rider, Automan, Street Hawk (almost!), Danger Mouse, A-team, and Battle of the Planets; one could argue it was the heyday of fashion; the cars… MR2, 300ZX, Delorean47, Countache, 288 GTO48! But statistically, computationally, algorithmically, many approaches to data analysis were only just beginning, as computers were becoming more and more commonplace, opening the doors of discovery. And it is weird to me that people were, and still are, writing applied books on statistics as if this revolution did not take palce. Programatically my training was actually worse- SPSS, and not even really programming49, just using the menus and punch-card-based syntax50 to run the available procedures. No one, absolutely no one was taught basic programming principles that would have made so many things much simpler. You pick some up along the way, but it was never focused on in the actual training. One of my cohorts, who, lucky for me, had somehow skirted the laws regarding the length of graduate training, was regularly going on and on about R. When I eventually got a job with him as part of the campus research support group, I gained more exposure and had some guidance to boot. SPSS was regularly failing at doing anything even remotely beyond the basics, like calculating an effect size or incorporating robust measures. Given my training, R was a complete pain in the ass initially, but it eventually did what I wanted, and far more easily than SPSS would have. And every time I used it, I saw something else I could do with it, and I learned something new statistically or programming-wise. It didn’t take too long before there was no reason to use SPSS, and in the time since R has become the standard for statistical programming51, while SPSS has essentially tumbled off a cliff in academic and even enterprise usage. More important for a new (or any) researcher, fair or not, and despite what SPSS or similar programming skill one might have, using it is basically stating in your article that ‘we probably screwed up somewhere, but we don’t know because we spent all our time repeatedly clicking on the same things and didn’t have time or tools for adequate data processing, exploration and visualization’. Probably most telling is that the person who created SPSS eventually jumped ship for R himself52. The take home messages here are: that you, yes you, can go from a spreadsheet-loving menu-clicker to a ‘decent-enough’ programmer to take your research to the next level, and secondly, that at least some people with mullets can be trusted to provide useful information53. Anyway, since my initial foray into statistical programming, I’ve consulted at three institutions, taught courses/classes at the graduate and undergraduate levels in multiple departments, consulted with people from several dozen departments and disciplines, participated in institutional research, given workshops etc. What’s more, I’ve gotten to hang out and share experiences with people who do the same. This is not to brag, an activity I detest, but to make a point. In all those experiences I’ve yet to come across an individual in academia who couldn’t do what’s suggested in this document. I’ve certainly seen those that have zero desire to do so, and want to do the bare minimum required for their degree, or to get published, but if you’re reading this, you aren’t one of those. And that’s what it comes down to primarily, desire, and secondly, time, which you won’t necessarily have copious amounts of as a graduate student, who has to manage classes and other aspects of life, or a faculty or other applied researcher, who may have courses, article writing, and a host of administrative duties to contend with. Finding the balance will be one of the more difficult challenges you’ll have54. It can be done though, and many do it all the time. I’m still trying to figure out why MANOVA is being taught for any reason in 2017, but I know that it is.↩ As long as you weren’t a minority, ignored the economy, and didn’t mind being taught to hide under your desk in case a nuclear bomb was dropped on your head. I admit that this is a U.S. centric recollection. I’m sure the Canadians were fine for example, though they didn’t learn to flip their collars for a couple more years.↩ Deloreans actually ceased production in 1983, but that was mostly because almost every household that could had purchased at least one and the market was exhausted. The streets were literally cluttered with them, and many people would even leave them in parking lots with wings open, hoping someone would steal it.↩ Insert Lazerhawk and/or Kavinsky here.↩ Not that learning SPSS syntax would have been a good way to spend one’s time.↩ I’m not kidding. SPSS syntax was developed during the punch card mainframe era of the 1970s. The first release was actually 1968. It has slightly more functionality now, though the graphics are about the same.↩ Despite what you may see in your discipline, this is an indisputable fact at this point. Your discipline is not statistics, computer science or other fields driving methodological development.↩ Perhaps surprisingly, he did not subsequently claim to have invented R.↩ At least those with one like Kenny Powers from Eastbound &amp; Down.↩ I’ve never understood the motivation behind faculty who request their graduate students do a power analysis for their dissertation or thesis. Want to know how many folks you need for your study? Here’s the more appropriate question: When do you want to graduate?↩ "],
["misc.html", "misc", " misc Her name was Shady Sides It’s been evenin’ all day long How can something so old, be so wrong? Sin and gravity Half hours on earth Fifty thousand Ash in your shoes Always use the old sense of the words Your third drink will lead you astray On the last day of your life, don’t forget to die Don’t believe in people who say it’s all been done Key ideas: general: apply to stats/programming/research generally compartmentalization, get a book, reproducibility, simple to complex, focusing more on vis, embracing uncertainty but avoiding relativism, flexibility basics: pretty much needed to understand basic stats and programming a few concepts to get you far, loops, minimizing and maximizing, explanation vs. prediction, data processing, downplaying nhst, regularization, coding style, naming conventions, visualization basics going further: for the really motivated functions, reusable code, debuggin’ de bugaloos, building a brand Here’s a smattering of articles and books from at least 20 years ago and well beyond55. Classification and regression trees. L Breiman J Friedman CJ Stone RA Olshen 1984 Induction of decision trees J R Quinlan 1986 Bagging predictors L Breiman 1996 Literate programming DE Knuth 1984 The jackknife, the bootstrap and other resampling plans B Efron 1982 A tutorial on hidden Markov models and selected applications in speech recognition L R Rabiner 1989 Convolutional networks for images, speech, and time series Y LeCun Y Bengio 1995 Support-vector networks C Cortes V Vapnik 1995 A simple neural network generating an interactive memory JA Anderson 1972 Generalized additive models TJ Hastie RJ Tibshirani 1990 A desicion-theoretic generalization of on-line learning and an application to boosting Y Freund RE Schapire 1995 Probabilistic reasoning in intelligent systems: Networks of plausible inference J Pearl 1995 Maximum likelihood from incomplete data via the EM algorithm AP Dempster NM Laird DB Rubin 1977 Bayesian data analysis A Gelman JB Carlin HS Stern DB Rubin 1995 An Essay towards solving a Problem in the Doctrine of Chances T Bayes 1763 Random forests, deep learning, the ideas for modern tools like Jupyter notebook and Rmarkdown, the Bayesian approach… some of it was old hat a long time ago. Despite that, one can regularly find today, and in many disciplines and at top institutions, instructors still teaching material that largely ignores 50 years of methodological advances, and actively entrenching poor practices that have been railed against for almost 100 years. Consider the following references that suggest that the standard null hypothesis testing approach typically employed might be problematic. Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test. J Berkson 1938 “The statistical folkways of a more primitive past continue to dominate the local scene.” Rozeboom 196056 “As a second example, consider significance tests. They are also widely overused and misused.” Cox 1977 “Are the effects of A and B different? They are always different, for some decimal place.” Tukey Those are some very old quotes. Unfortunately a lot of you may have to justify your use of techniques that are very appropriate to your research question "]
]
