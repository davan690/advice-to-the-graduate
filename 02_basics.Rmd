# Part II: Perfect Sound Forever

The goal here is not expertise, but primarily conceptual awareness and understanding.  For example, you don't have to know much about machine learning approaches other than why you might use them.  You can then learn more as needed when the time comes, if it even does.  For programming, it literally could be two lines of code to import your data and run a statistical model. But the data is *never* ready, and data processing will likely be where you spend the vast majority of your time.  However, knowing even a few things about basic programming will help you spend a tremendous amount less regarding what you *have* to do, but probably more of things you *want* to do.

Some motivation.  Here are some things I've seen:

- A social science grad student first learning R/programming at the beginning of the semester, that within a few months was a RStudio keyboard shortcut whiz, and was taking some Python workshops to boot.

- Another social science student using efficient programming practices to analyze big data on a high performance computing cluster.

- Architects, English Lit faculty, Staff librarians.  I have seen them all do basic programming and statistics to even some advanced stuff.

- I have seen 50+ years old SPSS folks get into machine learning.

It obviously isn't easy, but anyone can do it.  You'll likely be surprised at just what eventually does becomes easy, but be prepared for regular stumbles along the way, no matter what level you're at. As you improve, you'll just be at another level where the same amount of stuff will trip you up, only more advanced.  Case in point, here is the most common phrase I utter when programming:

<span class="" style="font-size:125%; font-variant:small-caps; color:#B2001D">~ You've got to be @%*^!ing kidding me!! </span>

Once you get good at some things, you'll still hit roadblocks, be surprised, or just make stupid mistakes.  It's just part of the journey, and mostly just means you're doing something interesting. If it helps, think of it as an RPG where you're leveling up, only, you'll probably be the only one to tell you that you've leveled up[^bob].


 
## Analytical

### Explanation vs. Prediction: a Starting Point

<h6> *A narrative of startling interest!!* </h6>

One way to look at the analytical endeavor is in terms of two goals, not mutually exclusive.  On the one hand we are interested in theory confirmation and comparison. Theory suggests certain variables have relations to other variables.  We set up mathematical models that convey the theory in a testable form, and collect data corresponding to those variables. Via a a particular framework, e.g. null hypothesis testing or Bayesian, we can say something about the statistical notability of those relationships.  The ultimate goal is to tell a story about the results and their relation to the theory, possibly suggesting exceptions and modifications.  In this approach, <span class="emph">explanation</span> is the focus.

On the other side, we need something that is practically viable.  The goal is not so much to tell a story as it is to get the best possible <span class="emph">prediction</span> from our models.  Techniques may be used that will tell us little about which predictors are best, or even why some predictors work better or what is even the primary feature of the data being learned.  However, not only does the result work well, such approaches almost always outperform the explanatory ones in practice. 

To make things a little more concrete, we can consider linear regression vs. deep learning.  With the former we can obtain highly interpretable coefficients, statistical significance, interval estimates, etc.  It is extremely easy to tell a story with the results. With deep learning on the other hand, it's difficult to tell what's being learned, or what features are being latched on to.  However, assuming it's applied appropriately[^deeplearnworks], it works extremely well!


Most applied disciplines seem to focus exclusively on explanation in their statistical education.  You might think this is not too big of an issue, but to see the consequences of this, one only has to look at the current 'replication crisis'[^repcrisis].  You've now had nearly a century of results interpreted solely in terms of statistical significance in the null hypothesis testing framework, with typically little to no focus on whether the damn model even works.  As a specific example, you can have statistically significant predictors in a linear regression and an adjusted R^2^, which would tell you how much variance in the outcome you can explain with the predictors, of *negative value*!  The following demonstrates this.  I construct completely random data, i.e. none of the $X$ has any relation to $y$, and use it in the model.

```{r badmod, echo=2:4}
set.seed(0)
N = 250
X = matrix(rnorm(N*10), ncol=10)
y = rnorm(N)

# summary(lm(y ~ ., data.frame(X)))
# broom::tidy(mod) %>% mutate_at(vars(-term), round, digits=3) %>% DT::datatable(options=list(dom='t', pageLength=11), rownames=F)
pander::pander(summary(lm(y ~ ., data.frame(X))), justify='lrrrr', round=3)
```
<br>
Yet you will find entire papers written about that one coefficient, with p-value slightly less than .05[^phack], while in terms of prediction it literally does worse than guessing the mean would with new data. Quite simply, this sort of practice has got to stop.

The key idea is <span class="emph">balance</span>. For most applied disciplines explanation is not only required, but to be focused on.  This is fine. Some people do researcher where collecting one observation might take a few days of effort.  This too, is okay. However, complicated models with little data require something more than statistical significance- the results bounce around too much.  But for any modeling endeavor, predictive validity is also important.  You simply cannot focus on the statistical (significance) story while entirely ignoring the practical value of the model.

#### A Word about Effect Size

Some disciplines, e.g. psychology, try to get their adherents to focus on effect size.  Reviewers regularly request it, but also are regularly non-specific about the metric, or if they do mention it, make clear they have no idea what they're talking about.  For example, one of the more popular effect sizes is <span class="emph">Cohen's d</span>, which is a standardized difference in two group means (e.g. control vs. treatment). If you are doing a t-test as your primary model and haven't gone back in time, then I can assure you no one is going to be citing your work regardless of your effect size.  Here are some of the issues.

- Without a corresponding interval estimate the effect size is of little value.
- As soon as you get into any complication, e.g. repeated measures, interactions, etc., there is little standard or possibly even a viable way to define it.
- Small data and large effects mean nothing other than that you need more data.  It is *definitely not* the case that the effect is 'more real' because you happened to see it with such a small sample.

Another common 'effect size' is the standardized regression coefficient.  You don't see this much outside of disciplines where SPSS is used a lot. However, even in the only setting where it's easily calculated, it still doesn't make comparable categorical effects vs. numeric ones. And as soon as you move beyond the standard linear model, it's pretty much not defined.  Also, what's the effect size here?

```{r effectgam, echo=FALSE}
library(mgcv)
data(mcycle, package="MASS")
mod = gam(accel ~ s(times), data=mcycle)

library(plotly)
mcycle %>% 
  modelr::add_predictions(mod) %>% 
  plot_ly(x=~times, y=~accel) %>% 
  add_markers(marker=list(color='#ff5503'), showlegend=F) %>% 
  add_lines(x=~times, y=~pred, line=list(color='#03b3ff'), showlegend=F) %>% 
  theme_plotly()
```

<br>

If you're dealing with interesting data and modeling, there is little chance there is an easily identified 'effect size'.  Instead, you'll need to know your measures well and think hard about what you're seeing.  If you know the target variable well, effects in terms of predicted values under a variety of data settings will be far more enlightening.



### The Standard Linear Model

Everything starts with the <span class="emph">standard linear regression model</span>.  To paraphrase @shalizi2017advanced, it's relatively simple[^simplerelative], it can do a decent enough job in a lot of situations, and it's a standard to the point it's essentially a tool of communication.  One of the more important aspects of it is that it serves as the foundation for everything else.

Simply put, you need to get to a point where practically nothing about what you get from it in standard settings should be a mystery to you.   Coefficients, standard errors, residuals, fitted values, prediction, how categorical predictor variables are used, residual standard error, interpreting interactions, model comparison.  It's also good to know what sorts of things are important (how you specify the model) and not so much (e.g. normality assumption).

If you don't understand standard regression, you'll be hard pressed to understand little else methodologically.  One additional benefit is that knowing it will take care of the special cases like ANOVA and t-tests, so you won't have to spend much time with those unless you just want to.

### Classification

The second most commonly used model after the standard linear model is probably <span class="emph">logistic regression</span>, where one predicts a binary target (e.g. yes-no).  Standard logistic regression, like the standard linear model, is a special case of the <span class="emph">generalized linear model</span>.  

This gets you beyond <span class="emph">ordinary least squares methods</span> and into <span class="emph">maximum likelihood</span>. We'll return to this later.  In addition to standard fare like coefficients and their standard errors, you'll get two types of predictions, probability of being in one of the categories, or the actual classification.  One can look at many metrics of performance with logistic regression, for example, accuracy, sensitivity, and specificity. Most only look at accuracy which is not a good way to do things. Actually, many don't even do that. You'll often see papers going on and on about the statistical significance of predictors even though their actual prediction is no better than guessing the most common category.


Even if you don't think you'll use it for your current work you'll come across papers that use it, and you will use it at some point if you do enough statistical modeling.  Furthermore, getting used to logistic regression can serve as a first step to other GLM models (e.g. <span class="emph">poisson regression</span>), and other categorical models, e.g. ordinal and multinomial.  If you do use it, interpret it in terms of actual predictions at key values of the predictors[^margins] rather than statistical significance and odds ratios.  Not only are predicted probabilities easy to interpret, it's pretty much the only way to interpret interactions and other nonlinear effects.

### Unsupervised Methods

Along with basic regression and logistic regression, you'll need some conceptual understanding of what are typically referred to as <span class="emph">unsupervised methods</span>.  Here, we are exploring structure more than predicting outcomes (though we could).  Probably the most common among these are PCA, factor analysis, and k-means cluster analysis.  I would suggest just getting a handle on why you'd use them, and be prepared for needing to implement them or similar variants for your work, because they are used a lot. Some of these are taught in classes on <span class="emph">multivariate analysis</span>, which is an utterly useless name, but those classes often include things like MANOVA, LDA, canonical correlation and others that have no place in modern methodology. However, I can say I do like name dropping canonical correlation, which MANOVA, LDA, and standard regression are special cases of, every once in a while just to befuddle people.


### Loss functions and maximum likelihood estimation

In statistical modeling we are interested in estimated parameters. For example, these might be regression coefficients or variances. But how do we do it?  We certainly don't just guess, and we definitely don't use magic, so where do the numbers come from?

The answer is that we pick values that bring about the best result.  But how do we define *best*? Well there is no one way to do it, and how you might do it depends on the situation.  However, there are a couple common approaches you should understand conceptually very well.

Let's begin with ordinary least squares. If this is our optimizing approach, then we are trying to minimize the squared residuals in prediction.  For classification, it could simply be that we want to minimize *mis*-classification errors.

Probably the most common method goes the other way.  Instead of minimizing errors in prediction, we seek parameters that maximize the likelihood of seeing the observed data.  This method, <span class="emph">maximum likelihood estimation</span>, is the most common way we estimate parameters in statistical modeling, and has equal importance in the Bayesian framework too.  Sometimes the estimates that might minimize the loss function may also be the maximum likelihood estimate. This is the case in standard linear regression, but it would not be in other settings.  The primary point is, knowing the <span class="emph">objective function</span> for your analysis will help it not seem so mysterious. There are many optimization algorithms, and while you don't need to know the details of them, do know that you may get errors because their default settings don't work for your analytical situation.

Many applied statistical courses will spend time almost exclusively on ordinary least squares techniques, for which they might not even talk about estimation at all, leaving the students to basically believe in magic when they do statistical analysis. Some will even report that they used maximum likelihood, but this is only because they know that is the default.  For your own sake, don't bluff this.

With R or Python, you can literally write your own regression function via least squares or maximum likelihood in one or two lines of code. I strongly suggest you do this, even if only once, just to drive the point home.  I can also say it is quite satisfiying when something you put together reproduces the output you expect from the statistical program.


### Problems with NHST

I'm sick and tired of talking about the issues with <span class="emph">null hypothesis testing</span>, and I think the statistician types in general are.  Its problems have been noted since it was first proposed, and people in both methodological and applied disciplines have been raising the issues for over almost a century at this point.  It's annoying for me to even think about it, so if you want details, I won't be rehashing them here, but you won't have to look far on the web to find them.

Assuming you're even using the approach correctly (almost never the case in applied research), just about every other statistic that falls out of an analysis is more important than the p-value. Yet I have seen people torture data, change their theories, ignore important findings etc. becuase of it.  Researchers still doing this are not doing important work, because almost by definition it's work that won't stand the test of time, probably even the second time.  They are wasting their own and others time in general.

Statistical analysis will not provide you a hard answer or truth, and claiming something is 'significant' doesn't make it so.  Go in with competing models, even if they are simpler ones vs. more complex, or theory-driven vs. purely exploratory.  Spend time expressing your data visually.  If you do you'll regularly have something interesting to talk about regardless of any p-value.


*Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test*. J Berkson **1938**

"The statistical folkways of a more primitive past continue to dominate the local scene." Rozeboom **1960**[^bestquote]

"As a second example, consider significance tests. They are also widely overused and misused." Cox **1977**

"Are the effects of A and B different? They are always different, for some decimal place." Tukey **1991**

That first one is an article title, but otherwise those are some very old quotes.  

### Overfitting


### The Bayes Way


### Machine Learning Basics

Any applied discipline that teaches a semester course in statistical analysis should have a week of it devoted to an overview of machine learning.  The only reason you may not see it much in your discipline is due to 1. your discipline never teaches it, or 2. people outside of your discipline are doing the interesting things for your discipline, and publishing elsewhere.  I can tell you however, that it is useful, you may need to employ it some day, or at the very least, you shouldn't ignore articles using it because you don't know what they're talking about conceptually.

Data can be big, complex, or both, and there are techniques to handle it if so.  If you don't know about them, you may be inclined to do some very silly things to coerce the situation to one you know, which is precisely the wrong thing to do.  If your discipline doesn't cover it in any fashion, do a workshop, look online for some exercises, or simply schedule some time to chat with someone who does know about them.


### Visualization

### Learn more

- regression and classification (check)
- explanation vs. prediction (check)
- unsupervised methods (check)
- loss function (check)
- maximum likelihood (check)
- problems with NHST (check)
- overfitting v.s - generalization
- bayesian basics
- ML basics
- visualization heavy

- what is needed further for your specific projects
    - learn more about whatever technique your using


## Programming

If you're doing serious scientific research, these days you're going to have to learn some programming[^notstartrek].  Don't want to? That's okay, just do something else besides scientific research.  Even if you're doing relatively simple anlaysis, a lot of data processing will have to take place beforehand to ensure that whatever results you obtain have integrity.  Unless you like to spend several weeks on something that could take a couple hours with just basic programming skills, you'll need to spend some time in this area.


One of the first things to note with statistical programming is, that once things are set up, it is as easy to run a model in R or Python as it is menu-driven programs like SPSS. It even requires *less* syntax than you would find with, e.g. SPSS and SAS.  Compare the following two approaches, one a menu-driven approach in SPSS and another programming approach in R.


- SPSS

```
Click File
Open
hunt for file on computer
wait a few seconds for SPSS to open its own file type
Click Analyze
Click/hover Regression
Click Linear Regression
Search for and Click your dependent variable (or click, start typing the name, and hope)
Search for and Click each 'independent' variable
Click on various options.
Click ok
```

- R

```{r, eval=F}
d = haven::read_spss('filelocation/file.ext')
model = lm(DV ~ x1 + x2 + x3, data=d)
```

There is no debate here, one is vastly more efficient and simply easier.  In fact, with RStudio's autocompletion, the odds are that the first line probably only required a few keystrokes. You may have to learn that you can use the <span class="pack">haven</span> package for importing SPSS files, or the <span class="func">lm</span> function to do regression, but that's very quickly picked up, and less than you would have had to learn regarding the SPSS menus, which have remained mostly unchanged and in their disorganized state for over 20 years.  The other nice thing is that you can redo the above in two steps with Ctrl+A (select all) and Ctrl+Enter (run)[^pyspss].




### R & Python

<div class='col2'>
<img src="img/Rlogo.svg" style="display:block; margin: 0 auto;" width='50%'>
<br>
<img src="img/py.svg" style="display:block; margin: 0 auto;" width='75%'>
</div>
<br>
When it comes to statistical programming and computation, there are two primary choices these days, but you should be flexible, because the situation will likely change at some point in your career.  <span style="color:#1f65b7">**R**</span> and <span style="color:#FFDC52">**Python**</span> are the primary tools of statistics/data science, and have pretty much overtaken both enterprise and academia.  They are both easier to program for data processing and analysis than they were even just a few years ago, and notably more so than they were in the early days of their use for modeling.  It doesn't really matter which you use.  Python will have more power when it comes to machine learning, potential enterprise applications, natural language processing, and, in general, speed.  R is still notably easier to use, and has far more developed packages for a far wider range of techniques.  However, these distinctions melt by the wayside more and more everyday. So you're fine with either.



#### Alternatives

##### Other Programming languages and related

Lower level programming languages such as Java, Fortran and C++ are not required knowledge for the vast majority of statistial and related application.  They are primarily used when you need to get the most speed you can, and you'll be able to find someone on campus to help you there, for example, in your research support group.

Matlab, Julia, and Mathematica are other tools or languages you might come across that are highly capable, but are either proprietary (i.e. require a renewable license, cost money, and don't share source code), or don't yet offer much of anything beyond what you can get with Python and R, and usually far less.

You will also find languages and frameworks within languages such as Scala, Spark, Hadoop, Keras, Torch, TensorFlow, SQL, Lua and so forth. Most of these have specific purposes, e.g. databases, or don't need to come into play unless you have massive data (e.g. TensorFlow).  Don't bother unless needed.

In addition, people are writing analytical functionality in things like javascript, Haskell and other things that weren't meant for it nor are all that good at it. I don't know why.

##### Statistical or similar 

The primary statistical programs in academia are SAS, SPSS and Stata.  Unlike the others, Stata use is actually growing, and they also have a great community. The other two are dwindling fast in academia, especially, and thankfully, SPSS.  Aside from those, there are relics, money-grabs, and WTFs of all kinds. Seriously, there is some ridiculous software out there that some poor folks in some specific niche of a discipline got suckered into using a long time ago, and for whatever reason still do.  If you're using non-open source software whose functionality is completely matched and bettered by others that are freely available, you'd better be able to justify it in some way, and I can't think of anything you could come up with.

If you use something like SAS or Stata, you still can't use the menus for analysis.  Aside from being inefficient, it's not reproducible, and thus not a way to do science.

Excel is not an option, ever.  I would say you could use it for data entry, but it's very problematic even then. Avoid at all costs.

If you don't want to take my word for it here are some other things to peruse.

[Link 1](http://spectrum.ieee.org/computing/software/the-2016-top-programming-languages)
[Link 2](https://www.tiobe.com/tiobe-index/)
[Link 3](http://r4stats.com/articles/popularity/)
[Link 4](http://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html)

### IDEs

After installing the basic components for R or Python, you can use them straight away. However, this would be silly, as they don't come with a way to use them that enables basic functionality or is easy.  If your R console comes with retina-bleeding red-colored font, or you're using the clown-colored IDLE from Python, just shut them down and walk slowly away.

You want to use what's called an <span class="emph">integreted development environment</span>, or IDE.  For R, the far and away most popular one is RStudio. For Python, I would suggest Anaconda for scientific work.  There are others, but unless you have a lot more experience, I'd suggest sticking with those.

What these do, practically speaking, is make your coding efforts a lot more easy and efficient. Syntax highlighting, code completion, interactivity... this is only the beginning.  I've written this document and many other things *entirely* with RStudio.  A good IDE is essential.


#### customize it

After you initially start to get the hang of your IDE, customize it. I'm always struck by the fact that people don't change the basic settings in their software, which are often chosen by people who should not have any say in design whatsoever[^baddesign].  .



### coding <span class="font-variant:small-caps; font-family:'Risque'">style</span>



<img src="img/code_quality.png" style="display:block; margin: 0 auto;" width=200%>
<br>
<img src="img/code_quality_2.png" style="display:block; margin: 0 auto;" width=50%>
<br>
<img src="img/code_quality_3.png" style="display:block; margin: 0 auto;" width=150%>
<br>


Coding style matters, but not in the way that many think.  


<span class="" style="size:125%; font-variant:small-caps">The only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code.<br> ~ Hadley Wickham</span>

Wickham is right on the first part, wrong on the second.  No one writes perfect code, *no one*.  It takes effort just to write halfway decent code.  That's fine.  You won't be writing good code for quite some time. That's okay too.

However, assuming you want others to be able to use your code, you should feel bad giving them shitty code, because you'll be wasting their time.  Going back to one of our initial suggestions, *Don't be a jerk*. Write better code, always. Time and other resources will always be working against you, but do the best you can.

The first step is to start with a coding style.  For example, both [Google](https://google.github.io/styleguide/Rguide.xml) and [Hadley](http://adv-r.had.co.nz/Style.html) have provided an R coding style for others to follow.  If you only followed those, your code will be great relative to what you would have done.  But realize they are only suggestions, some are based on decades of coding practice, others are based on whim, and they don't tell you which.

For example, the Google says using underscores for variable names, e.g. `this_is_a_variable`, are 'bad', while capital first camel casing is 'good', e.g. `ThisIsAVariable`.  It's very clear which is more readable, and just as easy to type. Furthermore, with RStudio, once the object is created, you only have to type a couple letters to complete the rest.  Both Wickham's and Google's style guide consistently contradict one another and themseleves, and they're both based on more experience than you'll ever have, so that should give you some sense that it isn't exactly easy.

The point is, there is no authority here, but there are better approaches than your nospace, semi;coloned, naming-things-'data', everything-in-a-single-script attempts will come up with, I guarantee you.  In coding, just trying to do better will make your code better[^github].



- errors as calls for help

- I/O

- basic object types
    - boolean
    - numeric of a kind
    - factor/categorical

- data structures
    - vectors
    - matrices/arrays
    - data frames
  
- indexing

- iterative programming
    - loops/while

- aggregation
  


## Reproducibility revisited

### Data 

(SHOULD THIS BE IN NEXT SECTION?)

Before we get to analysis, one needs to understand that 

> *everything begins and ends with the data*.  

If you don't collect the data carefully, it doesn't matter what else you do, despite what the folks in the Engineering department tell you. You've completely wasted your own time and resources, along with that of everyone else who is affiliated with the project.  If you're not willing to get your hands dirty with the data and make sure it's collected properly, you simply shouldn't be doing research.

Naming, labels, missing data etc.

### Analysis

For yourself, you should be able to load model-ready data, run the analysis on it, and reproduce the same result you intend to publish.  In addition, you might apply the idea of <span class="emph">convergent validity</span> to the process. Similar analytical techniques should come to the same conclusions.  And finally, give someone an outline of the necessary steps to take, and see if they can reproduce the same results.

### Programming

A starting point in your programming is to comment everything.  The comments should not be about what the code is doing. This is because if you know the language at all you'll know what it's doing, or if you forget, it will be easy to look up what the function or command.  Instead, comment about *why* you're doing it.

- Naming things
    - files
    - variables
    - data
    - objects
    
    "There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors."

## Other



### Other



  
- Documents
    - Latex or markdown

- Version control basics


[^deeplearnworks]: In other words, a ridiculous amount of data, with 'clean' data, and a whole host of other things in place.

[^phack]: I'm not even going into the fact that they probably finagled the data 20 different ways with umpteen models to get that p-value slightly less than .05, a practice called <span class="emph">p-hacking</span>.  People who do this engage in the worst type of anti-science.

[^bob]: Should you need assistance with knowing whether you have leveled up or not, there is someone in the IT department at LSU named Robert Gill III Esq. that can help you.  You cannot provide specifics, only ask, 'Have I leveled?'. He will then respond with a series of questions that will, in one way or another, probe the essence of your being, and provide the only answer you will ever need. Sprinkled with quotes from Buckaroo Bonzai, Goonies, and for matters of import, Forrest Gump.


[^simplerelative]: If you're just starting out, it probably isn't to you, but we're talking *relatively*. It'll be old hat to you as well at some point.

[^margins]: Sometimes called marginal effects.

[^notstartrek]: At least until the ideas of Star Trek become a reality.  And don't let the folks in the Bay Area fool you, it's still a very long way off.  For some perspective, at the turn of the 20^th^ century, there were no cars as we now know them other than demonstrations and prototypes. By World War I, less than 20 years later, cars were a common sight in cities, and airplanes were being used in combat. We've now had more than four decades of combined internet and personal computing capabilities, and probably the biggest difference between now and then is that people spend a lot of time staring at tiny screens in their hand.  Most of the stuff we're just now getting to was promised to be right around the corner during the space age of the 1950s and 60s.  The point is, a lot of this stuff will come, but don't hold your breath.

[^baddesign]: Just look at gmail, Facebook, Amazon.  They are essentially the anti-Tron of user esteem.  Amazon in particular seems to simply vomit out the return of a search that mildly references some of the letters you actually typed into its engine, if they used Google translate on the query first, while assuming the native language was an interpretive dance in Kabuki theater.  The 'art' of basic drilldown is a completely lost enterprise.  Using that website almost makes you want to ask Alexa for the best form of suicide.

[^github]: My main motivation for getting on GitHub was making my code better.  If the only reason you use it is that the thought of others looking at your code will make you write better code, then definitely do so.

[^bestquote]: You may try, but you will never, *ever* find a better quote in all of statistics.  Also, it's generally expected that when uttered, those in earshot must snap at least twice and say 'Right on, daddy-o'.

[^pyspss]: I was going to do a Python example too, but I couldn't find an example that wouldn't take 5 or more lines not counting the import lines, which would be several more.  

[^repcrisis]: It is *current* if you're not aware of statistical history of the past 60+ years.  The only thing that's different from past calls to do better work is people's ability to complain on social media.  Applied researchers will do no more methodologically than what is expected, which seems largely defined by whatever they come across in the few journals they spend time with.  This is not necessarily right or wrong, just the way it is.