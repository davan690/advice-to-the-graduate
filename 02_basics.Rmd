# Part II: Perfect Sound Forever

The goal here is not expertise, but primarily conceptual awareness and understanding.  For example, you don't have to know much about machine learning approaches other than why you might use them.  You can then learn more as needed when the time comes, if it even does.  For programming, it literally could be two lines of code to import your data and run a statistical model. But the data is *never* ready, and data processing will likely be where you spend the vast majority of your time.  However, knowing even a few things about basic programming will help you spend a tremendous amount less regarding what you *have* to do, but probably more of things you *want* to do.

Some motivation.  Here are some things I've seen:

- A social science grad student first learning R/programming at the beginning of the semester, that within a few months was a keyboard shortcut whiz, and was taking some Python workshops to boot.

- Another social science student using efficient programming practices to analyze big data on a high performance computing cluster.

- Architects, English Lit faculty, Staff librarians.  I have seen them all do basic programming and statistics to even some advanced stuff.

- I have seen 50+ years old SPSS folks get into machine learning.

It obviously isn't easy, but anyone can do it.  You'll likely be surprised at just what eventually does becomes easy, but be prepared for regular stumbles along the way, no matter what level you're at. As you improve, you'll just be at another level where the same amount of stuff will trip you up, only more advanced.  Case in point, here is the most common phrase I utter when programming:

<span class="" style="font-size:125%; font-variant:small-caps; color:#B2001D">~ You've got to be @%*^!ing kidding me!! </span>

Once you get good at some things, you'll still hit roadblocks, be surprised, or just make stupid mistakes.  It's just part of the journey, and mostly just means you're doing something interesting. If it helps, think of it as an RPG where you're leveling up, only, you'll probably be the only one to tell you that you've leveled up[^bob].


 
## Analytical

### Explanation and Prediction: a Starting Point

<h6> *A narrative of startling interest!!* </h6>

One way to look at the analytical endeavor is in terms of two goals, not mutually exclusive.  On the one hand we are interested in theory confirmation and comparison. Theory suggests certain constructs have relations to other constructs  We set up mathematical models that convey the theory in a testable form, and collect data corresponding to those variables. Via a a particular framework, e.g. null hypothesis testing or Bayesian, we can say something about the statistical notability of those relationships.  The ultimate goal is to tell a story about the results and their relation to the theory, possibly suggesting exceptions and modifications.  In this approach, <span class="emph">explanation</span> is the focus.

On the other side, we need something that is practically viable.  The goal is not so much to tell a story as it is to get the best possible <span class="emph">prediction</span> from our models.  Techniques may be used that will tell us little about which predictors are best, why some predictors work better, or what is even the primary feature of the data being learned.  However, not only does the result work well, such approaches almost always outperform the far more explanatory ones in practice. 

To make things a little more concrete, we can consider linear regression vs. deep learning.  With the former we can obtain highly interpretable coefficients, statistical probabilities, interval estimates, etc.  It is extremely easy to tell a story with the results. With deep learning on the other hand, it's difficult to tell what's being learned, or what features are being latched on to.  However, assuming it's applied appropriately[^deeplearnworks], it works extremely well!


Most applied disciplines seem to focus exclusively on explanation in their statistical education.  You might think this is not too big of an issue, but to see the consequences of this, one only has to look at the current 'replication crisis'[^repcrisis].  You've now had nearly a century of results interpreted solely in terms of statistical significance in the null hypothesis testing framework, with typically little to no focus on whether the damn model even works.  As a specific example, you can have statistically significant predictors in a linear regression and an adjusted R^2^, which would tell you how much variance in the outcome you can explain with the predictors, of *negative value*!  The following demonstrates this.  I construct completely random data, i.e. none of the $X$ has any relation to $y$, and use it in the model.

```{r badmod, echo=2:6, eval=-6}
set.seed(0)
N = 250
X = matrix(rnorm(N*10), ncol=10)
y = rnorm(N)

summary(lm(y ~ ., data.frame(X)))
# broom::tidy(mod) %>% mutate_at(vars(-term), round, digits=3) %>% DT::datatable(options=list(dom='t', pageLength=11), rownames=F)
pander::pander(summary(lm(y ~ ., data.frame(X))), justify='lrrrr', round=3)
```

<br>

One term has a p-value of `r round(summary(lm(y ~ ., data.frame(X)))$coefficients['X10', 'Pr(>|t|)'], 3)`, but we know this is meaningless. Yet you will find entire papers written about that one coefficient, with p-value slightly less than .05[^phack], while in terms of prediction the model literally does worse than guessing the mean would with new data. Quite simply, this sort of practice has got to stop.

The key idea is <span class="emph">balance</span>. For most applied disciplines explanation is not only required, but to be focused on.  This is fine. Some people do research where collecting one observation might take a few days of effort.  This too, is okay. However, complicated models with little data require something more than statistical significance- the results generally will have too much variability associated with them.  But for any modeling endeavor, predictive validity is also important.  You simply cannot focus on the statistical (significance) story while entirely ignoring the practical value of the model.

#### A Word about Effect Size

Some disciplines, e.g. psychology, try to get their adherents to focus on effect size.  Reviewers regularly request it, but also are regularly non-specific about the metric, or if they do mention it, make clear they have no idea what they're talking about.  For example, one of the more popular effect sizes is <span class="emph">Cohen's d</span>, which is a standardized difference in two group means (e.g. control vs. treatment).  It's applicable in cases where the primary goal is a single group comparison, e.g. via a t-test.  However, if you are doing a t-test as your primary model and haven't gone back in time, then I can assure you no one is going to be citing your work regardless of your effect size.  Here are just some of the issues.

- Without a corresponding interval estimate the effect size is of little value.  That's right, the effect size has associated uncertainty with it, just like everything else.
- As soon as you get into any complication, e.g. repeated measures, interactions, etc., there is little standard or possibly even a viable way to define it.
- Small data and large effects mean nothing other than that you need more data.  It is *definitely not* the case that the effect is 'more real' because you happened to see it with such a small sample. In fact, your first move after seeing a large effect in a small sample should probably be to go back to the data processing stage to make sure no errors were introduced.

Another common 'effect size' is the standardized regression coefficient.  You don't see this much outside of disciplines where SPSS is used a lot. However, even in the only setting where it's easily calculated, it still doesn't make comparable categorical effects vs. numeric ones. And as soon as you move beyond the standard linear model, which is where most models should be these days, it's pretty much not defined.  For example, what's the effect size here?

```{r effectgam, echo=FALSE}
library(mgcv)
data(mcycle, package="MASS")
mod = gam(accel ~ s(times), data=mcycle)

library(plotly)
mcycle %>% 
  modelr::add_predictions(mod) %>% 
  plot_ly(x=~times, y=~accel) %>% 
  add_markers(marker=list(color=palettes$orange$complementary[1]), showlegend=F) %>% 
  add_lines(x=~times, y=~pred, line=list(color=palettes$orange$complementary[2]), showlegend=F) %>% 
  theme_plotly()
```

<br>

If you're dealing with interesting data and modeling, there is little chance there is an easily identified 'effect size'.  Instead, you'll need to know your measures well and think hard about what you're seeing.  If you know the target variable well, effects in terms of predicted values under a variety of data settings will be far more enlightening.



### The Standard Linear Model

Everything starts with the <span class="emph">standard linear regression model</span>.  To paraphrase @shalizi2017advanced, it's relatively simple[^simplerelative], it can do a decent enough job in a lot of situations, and it's a standard to the point it's essentially a tool of communication.  One of the more important aspects of it is that it serves as the foundation for everything else.

Simply put, you need to get to a point where practically nothing about what you get from it in standard settings should be a mystery to you.   Coefficients, standard errors, residuals, fitted values, prediction, how categorical predictor variables are used, residual standard error, interpreting interactions, model comparison.  It's also good to know what sorts of things are important (how you specify the model), and not so much (e.g. normality assumption).

If you don't understand standard regression, you'll be hard pressed to understand little else methodologically.  One additional benefit is that knowing it will take care of the special cases like ANOVA and t-tests, so you won't have to spend time learning those unless you just want to.

### Classification

The second most commonly used model after the standard linear model is probably <span class="emph">logistic regression</span>, where one predicts a binary target (e.g. yes-no).  Standard logistic regression, like the standard linear model, is a special case of the <span class="emph">generalized linear model</span>.  

This gets you beyond <span class="emph">ordinary least squares methods</span> and into <span class="emph">maximum likelihood</span> estimation explicitly. We'll return to this later.  In addition to standard fare like coefficients and their standard errors, you'll get two types of predictions, probability of being in one of the categories, or the actual classification of being in one category or the other.  One can look at many metrics of performance with logistic regression, for example, accuracy, sensitivity, and specificity. Most only look at accuracy, which is not a good way to do things in many scenarios. Actually, many don't even do that. You'll often see papers going on and on about the statistical significance of predictors even though their actual prediction is no better than guessing the most common category.  Again, there is explanation and then there is prediction.


Even if you don't think you'll use it for your current work you'll come across papers that use it, and you will use it at some point if you do enough statistical modeling.  Furthermore, getting used to logistic regression can serve as a first step to other GLM models (e.g. <span class="emph">poisson regression</span>), and other categorical models, e.g. ordinal and multinomial.  If you do use it, consider interpreting it in terms of actual predictions at key values of the predictors[^margins] rather than statistical significance and odds ratios.  Not only are predicted probabilities easy to interpret, it's pretty much the only way to interpret interactions and other nonlinear effects that might be included in the model.

### Unsupervised Methods

Along with basic regression and logistic regression, you'll need some conceptual understanding of what are typically referred to as <span class="emph">unsupervised learning</span>, <span class="emph">dimension reduction</span>, or <span class="emph">matrix factorization</span>.  Here, we are exploring structure more than predicting outcomes (though we could do that too).  Probably the most common among these are principal components analysis (PCA), factor analysis, and k-means cluster analysis.  I would suggest just getting a handle on why you'd use them, and be prepared for needing to implement them or similar variants for your work, because they are used a lot. Some of these are taught in classes on <span class="emph">multivariate analysis</span>, which is an utterly useless name, but those classes also often include things like MANOVA, standard linear discriminant analysis, canonical correlation and others that have little place in modern methodology[^cancor].  Don't waste your time with those.


### Loss functions and maximum likelihood estimation

In statistical modeling we are interested in estimated parameters. For example, these might be regression coefficients or variances. But how do we do it?  We certainly don't just guess, and despite what the press would have you believe about AI we definitely don't use magic, so where do the numbers come from?

The answer is that we pick values that bring about the best result.  But how do we define *best*? Well there is no one way to do it, and how you might do it depends on the situation.  However, there are a couple common approaches you should understand conceptually very well.

Let's begin with ordinary least squares (OLS). If this is our optimizing approach, then we are trying to minimize the squared residuals, or errors, in prediction.  For classification, it could simply be that we want to minimize *mis*-classification errors.

Probably the most common method in standard statistical analysis goes the other way.  Instead of minimizing errors in prediction, we seek parameters that maximize the likelihood of seeing the observed data.  This method, <span class="emph">maximum likelihood estimation</span>, is the most common way we estimate parameters in statistical modeling, and has equal importance in the Bayesian framework too.  Sometimes the estimates that might minimize the loss function may also be the maximum likelihood estimate. This is the case in standard linear regression, but it would not be in other settings.  The primary point is, knowing the <span class="emph">objective function</span> for your analysis will help it not seem so mysterious. There are many optimization algorithms, and while you don't need to know the details of them, do know that you may get errors because their default settings don't work for your analytical situation.

Many applied statistical courses will spend time almost exclusively on ordinary least squares techniques, for which they might not even talk about estimation at all, leaving the students to basically believe in magic when they do statistical analysis. Some will even report that they used maximum likelihood, but this is only because they know that is the default.  For your own sake, don't bluff this.

With R or Python, you can literally write your own regression function via least squares or maximum likelihood in one or two lines of code. I strongly suggest you do this, even if only once, just to drive the point home.  I can also say it is quite satisfiying when something you put together reproduces the output you expect from the statistical program.


### Problems with NHST

I'm sick and tired of talking about the issues with <span class="emph">null hypothesis testing</span>, and I think the statistician types in general are.  Its problems have been noted since it was first proposed, and people in both methodological and applied disciplines have been raising the issues for almost a century at this point.  It's annoying for me to even think about it, so if you want details, I won't be rehashing them here, but you won't have to look far on the web to find them.

Assuming you're even using the approach correctly (almost never the case in applied research), just about every other statistic that falls out of an analysis is more important than the p-value. Yet I have seen people torture data, change their theories, ignore important findings, etc., because of it.  Researchers still doing this are not doing good analysis, because almost by definition it is work that likely won't stand the test of time, probably even the second time.  They are wasting their own and others time in general.


*Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test*. J Berkson **1938**

"The statistical folkways of a more primitive past continue to dominate the local scene." Rozeboom **1960**[^bestquote]

"As a second example, consider significance tests. They are also widely overused and misused." Cox **1977**

"Are the effects of A and B different? They are always different, for some decimal place." Tukey **1991**

That first one is an article title, but otherwise those are some very old quotes, indicating just how long this has been a problem.  NHST as a *scientific* paradigm, in which evidence is weighed to assess a theory's real-world effectiveness, for a variety of reasons, doesn't work in practice[^nhstpractice].  It is difficult to say whether it ever did. Many disciplines are possibly decades behind in their science both because of missed opportunities, and because of trudging through weak results that didn't hold up, but were adhered to merely because of statistical significance.

Statistical analysis will not provide you a hard answer or truth, and claiming something is 'significant' doesn't make it so.  The goal in statistical approaches to science is not to say whether a predictor is important or not, but rather whether the data is consistent with a given theory *in general*, and whether viable predictions can be made with a given theory. Go in with competing models, e.g. simpler ones vs. more complex, or theory-driven vs. purely exploratory.  Spend time expressing your data visually.  If you do, you'll regularly have something interesting to talk about regardless of any p-value.

### Under and Overfitting

> With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.

~ John Von Neumann 


#### Overfitting

A common problem in standard statistical modeling and machine learning includes <span class="emph">overfitting</span>. Overfitting may be described in a couple ways, but the key idea is one of model comparison.  For example, usually we want to compare simpler models vs. more complex models. While more complex models may better mimic what we see in nature, simpler models may perform nearly as well, allow for simpler interpretation, provide computational benefits etc.  Ideally we want a parsimonious model, one that is complex, but not overly so.  A more complex model can actually make no better or even worse predictions than a simpler model. In this case we are overfitting the data.

Take for example the following plot of data.

```{r load_model_packages, echo=FALSE, cache=FALSE}
library(modelr); library(mgcv)
```


```{r overfitting0, echo=FALSE}
virpal = viridis::plasma(4)
adj_rsq = function(rsq, p, n) {rsq - (1-rsq)*(p/(n-p-1))}
rmse = function(res){
  sqrt(crossprod(res)/length(residuals))
}
mae = function(res){
  mean(abs(res))
}
set.seed(1234)

d = gamSim(1, n=500, verbose=F)
idx = sample(1:500, 50)
d_50 = d[idx,]
d_450 = d[-idx, c('x2','y')]

d_50 %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange)) %>% 
  theme_plotly()
```

A sufficiently complex model might obtain the following fit.

```{r overfitting1, echo=FALSE}
model_justright = gam(y ~ s(x2, bs='gp'), data=d_50)
model_overfit = loess(y ~ x2, span=.15, data=d_50, surface='direct')
model_underfit = lm(y ~ poly(x2, 2), data=d_50)

d_50 %>% 
  add_predictions(model_overfit, var='fit') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), showlegend=F) %>% 
  add_lines(y=~fit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  theme_plotly()
```

Great fit right? The R^2^ for the model is `r round(cor(model_overfit$fitted, model_overfit$y)^2, 2)` with mean absolute error (MAE) of `r round(mae(model_overfit$residuals), 1)`. However, if we predict it on new data from the same underlying data generation process, the MAE is `r round(mae(predict(model_overfit, newdata=d_450)- d_450$y), 1)`, which is notably worse!

```{r overfitting2, echo=FALSE}
d_450 %>% 
  add_predictions(model_overfit, var='overfit') %>% 
  add_predictions(model_underfit, var='underfit') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), showlegend=F, opacity=.5) %>% 
  add_lines(y=~overfit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  add_lines(y=~underfit, color=I(palettes$orange$tetradic[3]), showlegend=F) %>%
  theme_plotly()

```

Conversely, a simple regression with a quadratic term has comparable fit, with even a lower MAE (`r round(mae(predict(model_underfit, newdata=d_450)- d_450$y), 1)`).  For reasons we're about to see, I would not recommend the standard regression fit either, but the point of overfitting is illustrated.


Another way to think about overfitting is in terms of the number of parameters you must estimate with the model vs. the number of data points you have to estimate them.  Some machine learning approaches may have thousands of parameters for example, so you better have a lot of data for them to work well.  A comon problem I see is with structural equation modeling, where people routinely fit complex models with several dozen to more than a hundred parameters with very little data. Because they also don't compare models to simpler ones, or even typically discuss prediction *at all*, they don't even speak to the notion of overfitting.  Sometimes, when their model doesn't fit as well as they'd like, they'll even use purely data driven approaches to add even more parameters!

For more on overfitting, see [this discussion](http://andrewgelman.com/2017/07/15/what-is-overfitting-exactly/) at Andrew Gelman's blog.


#### Underfitting

A converse problem is perhaps just as ubiquitous or even more prevalent in some fields, and that is not incorporating sufficiently complex models, or <span class="emph">underfitting</span>.  Take for example, the fact that standard linear regression, with no interactions or nonlinear effects, is still the most commonly employed model.  It seems suspect that a straight line would be 'good enough' to explain most relationships in nature.  In many cases the simplification may be worth it, but we are well beyond the time where that should be a so widely used as a default.

Consider a generalized additive model (GAM) fit to the previous data. It initially won't fit as well as the overparameterized model. But it certainly does better than a straight line would.


```{r underfitting0, echo=FALSE}
d_50 %>% 
  add_predictions(model_overfit, var='overfit') %>% 
  add_predictions(lm(y~x2, d_50), var='underfit') %>% 
  add_predictions(model_justright, var='justright') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), opacity=.5, showlegend=F) %>% 
  add_lines(y=~overfit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  add_lines(y=~underfit, color=I(palettes$orange$tetradic[3]), showlegend=F) %>%
  add_lines(y=~justright, color=I(palettes$orange$tetradic[4]), showlegend=F, line=list(width=3)) %>%
  theme_plotly()
```

<br>
When it comes to new data, it performs the best (MAE = `r round(mae(predict(model_justright, newdata=d_450)- d_450$y), 1)`).

```{r underfitting1, echo=FALSE}
d_450 %>% 
  add_predictions(model_overfit, var='overfit') %>% 
  add_predictions(model_underfit, var='underfit') %>% 
  add_predictions(model_justright, var='justright') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), opacity=.5, showlegend=F) %>% 
  add_lines(y=~overfit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  add_lines(y=~underfit, color=I(palettes$orange$tetradic[3]), showlegend=F) %>%
  add_lines(y=~justright, color=I(palettes$orange$tetradic[4]), showlegend=F, line=list(width=3)) %>%
  theme_plotly()

```

My own opinion is that a generalized additive model would be a better default model rather than the standard regression model. It penalizes complexity so that if a linear fit is better, that will likely be the result. In addition, interactions, spatial effects, random effects and myriad approaches are available to add to it.  For more on these models, see my intros [here](https://m-clark.github.io/docs/GAM.html) and [here](https://m-clark.github.io/workshops/stars/).

The issues around over- and underfitting can be somewhat subtle, but they are pervasive.  In general, I think underfitting is perhaps more the problem in the things I see, and with penalized regression approaches, one can incorporate more complex models while still guarding against overfitting.  Interpretation of more complex models may become more difficult, but also more rich, and more fun in my opinion.
 




### Visualization

There are a couple important things to know when you begin learning about visualization in the scientific context.  One of the more essential things to learn right off the bat is that your intuition is almost certainly wrong.  You likely don't know how to visualize well because most visualization is done poorly if not terribly, so going by what you see will simply perpetuate bad approaches.  For example, no one should be doing 3d pie charts, bar plots with error bars on the top of them[^barplots], or plots with unusually scaled axes, yet these are still regularly displayed in journals. On top of this, an applied stats course likely will not spend much time, if any, teaching the basics of visualization.  I don't mean producing plots, at least they usually do that, but actually thinking about the means of conveying scientific information visually.

Second, depending on the tool used, default settings may be just as bad as what you would choose with no information.  For example in terms of color, whatever you'd pick on your own is not going to do well, but unless the package developer put actual effort into it, you might end up with red-green or other problematic combos[^jetcolor]. Lines may be too thin, gridlines may overwhelm the actual plot content, and so forth. With color, issues of emphasis, colorblindness, etc., come in to play, and unless you put some thought into it, your visualization will have problems.

A good visualization in a research context will take complex relationships and make them easier to understand, *even if it takes some effort on the part of the viewer*.  So when thinking of a visualization, the goal is going to typically be to work with some complex, but interesting, data story and attempt present it in a way that tells that narrative in a clear fashion.  For example, showing the difference in group means is not complex, so a visualization is just going to create a bunch of whitespace to tell you what a single sentence could. 

```{r iris, echo=FALSE}
iris_plot_bad = iris %>% 
  mutate(Mean = predict(lm(Sepal.Width~Species)),
         se = round(predict(lm(Sepal.Width~Species), se=T)$se, 3)) %>% 
  distinct(Species, Mean, se) %>% 
  ggplot(aes(x=Species, y=Mean, color=Species)) +
  geom_pointrange(aes(ymin=Mean-2*se, ymax=Mean+2*se), size=1) +
  scale_color_manual(values=c( 'Green','Red', 'Yellow')) +
  ylab('Sepal Width') +
  theme_bw() +
  theme(panel.grid.major.y=element_line('black', .5),
        panel.grid.major.x=element_blank())
iris_plot_bad
```

<br>

The above has multiple problems, some of which include that the differences are perceptually exagerated, most of the plotting area isn't useful, the colors chosen are poor and draw the eye to versicolor more than the others, the colors add no information, it has a completely unnecesary gridlines, and the only 'story' here is simply that 'Setosa > rest'.

When displaying information for others to consume, always think beyond two dimensions.  2d plots such as simple bar and scatter plots are generally a waste of time, beyond initial data exploration (i.e. before modeling), and it's simply too easy to add more information while still not being overwhelming.  For example, with a scatterplot one can use point size to represent another variable, color for some grouping, and transparency for yet another variable.  You now have five pieces of information (including the basic x and y dimensions) represented in the same graph.  


The following represents all the information in the iris data set[^iris] (hovering will show the other variable values), including some summary information (means for x, y axes), and it utilizes all of the plot space. It doesn't distort the relationship of group means, because we have the context of the all the observations, and the colors chosen don't draw the eye to one group any more than another.  So it takes the previous 2d plot and adds various other information, but without being overwhelming.


```{r iris_2, echo=FALSE}
iris_plot_better = iris %>% 
  mutate(MeanSW = predict(lm(Sepal.Width~Species)),
         MeanSL = predict(lm(Sepal.Length~Species))) %>%
  group_by(Species) %>% 
  ggplot(aes(label=Petal.Width, label2=Petal.Length, color=Species)) +
  geom_point(aes(x=MeanSW, y=MeanSL), size=5, show.legend=F)+
  geom_point(aes(x=Sepal.Width, y=Sepal.Length), alpha=.35) +
  # viridis::scale_color_viridis(discrete=T) +
  labs(x='Sepal Width', y = 'Sepal Length')
ggplotly(tooltip=c('colour', 'x', 'y', 'label', 'label2')) %>% 
  lazerhawk::theme_plotly() 

# iris_plot_better = mtcars %>% 
#   mutate(MeanMPG = predict(lm(mpg~cyl)),
#          MeanWt = predict(lm(wt~cyl)),
#          cyl = factor(cyl)) %>%
#   group_by(cyl) %>% 
#   ggplot(aes(label=hp, label2=carb, color=cyl)) +
#   geom_point(aes(x=MeanWt, y=MeanMPG), size=5, show.legend=F)+
#   geom_point(aes(x=wt, y=mpg), alpha=.35) +
#   # viridis::scale_color_viridis(discrete=T) +
#   labs(x='Weight', y = 'Miles per Gallon')
# ggplotly(tooltip=c('colour', 'x', 'y', 'label', 'label2')) %>% 
#   lazerhawk::theme_plotly() 
```

<br>

It has gotten easier over time to pick some decent colors.  For example, <span class="pack">ggplot2</span> in R will default to evenly spaced colors, and the <span class="pack">viridis</span> module in Python and R will provide evenly spaced, colorblind-, and print-friendly palettes.  [Colorbrewer.org](https://colorbrewer2.org/) and the <span class="pack">RColorBrewer</span> package is another useful tool.

Also note that for scientific communication, [infovis](https://en.wikipedia.org/wiki/Information_visualization) is not an option.  This isn't advertising, and what may 'look cool' may not be very useful for scientific presentation.  Take for example, word clouds. People seem to love them, but they are almost useless.

As mentioned in the first part, your audience will be fine with something more than a bar plot. I have been told multiple times that the reason for not doing a particular style of plot is because the audience of Ph.D.s, M.D.s and graduate students won't be able to figure it out.  This is an utterly ridiculous notion with no proof whatsoever.  Unless you are thinking interactively first, you probably aren't even producing a visualization that is as complex (and commonplace) as those commonly seen in news outlets like the New York Times.  If someone is actually interested in your work, they will read the caption or associated text and have the cognitive capability to sort out what you're showing.





### Variable transformations

There are some good reasons to transform a variable. For example, mean centering (subtracting the mean), or additionally scaling numerical variables both makes a value of 0 practically meaningful (i.e. it is the mean), and can go a long way toward making the estimation process easier for many algorithms.  Some techniques are not even useful unless the variables are similarly scaled.  There are also times, especially in the case of categorical outcomes and predictors, when the need arises to collapse categories which have very few observations.  Otherwise they can lead a model to fail.  Finally, some, especially econometricians, prefer to speak in terms of <span class="emph">elasticities</span>, and so will take the log of a numeric variable to do so.  That's pretty much it as far as the reasons to transform a variable- easier estimation or theoretical interpretiability.

Here are couple things not to do.  If you have a numeric variable, there simply is no justification for discretizing it.  You are just making it a less reliable and expressive measure, and there is no benefit to that.  In addition, transforming a variable 'due to normality' is translated as 'I don't actually know what the normality assumption is'.  Case in point, the following is a perfectly reasonable target variable that would meet the normality assumption for regression.

```{r normality, echo=FALSE}
x = sample(0:1, 500, replace=T)
y = 5*x + rnorm(500)
ydens = density(y)

data_frame(y=ydens$x, dens=ydens$y) %>% 
  arrange(y) %>% 
  plot_ly(x=~y, y=~dens) %>% 
  add_lines(color=I(palettes$orange$complementary[2])) %>% 
  theme_plotly()
```

Why? Because the assumption regards the *residuals*, not the observed target variable. The above is just what you'd expect if you were dealing with a very strong group effect. The following shows the density plot of the residuals and a test of normality (non-significant means okay).

```{r residuals, echo=FALSE}
ydensres = density(residuals(lm(y~x)))

data_frame(y=ydensres$x, dens=ydensres$y) %>% 
  arrange(y) %>% 
  plot_ly(x=~y, y=~dens) %>% 
  add_lines(color=I(palettes$orange$complementary[2])) %>% 
  theme_plotly()
# shapiro.test(residuals(lm(y~x))) %>% broom::tidy()

```

```{r normality2, echo=FALSE}
# this completely f/d up the whole document by including it with the previous plotly chunk
shapiro.test(residuals(lm(y~x))) %>% pander::pander()
```


I've seen this assumption so confused that some think it applies to the predictors as well!  Some have also tried to apply it to ordinal variables. A moment's reflection will prove this will not meet with success.  Some also apply it to data, such as counts, where we actually expect it to be skewed.  And I don't know anyone that knows how to interpret a variable that has undergone an square root arcsine transformation.

If your model is not capturing tails or other aspects of the observed target, it's likely because the normal distribution is not a viable candidate for the underlying data generating process, or the model simply isn't capturing a key aspect of the data (e.g. inherent clustering).  It makes more sense to change your assumption about the distribution (e.g. use a t or beta distribution if appropriate), or fixing your model (e.g. using a mixed model if appropriate), than trying to torture your data in a misguided attempt to speak to one of the less important assumptions in the standard linear model.  You'll also find that the heteroscedasticity problems magically go away as well. 

### Analytical summary

- what is needed further for your specific projects
    - learn more about whatever technique your using


## Programming

If you're doing serious scientific research, these days you're going to have to learn some programming[^notstartrek].  Don't want to? That's okay, just do something else besides scientific research.  Even if you're doing relatively simple analysis, a lot of data processing will have to take place beforehand to ensure that whatever results you obtain have integrity.  Unless you like to spend several weeks on something that could take a couple hours with even just basic programming skills, you'll need to spend some time in this area to develop said skills.


One of the first things to note with statistical programming is, that once things are set up in terms of the data, it is as easy to run a model in R or Python as it is menu-driven programs like SPSS. It even requires *less* syntax than you would find with, e.g. SPSS and SAS.  Compare the following two approaches, one a menu-driven approach in SPSS and another programming approach in R.


- SPSS

```
Click File
Click Open
-- hunt for file on computer
-- wait a few seconds for SPSS to open its own file type
Click Analyze
Click/hover Regression
Click Linear Regression
Search for and Click your dependent variable (or click, start typing the name, and hope)
Search for and Click each 'independent' variable
Click on various options
Click ok
```

- R

```{r lm_vs_spss, eval=F}
d = haven::read_spss('filelocation/file.spss')
model = lm(DV ~ x1 + x2 + x3, data=d)
```



There is no debate here, even givng SPSS the head start of having the data in its own format, one approach is vastly more efficient, and simply easier.  In fact, with RStudio's autocompletion, the odds are that the first line probably only required a few keystrokes. You may have to learn that you can use the <span class="pack">haven</span> package for importing SPSS files, or the <span class="func">lm</span> function to do regression, but that's very quickly picked up, and less than you would have had to learn regarding the SPSS menus, which have remained mostly unchanged and in their disorganized state for over 20 years.  The other nice thing is that you can redo the above in two steps with Ctrl+A (select all) and Ctrl+Enter (run)[^pyspss].




### R & Python

<div class='col2'>
<img src="img/Rlogo.svg" style="display:block; margin: 0 auto;" width='50%'>
<br>
<img src="img/py.svg" style="display:block; margin: 0 auto;" width='75%'>
</div>
<br>
When it comes to statistical programming and computation, there are two primary choices these days, but you should be flexible, because the situation will likely change at some point in your career.  <span style="color:#1f65b7">**R**</span> and <span style="color:#FFDC52">**Python**</span> are the primary tools of statistics/data science, and have pretty much overtaken both enterprise and academia.  They are both easier to program for data processing and analysis than they were even just a few years ago, and notably more so than they were in the early days of their use for modeling.  It doesn't really matter which you use.  Python will have more power when it comes to machine learning, potential enterprise applications, natural language processing, and, in general, speed.  R is still notably easier to use for applied interactive analysis, and has far more developed packages for a far wider range of techniques.  However, these distinctions melt by the wayside more and more everyday. So you're fine with either.



#### Alternatives

##### Other Programming languages and related

Lower level programming languages such as Java, Fortran, and C++ are not required knowledge for the vast majority of applied statistical and related application.  They are primarily used when you need to get the most speed you can, and you'll be able to find someone on campus to help you there, for example, in your research support group.

Matlab, Julia, and Mathematica are other tools or languages you might come across that are highly capable, but are either proprietary (i.e. require a renewable license, cost money, and don't share source code), or don't yet offer much beyond what you can get with Python and R, and usually far less.  Of these I'd recommend Julia as its use appears to be on the rise and is freely available.

You will also find languages and frameworks within languages such as Scala, Spark, Hadoop, Keras, Torch, TensorFlow, SQL, Lua and so forth. Most of these have specific purposes, e.g. databases, or don't need to come into play unless you have massive data (e.g. TensorFlow).  Don't bother unless needed, but don't be surprised if you do.

In addition, people are writing analytical functionality in things like javascript, Ruby, Haskell and other things that weren't meant for it nor are all that good at it. I don't know why.


##### Statistical or similar 

The primary statistical programs in academia are SAS, SPSS and Stata.  Unlike the others, Stata use actually appears to be growing, and they also have a great community. The other two are dwindling fast in academia, especially, and thankfully, SPSS.  Aside from those, there are relics, money-grabs, and WTFs of all kinds. Seriously, there is some ridiculous software out there that some poor folks in some specific niche of a discipline got suckered into using a long time ago, and for whatever reason still use it.  If you're using non-open source software whose functionality is completely matched and bettered by others that are freely available, you'd better be able to justify it in some way, and I can't think of anything you could come up with.

If you use something like SAS or Stata, you still can't use the menus for analysis.  Aside from being inefficient, it's not reproducible, and thus not a way to do science.

Excel is not an option for scientific work, ever.  I would say you could use it for data entry, but it's very problematic even then. Avoid at all costs.

If you don't want to take my word for it here are some other things to peruse.

- [Link 1](http://spectrum.ieee.org/computing/software/the-2017-top-programming-languages) spectrum.ieee.org
- [Link 2](https://www.tiobe.com/tiobe-index/) tiobe.com
- [Link 3](http://r4stats.com/articles/popularity/) r4stats.com
- [Link 4](http://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html) kdnuggets.com

### IDEs

After installing the basic components for R or Python, you can use them straight away. However, this would be silly, as they don't come with a way to use them that enables basic functionality or is easy.  If your R console comes with retina-bleeding red-colored font, or you're using the clown-colored IDLE from Python, just shut them down and walk slowly away.

You want to use what's called an <span class="emph">integreted development environment</span>, or IDE.  For R, the far and away most popular one is RStudio. For Python, I would suggest Anaconda/Spyder for scientific work.  There are others, but unless you have a lot more experience, I'd suggest sticking with those[^atom].

What these do, practically speaking, is make your coding efforts a lot more easy and efficient. Syntax highlighting, code completion, autospacing, interactivity... this is only the beginning.  I've written this document and many other things *entirely* with RStudio.  A good IDE is essential.


#### Customize it

After you initially start to get the hang of your IDE, customize it. I'm always struck by the fact that people don't change the basic settings in their software, which are often chosen by people who should not have any say in design whatsoever[^baddesign].


### Coding <span class="font-variant:small-caps; font-family:'Dancing Script'">style</span>



<img src="img/code_quality.png" style="display:block; margin: 0 auto;" width=200%>
<br>
<img src="img/code_quality_2.png" style="display:block; margin: 0 auto;" width=50%>
<br>
<img src="img/code_quality_3.png" style="display:block; margin: 0 auto;" width=150%>
<br>


Coding style matters, but perhaps not in the way that many think.  


<span class="" style="size:125%; font-variant:small-caps">The only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code.<br> ~ Hadley Wickham</span>

Wickham is right on the first part, but I'm not so much in agreeement with the second.  No one writes perfect code, *no one*.  It takes effort just to write halfway decent code.  That's fine.  As you start out, you won't be writing good code for quite some time. That's okay too.

However, assuming you want others, including yourself at a future date, to be able to use your code, you should feel bad giving them shitty code, because you'll be wasting their time.  Going back to one of our initial suggestions, *Don't be a jerk*. Write better code, always. Time and other resources will always be working against you, but do the best you can.

The first step is to start with a coding style.  For example, both [Google](https://google.github.io/styleguide/Rguide.xml) and [Hadley](http://adv-r.had.co.nz/Style.html) have provided an R coding style for others to follow, and there is [PEP8](https://www.python.org/dev/peps/pep-0008/) for Python.  If you only followed those, your code will be great relative to what you would have done.  But realize they are only suggestions, some are based on decades of coding practice, others are based on whim, and they don't tell you which.

For example, the Google says using underscores for variable names, e.g. `this_is_a_variable`, is 'bad', while capital first camel casing is 'good', e.g. `ThisIsAVariable`.  However, it's very clear which is more readable, and just as easy to type. Furthermore, with modern IDEs, once the object is created, you only have to type a couple letters to complete the rest.  Both Wickham's and Google's style guide consistently contradict one another and themseleves, and yet they're both based on more experience than you'll ever have, so that should give you some sense that it isn't exactly easy to figure out what works best.

The point is, there is no authority here, but there are better approaches than your nospace, semi;coloned, naming-things-'data', everything-in-a-single-script attempts will come up with, I guarantee you.  In coding, just trying to do better will make your code better[^github].


### Errors as calls for help

When you first start programming, you'll get a lot of errors.  Rather than getting frustrated, you should see them as calls for help from the programming environment you're working in.  For example, take the following.

```{r errors, error=TRUE}
sum(z)
```

This is simply R telling you that it can't find something named <span class="objclass">z</span>. It may be that you called it `Z`, or haven't yet run the line in your code that starts with `z = ` to create it.  But it's no reason to be frustrated.

When running models, errors typically signify that a variable or even the whole data set is not in the correct format. Check the documentation.  In other cases, you're probably spelling it wrong.

Warnings are not errors, but you should investigate why they are occurring, rather than ignoring them.  A warning telling you that a package was built with a more recent version of R is no big deal.  A warning telling you that your model did not converge is serious. As you start out you may not know the difference, so treat them as serious until you understand them.

The more programming you do, the fewer errors you'll see, but when they do occur, you'll also more likely know what they mean and can more easily deal with them.

### I/O

One of the first things you want to learn in scientific programming is how to get common data formats, such as text files or statistical program formats, in and out of your programming environment.  As long as it is difficult for you to even get started, the less likely you are going to want to do any programming in the first place.  In the past it might have taken a couple steps to go from one format to another, but this is no longer the case[^stattransfer].  And every programming language or statistical package can easily read in common formats, but you'll have to get used to how to do it initially.  Even RStudio provides a point-and-click approach to importing data, but you should be doing things such that coding it will be more efficient.  Note that very large data sets may require special needs, but you'll just cross that bridge when you get to it.

### Basic object types

There are object types that are common to practically all programming languages, and you should be familiar with them.

- <span class="objclass">boolean</span>: a binary 0-1, or TRUE/FALSE. These are especially important to be familiar with for [indexing][indexing] purposes.
- <span class="objclass">numeric</span> of a kind: specifically 'floats' or 'integers' but for practical reasons can be anything that isn't a string variable (and may include boolean)
- <span class="objclass">string</span>: text based represenation. For example, a variable may be a boolean called `Male`, which would indicate that values of 1 or TRUE would indicate an observation is a male, or the variable might be a string called `Sex` with values of 'Male' or 'Female'.

Beyond that, statistically oriented programming environments typically distinguish categorical variables, called <span class="objclass">factors</span>. You can think of them as labeled integers.  For example, we might have a variable called `Sex` that has values of 1 and 2 but with labels 'Male' and 'Female'.  The distinction between factors and strings is less an issue these days, but depending on the package you're using or analysis being conducted, you may need one or the other.


### Data structures

Beyond basic objects/variables, we also have more complicated structures, which again you'll want to be aware of, because you'll use them often.

- <span class="objclass">vectors</span>: Vectors contain multiple elements of the same type. 
- <span class="objclass">matrices</span>/arrays: Matrices in R can be thought of as two-dimensional data structures, and arrays will allow beyond 2-d. In Python, these would be numpy arrays.
- <span class="objclass">data frames</span>: Data frames are, at least in R, the fundamental modeling object that has all the variables you want to analyze.  They are distinguished from matrices in that they are able to possess variables of different types, e.g. factors and integers.  The corresponding Python object is a pandas <span class="objclass">DataFrame</span>.
- <span class="objclass">lists</span>: lists are objects with no restrictions on what the elements may be[^rdataframe]. The first element could be the word 'cat', the second could be a list of strings pertaining to animal names, the third could be a thousand numeric matrices.  They are extremely common in R, as practically every model function returns a list (e.g. including elements of coefficients, fitted values, etc.).  Lists are found in Python too, and aditionally one can think of Python <span class="objclass">dictionaries</span> as named lists (though there is a little more going on than that). Note that R doesn't care whether you name the elements of the list or not, it's still just a list.
  
### Indexing

Once you have a data structure you'll need to learn how access parts of it.  For example, you may want to inspect the first 10 rows, or the fifth column, or all the variables that start with 'X'.  This is what a base R approach might look like.

```{r demoIndexing, eval=FALSE}
mydata[c(1:2, 6, 8), 5:10]      # works for matrices or data frames
mydata[mydate$Sex == 'Male', ]  # boolean indexing
mydata[,'variable_name']        # works for matrices or data frames
mydata$variable_name            # works for lists/data frames
mydata[['variable_name']]       # works for lists/data frames
```

However, base R indexing for data frames is largely a good way to write ugly and/or bad code.  One of the first and lasting rules in programming for scientific work is:

> <span class="" style="font-size:500%">Avoid magic numbers.</span>

Indexing by numbers goes against reproducibility, because row and columns can change quite dramatically both in data collection and data object creation. The `Age` column might be the fifth one now, but it may not be later. If you want the `Age` column, reference it by name.

The <span class="pack">tidyverse</span> approach in R enhances reproducibility by making it easier to use names and by using functions that make it clear what's going on.  Take the following for example.

```{r tidyverse, eval=FALSE}
mydata %>% 
  select(Age, contains('blue'), starts_with('A'))
```

You might not have ever worked with the <span class="pack">dplyr</span> package, but I'll be you know what that's doing.  Furthermore, if it's your code, you'll know what it means six months from now.

The gist is, you need to get familiar with how to slice and dice your data structures, and the quicker you will be able to process the data and get to the interesting aspects of it.


### Aggregation

One of the most common data processing procedures beyond slicing rows or columns is the 'group-by' operation.  As such, you'll need to get used to using it in your environment as early as possible, such that it becomes second nature. When it comes to R, do *not* use base R approaches like <span class="func">tapply</span> or <span class="func">aggregate</span>. They are slow, memory-intensive, or just plain annoying in implementation.  Again, the <span class="pack">tidyverse</span> will help here.

```{r group_by, eval=FALSE}
mydata %>% 
  group_by(Sex) %>% 
  summarize(Age = mean(Age))
```

The above would return the mean age for both Males and Females. Very straightforward.  Pandas in Python will have similar functionality[^wes].



### Iterative programming

One of the most common activities in programming is doing a specific operation on all rows or columns, or some subset of them.  It's so common, that you'll need to learn it early on, and use it often.  The basic approach for most programming languages, called a <span class="emph">for loop</span> is as follows:

```{r loop_example, eval=FALSE}
column_means = list()
for (i in 1:10) {
  column_means[[i]] = mean(mydata[,i])
}
```

The above would calculate the mean of the first 10 columns of some matrix/dataframe x. When you look at the column_means object after running the loop, it will have 10 elements, each being the mean of the associated column in x.

It is *extremely important* to know how to use loops very early on in your programming, or you will be wasting enormous amounts of time, because it *always* comes up.  Once you get the basic idea down, know that it will be conceptually identical and often even look the same in almost every programming language.  Here is the same loop in Python, using the <span class="pack">numpy</span> module (as `np`).

<!-- ```{python, eval=FALSE, echo=FALSE} -->
<!-- # because knitr doesn't know what the eval argument is when it's a python chunk anymore apparently. -->
<!-- import numpy as np -->
<!-- x = np.array(np.random.uniform(size=100)).reshape(10,10) -->
<!-- ``` -->


```{python pyloop, eval=FALSE}
column_means = []
for i in range(0, 10):
  column_means.append(np.mean(x[:,i]))
```


However, for data science, there is typically a more efficient approach, in terms of code, computational time, or both.  All of the following would be a better way to do the same operation in R.

```{r noloops, eval=FALSE}
colMeans(x[,1:10])
apply(x[,1:10], 2, mean)
sapply(x[,1:10], mean)
x %>% summarise_at(1:10, mean)
```


Similarly in Python:

```{python colmeans, eval=FALSE}
np.mean(x, axis=0)
```

Again, you need the concept first, so start by doing explicit loops, then see how to use the tools available so that you almost never have to write an explicit loop.

### Programming summary

Programming is a fundamental part of modern science, and I personally would go so far to say that if you aren't doing any programming, you aren't doing science. It formalizes the analytical approach, makes all your data processing efforts more efficient, and tends the entire endeavor toward a more reproducible setting.  The better you become at it, the more time you can spend on the other things.

## Reproducibility revisited

### Data 

Before we get to analysis, one needs to understand that 

> *everything begins and ends with the data*.  

If you don't collect the data carefully, it doesn't matter what else you do[^hardscience]. You've completely wasted your own time and resources, along with that of everyone else who is affiliated with the project.  At this stage in your career, if you're not willing to get your hands dirty with the data and make sure it's collected and processed properly, you simply shouldn't be doing research[^admin].

Collecting and processing the data is not as fun as analyzing it, but analyzing it isn't any fun at all if you can't trust the data.  Furthermore, the more time you spend with the data, the analysis stage will likely be very brief, because there will be less going back and forth from analysis to data checks.



### Analysis

The first step in reproducibility is to be able to duplicate the results of an analysis.  We can refer to this as <span class="emph">reproducible data analysis</span>.  The key idea is whether you could start with the raw data and produce the same results as you publish.  In addition, you might apply the idea of <span class="emph">convergent validity</span> to the process. Similar analytical techniques should come to the same conclusions.  

However, this is a very low bar for reproducibility in general.  A real first step would involve setting things up so that someone *else* could reproduce the results you came up with.  The best way to achieve this is to engage in <span class="emph">literate statistical programming</span>, probably best exemplified with using Rmarkdown for your documents.  Using LSP, the document, data, and code are combined into one entity.  

As an example, one can see [this article](https://m-clark.github.io/docs/George_2017_Readiness_preprint.pdf).  The actual files that produced it are a combination of text, R code, HTML, $\LaTeX$, and more.  Not a single number you see in it was typed- there is R code embedded throughout the text producing those values.  The figures[^notme] are produced with the document, not cut and paste into it.  Same with the statistical results.  To reproduce the results one merely has to compile the document with a single click.  In fact, it's *impossible* to get different results.

There is actually no need to write 'scripts' anymore.  One can, with Rmarkdown, Jupyter Notebook/Lab, and similar tools, take a reproducible manuscript as the default scientific product.



### Programming

> There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors.


#### Comments

A starting point for reproducibility in your programming is to comment everything.  The comments should not be about what the code is doing. This is because if you know the language at all you'll know what it's doing, or if you forget, it will be easy to look up what the function or command does.  Instead, comment about *why* you're doing it, or what the goal of the code execution is.  However comments aren't exactly necessary if your code is embedded in the research product, i.e. the document, itself, though they still might prove useful.


#### Naming things

Another thing to think about is naming things. If you are naming anything of importance as 'data', you deserve whatever happens.  Even more if you are naming things 'data3' or 'finaldata7'.  However, most of us aren't taught this, and so are guilty of it at some point or another.  But now you know. So don't.

This goes for files, objects, variables, *everything*. Name things as though you won't need it for a year, and yet a year later you'd know what it was.  With modern IDEs autocomplete means you can use long names and not even have to type but a couple letters. Take advantage of this.


#### Projects

This was mentioned in the previous [getting organized section][get organized]. IDEs and even some statistical software allow for the creation of <span class="emph">projects</span>.  These become self-contained folders where all your scripts, documents, visuals, and so forth reside.  Basically, if you are hunting for files or have to set your working directory, you're not doing things in an efficient or reproducible fashion.  Creating projects is so simple, there is no reason not to.  For any distinct work you're doing, create an organized and self-contained folder that only pertains to the work done for that project.
    


### Documents

<div class='col2'>
<img src="img/RMarkdownOutputFormats.png" style="display:block; margin: 0 auto;" width='50%'>
<br><br><br><br>
<img src="img/jupyter.svg" style="display:block; margin: 0 auto;" width='50%'>
</div>
<br>
   
Writing up scientific results in something like Microsoft Word is a deplorable practice in terms of science.  Copying and pasting output from a statistical or  other programming environment into a Word document is practically the definition of irreproducibility, opens the door for numerous errors, and, on the practical side, is exceedingly tedious.  If any change has to be made to the data, e.g. an error is discovered, analyses re-run etc., one will potentially have to change every number and visualization associated with the document.  That alone should be reason enough not to want to engage in such a practice.
   
Many disciplines have used <span class="emph">$\LaTeX$</span> as their approach and still do. Not only does it produce beautiful documents for print, it eventually became a means for <span class="emph">literate programming</span>, simultaneously embedding word and text and providing the means for more reproducible work.  Unfortunately, $\LaTeX$ is inherently a print medium, making it at best marginally useful these days.  If you've learned it, don't fret you can still use it, e.g. with math formulas, but everything you could have done with it you can now do via other means. Except you won't have to completely disrupt your train of thought and text messing with floats and tables.
   
<span class="emph">Markdown</span> has displaced $\LaTeX$ for scientific publishing these days.  In the right environment, there is nothing you can't do with it that you couldn't do with $\LaTeX$, and if you think otherwise, it just means you haven't gotten used to it yet.  The issue is that, despite what many journals still seem to think, the web is the medium of scientific communication, just as it is with practically everything else.  If you're creating a scientific document with a print-first mentality, you are probably not interested in disseminating your work to the broadest possible audience, and you are going to continually handicap what you might have been able to show with your efforts.

<span class="emph">Rmarkdown</span> allows one to produce HTML, PDF, MS Word and a whole host of other formats (e.g. slide presentations), and in about as easy a fashion as one could, especially compared to $\LaTeX$.  Other statistical environments such as Matlab and Stata are playing catchup, while others, still haven't figured it out.  In the Python world, you have <span class="emph">Jupyter Notebook</span> and eventually <span class="emph">Jupyter Lab</span>, though I haven't seen it used in as extensive fashion as Rmarkdown[^jn], which incidentally would allow you to incorporate Python and some other languages along with your R.

The gist is, your document first needs to be connected to the underlying data, and secondly needs to be ready for the web.  To get a sense how far one can go, I recently collaborated on a paper entirely in Rmarkdown, in which not a single digit in the final copy was actually typed.  All values, tables (which were also formatted via code), figures, and inline text was produced via underlying R code, and with the final output, as much as it pains me to admit, MS Word.  For those using R, [Rmarkdown](http://rmarkdown.rstudio.com/) makes it such that you don't even really have to write traditional scripts anymore, you can just use Rmarkdown and let your normal text take the place of comments.  For Python, you have [Jupyter Notebook](https://jupyter.org/) that appears to allow much of the same.


### Version control basics

<img src="img/octocat.png" style="display:block; margin: 0 auto;" width=35%>

I'll talk more about version control in the next section, but it's something you should be aware of even if you don't use it beyond the versions in things like Box drive.  For decades now there have been what are called <span class="emph">version control systems</span> (VCS) for software development.  And even if you don't use formal a formal VCS, the concept of thinking of your <span class="emph">science as software development</span> will go a long way toward improving your scientific method.

Tools like [Git](https://git-scm.com/) and [Mercurial](https://www.mercurial-scm.org/) allow for complex development involving many developers and iterations of applications to come together in a way that allows one to more rapidly correct mistakes, branch off into exploratory endeavors, add new people to the project, etc.  But you're not a software developer you say.  I say so what.

Consider a faculty member with several grad students working on different phases of multiple projects.  These tools would potentially allow for much more seemless collaboration, where, for example on one project a student could be working on the visualization 'branch', another on the conference presentation, another may be correcting some data, and while the faculty is working on the write-up, and eventually all these pieces can be merged into a final product.  If there is any issue found later, it is easy to revert back to the correct state and continue from that point.  VCS are also typically hosted remotely, e.g. at [GitHub](https://github.com/), and so automatic backup is in place, and it also provides a potential way to have your work exposed to even more people. The latter is especially nice for students, who will usually want to be marketing themselves and their skills as much as possible.  However the real benefit to using something like Git and GitHub, is that if you know others are going to see your code, you *will* write better code.

When you start thinking of your science and modeling as software development, one that goes through a potentially continual (and public) process of improvement, you'll almost automatically avoid many of the problems associated with the so-called 'crisis' of reproducibility.  People engaged in this aren't having any crisis at all, their methods are open, their documents are infused with data.  Journal outlets, as they are now, are impediments, not facilitators.  'Articles', if the term should even apply, should come with titles plus a version number, ready to be extended, improved, or *merely* replicated.


## Summary of Part II

Most applied researchers, or anyone engaging in statistical modeling generally, can not only benefit from many of the concepts outlined in Part I and II, but also have plenty of time to do the other things they need and/or want to do.  It may seem like spending time with these things might seem like something you don't want to do, but the alternative includes far *more* time cutting and pasting, redoing uninteresting analyses, trying to find which file/data goes with what, writing about noisy results, creating ugly visualizations etc.  


In other words, you don't have to be a strong programmer, though you will have to do some programming, and take steps to do it better.  You don't have to be a statistician, but you will need to learn well a technique you intend to use.  It won't be easy, but I've never seen an instance in which the alternative was easier.  Don't deceive yourself. Do better.








[^simplerelative]: If you're just starting out, it probably isn't simple to you, but we're talking *relatively*. It'll be old hat to you as well at some point.

[^margins]: Sometimes called marginal effects.

[^cancor]: However, I can say I do like name dropping canonical correlation, which MANOVA, LDA, and standard regression are special cases of, every once in a while just to befuddle people.

[^notstartrek]: At least until the ideas of Star Trek become a reality.  And don't let the folks in the Bay Area fool you, it's still a very long way off.  For some perspective, at the turn of the 20^th^ century, there were no cars as we now know them other than demonstrations and prototypes. By World War I, less than 20 years later, cars were a common sight in cities, and airplanes were being used in combat. We've now had more than four decades of combined internet and personal computing capabilities, and probably the biggest difference between now and then is that people spend a lot of time staring at tiny screens in their hand.  Most of the stuff we're just now getting to was promised to be right around the corner during the space age of the 1950s and 60s.  The point is, a lot of this stuff will come, but don't hold your breath.

[^baddesign]: Just look at Google's gmail, Facebook, Amazon.  They are essentially the anti-Tron of user esteem.  Amazon in particular seems to simply vomit out the return of a search that mildly references some of the letters you actually typed into its engine, if they used Google translate on the query first, while assuming the native language was an interpretive dance in Kabuki theater.  The 'art' of basic drilldown is a completely lost enterprise.  Using that website almost makes you want to ask Alexa for the best form of suicide.

[^github]: My main motivation for getting on GitHub was making my code better.  If the only reason you use it is that the thought of others looking at your code will make you write better code, then definitely do so.

[^bestquote]: You may try, but you will never, *ever* find a better quote in all of statistics.  Also, it's generally expected that when uttered, those in earshot must snap at least twice and say 'Right on, daddy-o'.


[^nhstpractice]: The reason many sciences *have* progressed is because they don't use NHST correctly.

[^pyspss]: I was going to do a Python example too, but I couldn't find an example that wouldn't take 5 or more lines not counting the import lines, which would be several more.  

[^repcrisis]: It is *current* if you're not aware of statistical history of the past 60+ years.  The only thing that's different from past calls to do better work is people's ability to complain on social media.  Applied researchers will do no more methodologically than what is expected, which seems largely defined by whatever they come across in the few journals they spend time with.  This is not necessarily right or wrong, just the way it is.

[^atom]: I do like Atom for Julia and Python, but it's a general text editor, not a program specific IDE.

[^bayesianr2]:  See Gelman [here](http://www.stat.columbia.edu/~gelman/research/published/rsquared.pdf) and [here](http://www.stat.columbia.edu/~gelman/research/unpublished/bayes_R2.pdf).

[^barplots]: Or barplots at all in my opinion.

[^jetcolor]: Matlab ignored for years calls to get rid of its default 'jet' colorscheme.  They finally did, and laughably have tried to make their new choice, *which is still problematic*, proprietary.

[^stattransfer]: Some of you may still come across a tool like StatTransfer to go from say, SPSS to Stata. This completely unnecessary.

[^rdataframe]: Technically, R data.frames are lists.

[^wes]: In case it wasn't obvious, the Pandas and tidyverse authors are in cahoots with one another, though the Pandas author is no longer actively involved in its development as far as code is concerned.  It should be fairly easy enough to move from one to the other.

[^hardscience]: Despite what some in the 'hard' sciences seem to think, merely collecting the data and having a good theory is not enough for strong science.

[^admin]: One can distinguish researchers from (research) administrators.  I have respect for the latter, as they are often the ones who coordinate the teams, acquire the funds, and drive the research in general, though in many situations, I would hesitate to say they are actually *doing* the research.  For graduate students and early career folks for whom this document is intended however, this distinction doesn't really apply.  You should be neck deep in your data.

[^notme]: Sadly I largely lost most of the arguments on the visuals.

[^jn]: I don't see many bells and whistles with what people do with Jupyter Notebook - practically all the ones I come across look like the default - which makes me think it isn't easy to go into deep customization, though I don't really know (I've only dabbled with it myself, so I'm speaking from a place of ignorance).  However notebooks in general, including R notebooks generated by RStudio or Jupyter, are very much geared toward interactive coding rather than creating a more or less static scientific document.  Though things may change in the future, that has less a role in typical scientific communication, where you wouldn't want code interfering with the data story you're trying to convey. It will be interesting to see where [Jupyter Lab](https://jupyterlab.github.io/jupyterlab/) goes in furthering the development.

[^iris]: This is the last time I use this terrible data set.  

[^deeplearnworks]: In other words, a ridiculous amount of data, with 'clean' data, and a whole host of other things in place.

[^phack]: I'm not even going into the fact that they probably finagled the data 20 different ways with umpteen models to get that p-value slightly less than .05, a practice called <span class="emph">p-hacking</span>.  People who do this engage in the worst type of anti-science.

[^bob]: Should you need assistance with knowing whether you have leveled up or not, there is someone in the IT department at LSU named Robert Gill III Esq. that can help you.  You cannot provide specifics, only ask, 'Have I leveled?'. He will then respond with a series of questions that will, in one way or another, probe the essence of your being, and provide the only answer you will ever need. Sprinkled with quotes from Buckaroo Bonzai, Goonies, and for matters of import, Forrest Gump.