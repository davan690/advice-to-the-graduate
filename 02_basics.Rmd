# Part II: Perfect Sound Forever

The goal here is not expertise, but primarily conceptual awareness and understanding.  For example, you don't have to know much about machine learning approaches other than why you might use them.  You can then learn more as needed when the time comes, if it even does.  For programming, it literally could be two lines of code to import your data and run a statistical model. But the data is *never* ready, and data processing will likely be where you spend the vast majority of your time.  However, knowing even a few things about basic programming will help you spend a tremendous amount less regarding what you *have* to do, but probably more of things you *want* to do.

Some motivation.  Here are some things I've seen:

- A social science grad student first learning R at the beginning of the semester, that within a few months was a RStudio keyboard shortcut whiz, and was taking some Python workshops to boot.

- Another social science student using efficient programming practices to analyze big data on a high performance computing cluster.

- Architects, English Lit faculty, Staff librarians.  I have seen them all do basic programming and statistics to even some advanced stuff.

- I have seen 50+ years old SPSS folks get into machine learning.

It obviously isn't easy, but anyone can do it.  You'll likely be surprised at just what eventually does becomes easy, but be prepared for regular stumbles along the way, no matter what level you're at. As you improve, you'll just be at another level where the same amount of stuff will trip you up, only more advanced.  Case in point, here is the most common phrase I utter when programming:

<span class="" style="font-size:125%; font-variant:small-caps; color:#B2001D">~ You've got to be @%*^!ing kidding me!! </span>

Once you get good at some things, you'll still hit roadblocks, be surprised, or just make stupid mistakes.  It's just part of the journey, and mostly just means you're doing something interesting. If it helps, think of it as an RPG where you're leveling up, only, you'll probably be the only one to tell you that you've leveled up[^bob].


 
## Analytical

### Explanation vs. Prediction: a Starting Point

<h6> *A narrative of startling interest!!* </h6>

One way to look at the analytical endeavor is in terms of two goals, not mutually exclusive.  On the one hand we are interested in theory confirmation and comparison. Theory suggests certain variables have relations to other variables.  We set up mathematical models that convey the theory in a testable form, and collect data corresponding to those variables. Via a a particular framework, e.g. null hypothesis testing or Bayesian, we can say something about the statistical notability of those relationships.  The ultimate goal is to tell a story about the results and their relation to the theory, possibly suggesting exceptions and modifications.  In this approach, <span class="emph">explanation</span> is the focus.

On the other side, we need something that is practically viable.  The goal is not so much to tell a story as it is to get the best possible <span class="emph">prediction</span> from our models.  Techniques may be used that will tell us little about which predictors are best, or even why some predictors work better or what is even the primary feature of the data being learned.  However, not only does the result work well, such approaches almost always outperform the explanatory ones in practice. 

To make things a little more concrete, we can consider linear regression vs. deep learning.  With the former we can obtain highly interpretable coefficients, statistical significance, interval estimates, etc.  It is extremely easy to tell a story with the results. With deep learning on the other hand, it's difficult to tell what's being learned, or what features are being latched on to.  However, assuming it's applied appropriately[^deeplearnworks], it works extremely well!


Practically every applied discipline seems to focus exclusively on explanation in their statistical education.  You might think this is not too big of an issue, but to see the consequences of this, one only has to look at the current 'replication crisis'.  You've now had nearly a century of results interpreted solely in terms of statistical significance in the null hypothesis testing framework, with typically little to no focus on whether the damn model even works.  As a specific example, you can have statistically significant predictors in a linear regression and an adjusted R^2^, which would tell you how much variance in the outcome you can explain with the predictors, of *negative value*!  The following demonstrates this.  I construct completely random data, i.e. none of the $X$ has any relation to $y$, and use it in the model.

```{r badmod, echo=2:4}
set.seed(0)
N = 250
X = matrix(rnorm(N*10), ncol=10)
y = rnorm(N)

# summary(lm(y ~ ., data.frame(X)))
# broom::tidy(mod) %>% mutate_at(vars(-term), round, digits=3) %>% DT::datatable(options=list(dom='t', pageLength=11), rownames=F)
pander::pander(summary(lm(y ~ ., data.frame(X))), justify='lrrrr', round=3)
```
<br>
Yet you will find entire papers written about that one coefficient, with p-value slightly less than .05[^phack], while in terms of prediction it literally does worse than guessing the mean would with new data. Quite simply, this sort of practice has got to stop.

The key idea is <span class="emph">balance</span>. For most applied disciplines explanation is not only required, but to be focused on.  This is fine. Some people do researcher where collecting one observation might take a few days of effort.  This too, is okay. However, complicated models with little data require something more than statistical significance- the results bounce around too much.  But for any modeling endeavor, predictive validity is also important.  You simply cannot focus on the statistical (significance) story while entirely ignoring the practical value of the model.

#### A Word about Effect Size

Some disciplines, e.g. psychology, try to get their adherents to focus on effect size.  Reviewers regularly request it, but also are regularly non-specific about the metric, or if they do mention it, make clear they have no idea what they're talking about.  For example, one of the more popular effect sizes is <span class="emph">Cohen's d</span>, which is a standardized difference in two group means (e.g. control vs. treatment). If you are doing a t-test as your primary model and haven't gone back in time, then I can assure you no one is going to be citing your work regardless of your effect size.  Here are some of the issues.

- Without a corresponding interval estimate the effect size of little value.
- As soon as you get into any complication, e.g. repeated measures, interactions, etc., there is little standard.
- Small data and large effects mean nothing other than that you need more data.  It is *definitely not* the case that the effect is 'more real' because you happened to see it with such a small sample.

Another common 'effect size' is the standardized regression coefficient.  You don't see this much outside of disciplines where SPSS is used a lot. However, even in the only setting where it's easily calculated, it still doesn't make comparable categorical effects vs. numeric ones. And as soon as you move beyond the standard linear model, it's pretty much not defined.  Also, what's the effect size here?

```{r effectgam, echo=FALSE}
library(mgcv)
data(mcycle, package="MASS")
mod = gam(accel ~ s(times), data=mcycle)

library(plotly)
mcycle %>% 
  modelr::add_predictions(mod) %>% 
  plot_ly(x=~times, y=~accel) %>% 
  add_markers(marker=list(color='#ff5503'), showlegend=F) %>% 
  add_lines(x=~times, y=~pred, line=list(color='#03b3ff'), showlegend=F) %>% 
  theme_plotly()
```

<br>

If you're dealing with interesting data and modeling, there is little chance there is an easily identified 'effect size'.  Instead, you'll need to know your measures well and think hard about what you're seeing.  If you know the target variable well, effects in terms of predicted values under a variety of settings will be far more enlightening.



### The Standard Linear Model

Everything starts with the <span class="emph">standard linear regression model</span>.  To paraphrase @shalizi2017advanced, it's relatively simple[^simplerelative], it can do a decent enough job in a lot of situations, and it's a standard to the point it's essentially a tool of communication.  One of the more important aspects of it is that it serves as the foundation for everything else.

Simply put, you need to get to a point where practically nothing about what you get from it in standard settings should be a mystery to you.   Coefficients, standard errors, residuals, fitted values, prediction, how categorical predictor variables are used, residual standard error, interpreting interactions, model comparison.  It's also good to know what sorts of things are important (how you specify the model) and not so much (e.g. normality assumption).

If you don't understand standard regression, you'll be hard pressed to understand little else methodologically.  One additional benefit is that knowing it will take care of the special cases like ANOVA and t-tests, so you won't have to spend much time with those unless you just want to.

### Classification

The second most commonly used model after the standard linear model is probably <span class="emph">logistic regression</span>, where one predicts a binary target (e.g. yes-no).  Standard logistic regression, like the standard linear model, is a special case of the <span class="emph">generalized linear model</span>.  

This gets you beyond <span class="emph">ordinary least squares methods</span> and into <span class="emph">maximum likelihood</span>. We'll return to this later.  In addition to standard fare like coefficients and their standard errors, you'll get two types of predictions, probability of being in one of the categories, or the actual classification.  One can look at many metrics of performance with logistic regression, for example, accuracy, sensitivity, and specificity. Most only look at accuracy which is not a good way to do things. Actually, many don't even do that. You'll often see papers going on and on about the statistical significance of predictors even though their actual prediction is no better than guessing the most common category.


Even if you don't think you'll use it for your current work you'll come across papers that use it, and you will use it at some point if you do enough statistical modeling.  Furthermore, getting used to logistic regression can serve as a first step to other GLM models (e.g. <span class="emph">poisson regression</span>), and other categorical models, e.g. ordinal and multinomial.  If you do use it, interpret it in terms of actual predictions at key values of the predictors[^margins] rather than statistical significance and odds ratios.  Not only are predicted probabilities easy to interpret, it's pretty much the only way to interpret interactions and other nonlinear effects.

### Unsupervised Methods

Along with basic regression and logistic regression, you'll need some conceptual understanding of what are typically referred to as <span class="emph">unsupervised methods</span>.  Here, we are exploring structure more than predicting outcomes (though we could).  Probably the most common among these are PCA, factor analysis, and k-means cluster analysis.  I would suggest just getting a handle on why you'd use them, and be prepared for needing to implement them or similar variants for your work, because they are used a lot. Some of these are taught in classes on <span class="emph">multivariate analysis</span>, which is an utterly useless name, but those classes often include things like MANOVA, LDA, canonical correlation and others that have no place in modern methodology. However, I can say I do like name dropping canonical correlation, which MANOVA, LDA, and standard regression are special cases of, every once in a while just to befuddle people.


- regression and classification (check)
- explanation vs. prediction (check)
- unsupervised methods (check)
- loss function
- maximum likelihood
- problems with NHST
- overfitting
- generalization
- bayesian basics
- ML basics
- visualization heavy

- what is needed further for your specific projects
    - learn more about whatever technique your using


## Programming

One of the first things to note is, that once things are set up, it is as easy to run a model in R or python. It even requires less syntax than you would find with, e.g. SPSS and SAS.



### R & Python

When it comes to statistical programming and computation, there are two primary choices these days, but you should be flexible, because things will likely change at some point in your career.  



#### Alternatives

##### Other Programming languages

Lower level, Julia, 

##### Statistical or similar 

### IDEs



#### customize it


### coding style

- errors as calls for help

- I/O

- basic object types
    - boolean
    - numeric of a kind
    - factor/categorical

- data structures
    - vectors
    - matrices/arrays
    - data frames
  
- indexing

- iterative programming
    - loops/while

- aggregation
  


## Reproducibility revisited

### Data 

(SHOULD THIS BE IN NEXT SECTION?)

Before we get to analysis, one needs to understand that 

> *everything begins and ends with the data*.  

If you don't collect the data carefully, it doesn't matter what else you do. You've completely wasted your own time and resources, along with that of everyone else who is affiliated with the project.  If you're not willing to get your hands dirty with the data and make sure it's collected properly, you simply shouldn't be doing research.

Naming, labels, missing data etc.

### Analysis

For yourself, you should be able to load model-ready data, run the analysis on it, and reproduce the same result you intend to publish.  In addition, you might apply the idea of <span class="emph">convergent validity</span> to the process. Similar analytical techniques should come to the same conclusions.  And finally, give someone an outline of the necessary steps to take, and see if they can reproduce the same results.

### Programming

A starting point in your programming is to comment everything.  The comments should not be about what the code is doing. This is because if you know the language at all you'll know what it's doing, or if you forget, it will be easy to look up what the function or command.  Instead, comment about *why* you're doing it.

- Naming things
    - files
    - variables
    - data
    - objects
    
    "There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors."

## Other



### Other



  
- Documents
    - Latex or markdown

- Version control basics


[^deeplearnworks]: In other words, a ridiculous amount of data, with 'clean' data, and a whole host of other things in place.

[^phack]: I'm not even going into the fact that they probably finagled the data 20 different ways with umpteen models to get that p-value slightly less than .05, a practice called <span class="emph">p-hacking</span>.  People who do this engage in the worst type of anti-science.

[^bob]: Should you need assistance with knowing whether you have leveled up or not, there is someone in the IT department at LSU named Robert Gill III Esq. that can help you.  You cannot provide specifics, only ask, 'Have I leveled?'. He will then respond with a series of questions that will, in one way or another, probe the essence of your being, and provide the only answer you will ever need. Sprinkled with quotes from Buckaroo Bonzai, Goonies, and for matters of import, Forrest Gump.


[^simplerelative]: If you're just starting out, it probably isn't to you, but we're talking *relatively*. It'll be old hat to you as well at some point.

[^margins]: Sometimes called marginal effects.

