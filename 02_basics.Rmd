# Part II: Perfect Sound Forever

The goal here is not expertise, but primarily conceptual awareness and understanding.  For example, you don't have to know much about machine learning approaches other than why you might use them.  You can then learn more as needed when the time comes, if it even does.  For programming, it literally could be two lines of code to import your data and run a statistical model. But the data is *never* ready, and data processing will likely be where you spend the vast majority of your time.  However, knowing even a few things about basic programming will help you spend a tremendous amount less regarding what you *have* to do, but probably more of things you *want* to do.

Some motivation.  Here are some things I've seen:

- A social science grad student first learning R/programming at the beginning of the semester, that within a few months was a keyboard shortcut whiz, and was taking some Python workshops to boot.

- Another social science student using efficient programming practices to analyze big data on a high performance computing cluster.

- Architects, English Lit faculty, Staff librarians.  I have seen them all do basic programming and statistics to even some advanced stuff.

- I have seen 50+ years old SPSS folks get into machine learning.

It obviously isn't easy, but anyone can do it.  You'll likely be surprised at just what eventually does becomes easy, but be prepared for regular stumbles along the way, no matter what level you're at. As you improve, you'll just be at another level where the same amount of stuff will trip you up, only more advanced.  Case in point, here is the most common phrase I utter when programming:

<span class="" style="font-size:125%; font-variant:small-caps; color:#B2001D">~ You've got to be @%*^!ing kidding me!! </span>

Once you get good at some things, you'll still hit roadblocks, be surprised, or just make stupid mistakes.  It's just part of the journey, and mostly just means you're doing something interesting. If it helps, think of it as an RPG where you're leveling up, only, you'll probably be the only one to tell you that you've leveled up[^bob].


 
## Analytical

### Explanation and Prediction: a Starting Point

<h6> *A narrative of startling interest!!* </h6>

One way to look at the analytical endeavor is in terms of two goals, not mutually exclusive.  On the one hand we are interested in theory confirmation and comparison. Theory suggests certain variables have relations to other variables.  We set up mathematical models that convey the theory in a testable form, and collect data corresponding to those variables. Via a a particular framework, e.g. null hypothesis testing or Bayesian, we can say something about the statistical notability of those relationships.  The ultimate goal is to tell a story about the results and their relation to the theory, possibly suggesting exceptions and modifications.  In this approach, <span class="emph">explanation</span> is the focus.

On the other side, we need something that is practically viable.  The goal is not so much to tell a story as it is to get the best possible <span class="emph">prediction</span> from our models.  Techniques may be used that will tell us little about which predictors are best, why some predictors work better, or what is even the primary feature of the data being learned.  However, not only does the result work well, such approaches almost always outperform the far more explanatory ones in practice. 

To make things a little more concrete, we can consider linear regression vs. deep learning.  With the former we can obtain highly interpretable coefficients, statistical significance, interval estimates, etc.  It is extremely easy to tell a story with the results. With deep learning on the other hand, it's difficult to tell what's being learned, or what features are being latched on to.  However, assuming it's applied appropriately[^deeplearnworks], it works extremely well!


Most applied disciplines seem to focus exclusively on explanation in their statistical education.  You might think this is not too big of an issue, but to see the consequences of this, one only has to look at the current 'replication crisis'[^repcrisis].  You've now had nearly a century of results interpreted solely in terms of statistical significance in the null hypothesis testing framework, with typically little to no focus on whether the damn model even works.  As a specific example, you can have statistically significant predictors in a linear regression and an adjusted R^2^, which would tell you how much variance in the outcome you can explain with the predictors, of *negative value*!  The following demonstrates this.  I construct completely random data, i.e. none of the $X$ has any relation to $y$, and use it in the model.

```{r badmod, echo=2:6, eval=-6}
set.seed(0)
N = 250
X = matrix(rnorm(N*10), ncol=10)
y = rnorm(N)

summary(lm(y ~ ., data.frame(X)))
# broom::tidy(mod) %>% mutate_at(vars(-term), round, digits=3) %>% DT::datatable(options=list(dom='t', pageLength=11), rownames=F)
pander::pander(summary(lm(y ~ ., data.frame(X))), justify='lrrrr', round=3)
```

<br>

One term has a p-value of `r round(summary(lm(y ~ ., data.frame(X)))$coefficients['X10', 'Pr(>|t|)'], 3)`, but we know this is meaningless. Yet you will find entire papers written about that one coefficient, with p-value slightly less than .05[^phack], while in terms of prediction the model literally does worse than guessing the mean would with new data. Quite simply, this sort of practice has got to stop.

The key idea is <span class="emph">balance</span>. For most applied disciplines explanation is not only required, but to be focused on.  This is fine. Some people do research where collecting one observation might take a few days of effort.  This too, is okay. However, complicated models with little data require something more than statistical significance- the results generally will have too much variability associated with them.  But for any modeling endeavor, predictive validity is also important.  You simply cannot focus on the statistical (significance) story while entirely ignoring the practical value of the model.

#### A Word about Effect Size

Some disciplines, e.g. psychology, try to get their adherents to focus on effect size.  Reviewers regularly request it, but also are regularly non-specific about the metric, or if they do mention it, make clear they have no idea what they're talking about.  For example, one of the more popular effect sizes is <span class="emph">Cohen's d</span>, which is a standardized difference in two group means (e.g. control vs. treatment).  It's applicable in cases where the primary goal is a single group comparison, e.g. via a t-test.  However, if you are doing a t-test as your primary model and haven't gone back in time, then I can assure you no one is going to be citing your work regardless of your effect size.  Here are some of the issues.

- Without a corresponding interval estimate the effect size is of little value.  That's right, the effect size has associated uncertainty with it just like everything else.
- As soon as you get into any complication, e.g. repeated measures, interactions, etc., there is little standard or possibly even a viable way to define it.
- Small data and large effects mean nothing other than that you need more data.  It is *definitely not* the case that the effect is 'more real' because you happened to see it with such a small sample. In fact, your first move after seeing a large effect in a small sample should probably be to go back to the data processing stage to make sure no errors were introduced.

Another common 'effect size' is the standardized regression coefficient.  You don't see this much outside of disciplines where SPSS is used a lot. However, even in the only setting where it's easily calculated, it still doesn't make comparable categorical effects vs. numeric ones. And as soon as you move beyond the standard linear model, which is where most models should be these days, it's pretty much not defined.  For example, what's the effect size here?

```{r effectgam, echo=FALSE}
library(mgcv)
data(mcycle, package="MASS")
mod = gam(accel ~ s(times), data=mcycle)

library(plotly)
mcycle %>% 
  modelr::add_predictions(mod) %>% 
  plot_ly(x=~times, y=~accel) %>% 
  add_markers(marker=list(color=palettes$orange$complementary[1]), showlegend=F) %>% 
  add_lines(x=~times, y=~pred, line=list(color=palettes$orange$complementary[2]), showlegend=F) %>% 
  theme_plotly()
```

<br>

If you're dealing with interesting data and modeling, there is little chance there is an easily identified 'effect size'.  Instead, you'll need to know your measures well and think hard about what you're seeing.  If you know the target variable well, effects in terms of predicted values under a variety of data settings will be far more enlightening.



### The Standard Linear Model

Everything starts with the <span class="emph">standard linear regression model</span>.  To paraphrase @shalizi2017advanced, it's relatively simple[^simplerelative], it can do a decent enough job in a lot of situations, and it's a standard to the point it's essentially a tool of communication.  One of the more important aspects of it is that it serves as the foundation for everything else.

Simply put, you need to get to a point where practically nothing about what you get from it in standard settings should be a mystery to you.   Coefficients, standard errors, residuals, fitted values, prediction, how categorical predictor variables are used, residual standard error, interpreting interactions, model comparison.  It's also good to know what sorts of things are important (how you specify the model), and not so much (e.g. normality assumption).

If you don't understand standard regression, you'll be hard pressed to understand little else methodologically.  One additional benefit is that knowing it will take care of the special cases like ANOVA and t-tests, so you won't have to spend time learning those unless you just want to.

### Classification

The second most commonly used model after the standard linear model is probably <span class="emph">logistic regression</span>, where one predicts a binary target (e.g. yes-no).  Standard logistic regression, like the standard linear model, is a special case of the <span class="emph">generalized linear model</span>.  

This gets you beyond <span class="emph">ordinary least squares methods</span> and into <span class="emph">maximum likelihood</span>. We'll return to this later.  In addition to standard fare like coefficients and their standard errors, you'll get two types of predictions, probability of being in one of the categories, or the actual classification.  One can look at many metrics of performance with logistic regression, for example, accuracy, sensitivity, and specificity. Most only look at accuracy which is not a good way to do things. Actually, many don't even do that. You'll often see papers going on and on about the statistical significance of predictors even though their actual prediction is no better than guessing the most common category.


Even if you don't think you'll use it for your current work you'll come across papers that use it, and you will use it at some point if you do enough statistical modeling.  Furthermore, getting used to logistic regression can serve as a first step to other GLM models (e.g. <span class="emph">poisson regression</span>), and other categorical models, e.g. ordinal and multinomial.  If you do use it, consider interpreting it in terms of actual predictions at key values of the predictors[^margins] rather than statistical significance and odds ratios.  Not only are predicted probabilities easy to interpret, it's pretty much the only way to interpret interactions and other nonlinear effects that might be included in the model.

### Unsupervised Methods

Along with basic regression and logistic regression, you'll need some conceptual understanding of what are typically referred to as <span class="emph">unsupervised methods</span>.  Here, we are exploring structure more than predicting outcomes (though we could do that too).  Probably the most common among these are PCA, factor analysis, and k-means cluster analysis.  I would suggest just getting a handle on why you'd use them, and be prepared for needing to implement them or similar variants for your work, because they are used a lot. Some of these are taught in classes on <span class="emph">multivariate analysis</span>, which is an utterly useless name, but those classes often include things like MANOVA, standard linear discriminant analysis, canonical correlation and others that have no place in modern methodology. However, I can say I do like name dropping canonical correlation, which MANOVA, LDA, and standard regression are special cases of, every once in a while just to befuddle people.


### Loss functions and maximum likelihood estimation

In statistical modeling we are interested in estimated parameters. For example, these might be regression coefficients or variances. But how do we do it?  We certainly don't just guess, and we definitely don't use magic, so where do the numbers come from?

The answer is that we pick values that bring about the best result.  But how do we define *best*? Well there is no one way to do it, and how you might do it depends on the situation.  However, there are a couple common approaches you should understand conceptually very well.

Let's begin with ordinary least squares. If this is our optimizing approach, then we are trying to minimize the squared residuals in prediction.  For classification, it could simply be that we want to minimize *mis*-classification errors.

Probably the most common method goes the other way.  Instead of minimizing errors in prediction, we seek parameters that maximize the likelihood of seeing the observed data.  This method, <span class="emph">maximum likelihood estimation</span>, is the most common way we estimate parameters in statistical modeling, and has equal importance in the Bayesian framework too.  Sometimes the estimates that might minimize the loss function may also be the maximum likelihood estimate. This is the case in standard linear regression, but it would not be in other settings.  The primary point is, knowing the <span class="emph">objective function</span> for your analysis will help it not seem so mysterious. There are many optimization algorithms, and while you don't need to know the details of them, do know that you may get errors because their default settings don't work for your analytical situation.

Many applied statistical courses will spend time almost exclusively on ordinary least squares techniques, for which they might not even talk about estimation at all, leaving the students to basically believe in magic when they do statistical analysis. Some will even report that they used maximum likelihood, but this is only because they know that is the default.  For your own sake, don't bluff this.

With R or Python, you can literally write your own regression function via least squares or maximum likelihood in one or two lines of code. I strongly suggest you do this, even if only once, just to drive the point home.  I can also say it is quite satisfiying when something you put together reproduces the output you expect from the statistical program.


### Problems with NHST

I'm sick and tired of talking about the issues with <span class="emph">null hypothesis testing</span>, and I think the statistician types in general are.  Its problems have been noted since it was first proposed, and people in both methodological and applied disciplines have been raising the issues for over almost a century at this point.  It's annoying for me to even think about it, so if you want details, I won't be rehashing them here, but you won't have to look far on the web to find them.

Assuming you're even using the approach correctly (almost never the case in applied research), just about every other statistic that falls out of an analysis is more important than the p-value. Yet I have seen people torture data, change their theories, ignore important findings, etc., because of it.  Researchers still doing this are not doing good work, because almost by definition it is work that likely won't stand the test of time, probably even the second time.  They are wasting their own and others time in general.


*Some Difficulties of Interpretation Encountered in the Application of the Chi-Square Test*. J Berkson **1938**

"The statistical folkways of a more primitive past continue to dominate the local scene." Rozeboom **1960**[^bestquote]

"As a second example, consider significance tests. They are also widely overused and misused." Cox **1977**

"Are the effects of A and B different? They are always different, for some decimal place." Tukey **1991**

That first one is an article title, but otherwise those are some very old quotes, indicating just how long this has been a problem.  NHST as a *scientific* paradigm, in which evidence is weighed to assess a theory's real-world effectiveness, for a variety of reasons, doesn't work in practice.  It is difficult to say whether it ever did. Many disciplines are possibly decades behind in their science both because of missed opportunities, and because of trudging through weak results that didn't hold up, but were adhered to merely because of statistical significance.

Statistical analysis will not provide you a hard answer or truth, and claiming something is 'significant' doesn't make it so.  The goal in statistical approaches to science is not to say whether a predictor is important or not, but rather whether the data is consistent with a given theory in general, and whether viable predictions can be made with a given theory. Go in with competing models, e.g. simpler ones vs. more complex, or theory-driven vs. purely exploratory.  Spend time expressing your data visually.  If you do you'll regularly have something interesting to talk about regardless of any p-value.

### Under and Overfitting

> With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.

~ John Von Neumann 


#### Overfitting

A common problem in standard statistical modeling and machine learning includes <span class="emph">overfitting</span>. Overfitting may be described in a couple ways, but the key idea is one of model comparison.  For example, usually we want to compare simpler models vs. more complex models. While more complex models may better mimic what we see in nature, simpler models may perform nearly as well, allow for simpler interpretation, provide computational benefits etc.  Ideally we want a parsimonious model, one that is complex, but not overly so.  A more complex model can actually make no better or even worse predictions than a simpler model. In this case we are overfitting the data.

Take for example the following plot of data.

```{r overfitting0, echo=FALSE}
adj_rsq = function(rsq, p, n) {rsq - (1-rsq)*(p/(n-p-1))}
rmse = function(res){
  sqrt(crossprod(res)/length(residuals))
}
mae = function(res){
  mean(abs(res))
}
set.seed(1234)
library(mgcv)
d = gamSim(1, n=500, verbose=F)
idx = sample(1:500, 50)
d_50 = d[idx,]
d_450 = d[-idx, c('x2','y')]

d_50 %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange)) %>% 
  theme_plotly()
```

A sufficiently complex model might obtain the following fit.

```{r overfitting1, echo=FALSE}
library(modelr)

model_justright = gam(y ~ s(x2, bs='gp'), data=d_50)
model_overfit = loess(y ~ x2, span=.15, data=d_50, surface='direct')
model_underfit = lm(y ~ poly(x2, 2), data=d_50)

d_50 %>% 
  add_predictions(model_overfit, var='fit') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), showlegend=F) %>% 
  add_lines(y=~fit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  theme_plotly()
```

Great fit right? The R^2^ for the model is `r round(cor(model_overfit$fitted, model_overfit$y)^2, 2)` with mean absolute error (MAE) of `r round(mae(model_overfit$residuals), 1)`. However, if we predict it on new data from the same underlying data generation process, the MAE is `r round(mae(predict(model_overfit, newdata=d_450)- d_450$y), 1)`, which is notably worse!

```{r overfitting2, echo=FALSE}
d_450 %>% 
  add_predictions(model_overfit, var='overfit') %>% 
  add_predictions(model_underfit, var='underfit') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), showlegend=F, opacity=.5) %>% 
  add_lines(y=~overfit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  add_lines(y=~underfit, color=I(palettes$orange$tetradic[3]), showlegend=F) %>%
  theme_plotly()

```

Conversely, a simple regression with a quadratic term has comparable fit, with even a lower MAE (`r round(mae(predict(model_underfit, newdata=d_450)- d_450$y), 1)`).  For reasons we're about to see, I would not recommend the standard regression fit either, but the point of overfitting is illustrated.


Another way to think about overfitting is in terms of the number of parameters you must estimate with the model vs. the number of data points you have to estimate them.  Some machine learning approaches may have thousands of parameters for example, so you better have a lot of data for them to work well.  A comon problem I see is with structural equation modeling, where people routinely fit complex models with several dozen to more than a hundred parameters with very little data. Because they also don't compare models to simpler ones, or even typically discuss prediction *at all*, they don't even speak to the notion of overfitting.  Sometimes, when their model doesn't fit as well as they'd like, they'll even use purely data driven approaches to add even more parameters!

For more on overfitting, see [this discussion](http://andrewgelman.com/2017/07/15/what-is-overfitting-exactly/) at Andrew Gelman's blog.


#### Underfitting

A converse problem is perhaps just as ubiquitous or even more prevalent in some fields, and that is not incorporating sufficiently complex models, or <span class="emph">underfitting</span>.  Take for example, the fact that standard linear regression, with no interactions or nonlinear effects, is still the most commonly employed model.  It seems suspect that a straight line would be 'good enough' to explain most relationships in nature.  In many cases the simplification may be worth it, but we are well beyond the time where that should be a so widely used as a default.

Consider an generalized additive model (GAM) fit to the previous data. It initially won't fit as well as the overparameterized model. But it certainly does better than a straight line would.


```{r underfitting0, echo=FALSE}
d_50 %>% 
  add_predictions(model_overfit, var='overfit') %>% 
  add_predictions(lm(y~x2, d_50), var='underfit') %>% 
  add_predictions(model_justright, var='justright') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), opacity=.5, showlegend=F) %>% 
  add_lines(y=~overfit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  add_lines(y=~underfit, color=I(palettes$orange$tetradic[3]), showlegend=F) %>%
  add_lines(y=~justright, color=I(palettes$orange$tetradic[4]), showlegend=F, line=list(width=3)) %>%
  theme_plotly()
```


When it comes to new data, it performs the best (MAE = `r round(mae(predict(model_justright, newdata=d_450)- d_450$y), 1)`).

```{r underfitting1, echo=FALSE}
d_450 %>% 
  add_predictions(model_overfit, var='overfit') %>% 
  add_predictions(model_underfit, var='underfit') %>% 
  add_predictions(model_justright, var='justright') %>% 
  plot_ly(x=~x2, y=~y) %>% 
  add_markers(color=I(palettes$orange$orange), opacity=.5, showlegend=F) %>% 
  add_lines(y=~overfit, color=I(palettes$orange$complementary[2]), showlegend=F) %>%
  add_lines(y=~underfit, color=I(palettes$orange$tetradic[3]), showlegend=F) %>%
  add_lines(y=~justright, color=I(palettes$orange$tetradic[4]), showlegend=F, line=list(width=3)) %>%
  theme_plotly()

```

My own opinion is that a generalized additive model should be the default model rather than the standard regression model. It penalizes complexity so that if a linear fit is better, that will likely be the result. In addition interactions, spatial effects, random effects and myriad approaches are avaible to add to it.

The issues around over- and underfitting can be somewhat subtle.  In general I think underfitting is more the problem in the stuff I see, and with penalized regression approaches, one can incorporate more complex models while still guarding against overfitting.  Interpretation becomes more difficult, but also more rich, and more fun in my opinion.
 


### The Bayes Way


### Machine Learning Basics

Any applied discipline that teaches a semester course in statistical analysis should have a week of it devoted to an overview of machine learning.  The only reason you may not see it practiced much in your discipline is due to 1. your discipline never teaches it, or 2. people outside of your discipline are doing the interesting things for your discipline, and publishing elsewhere.  I can tell you however, that it is useful, you may need to employ it some day, or at the very least, you shouldn't ignore articles using it because you don't know what they're talking about conceptually.

Data can be big, complex, or both, and there are techniques to handle it if so.  If you don't know about them, you may be inclined to do some very silly things to coerce the situation to one you know, which is precisely the wrong thing to do.  If your discipline doesn't cover it in any fashion, do a workshop, look online for some exercises, or simply schedule some time to chat with someone who does know about them.  With tools like R and Python, the difficulty will *not* be running the models.


### Visualization

### Variable transformations

There are some good reasons to transform a variable. For example, mean centering (subtracting the mean), or additionally scaling numerical variables both makes a value of 0 practically meaningful (i.e. it is the mean), and can go a long way toward making the estimation process easier for many algorithms.  Some techniques are not even useful unless the variables are similarly scaled.  There are also times, especially in the case of categorical outcomes and predictors, when the need arises to collapse categories which have very few observations.  Otherwise they can lead a model to fail.  Finally, some, especially econometricians, prefer to speak in terms of <span class="emph">elasticities</span>, and so will take the log of a numeric variable to do so.  That's pretty much it as far as the reasons to transform a variable, easier estimation or theoretical interpretiability.

Here are couple things not to do.  If you have a numeric variable, there simply is no justification for discretizing it.  You are just making it a less reliable and expressive measure, and there is no benefit to that.  In addition, transforming a variable 'due to normality' is translated as 'I don't actually know what the normality assumption is'.  Case in point, the following is a perfectly reasonable target variable that would meet the normality assumption for regression.

```{r normality, echo=FALSE}
x = sample(0:1, 500, replace=T)
y = 5*x + rnorm(500)
ydens = density(y)

data_frame(y=ydens$x, dens=ydens$y) %>% 
  arrange(y) %>% 
  plot_ly(x=~y, y=~dens) %>% 
  add_lines(color=I(palettes$orange$complementary[2])) %>% 
  theme_plotly()
```

Why? Because the assumption regards the *residuals*, not the observed target variable. The above is just what you'd expect if you were dealing with a very strong group effect. The following shows the density plot of the residuals and a test of normality (non-significant means okay).

```{r residuals, echo=FALSE}
ydensres = density(residuals(lm(y~x)))

data_frame(y=ydensres$x, dens=ydensres$y) %>% 
  arrange(y) %>% 
  plot_ly(x=~y, y=~dens) %>% 
  add_lines(color=I(palettes$orange$complementary[2])) %>% 
  theme_plotly()
# shapiro.test(residuals(lm(y~x))) %>% broom::tidy()

```

```{r normality2, echo=FALSE}
# this completely f/d up the whole document by including it with the previous plotly chunk
shapiro.test(residuals(lm(y~x))) %>% pander::pander()
```


I've seen this assumption so confused that some think it applies to the predictors as well!  Some have also tried to apply it to ordinal variables. A moment's reflection will prove this will not meet with success.  Some also apply it to data, such as counts, where we actually expect it to be skewed.  And I don't know anyone that knows how to interpret a variable that has undergone an square root arcsine transformation.

If you're model is not capturing tails or other aspects of the observed target, it's likely because the normal distribution is not a viable candidate for the underlying data generating process, or the model simply isn't capturing a key aspect of the data (e.g. inherent clustering).  It makes more sense to change your assumption about the distribution (e.g. use a t or beta distribution if appropriate), or fixing your model (e.g. using a mixed model if appropriate), than trying to torture your data in a misguided attempt to speak to one of the less important assumptions in the standard linear model.  You'll also find that the heteroscedasticity problems magically go away as well. 

### Learn more

- regression and classification (check)
- explanation vs. prediction (check)
- unsupervised methods (check)
- loss function (check)
- maximum likelihood (check)
- problems with NHST (check)
- overfitting v.s - generalization
- bayesian basics
- ML basics (check)
- visualization heavy
- variable transformation (check)

- what is needed further for your specific projects
    - learn more about whatever technique your using


## Programming

If you're doing serious scientific research, these days you're going to have to learn some programming[^notstartrek].  Don't want to? That's okay, just do something else besides scientific research.  Even if you're doing relatively simple analysis, a lot of data processing will have to take place beforehand to ensure that whatever results you obtain have integrity.  Unless you like to spend several weeks on something that could take a couple hours with even just basic programming skills, you'll need to spend some time in this area to develop said skills.


One of the first things to note with statistical programming is, that once things are set up in terms of the data, it is as easy to run a model in R or Python as it is menu-driven programs like SPSS. It even requires *less* syntax than you would find with, e.g. SPSS and SAS.  Compare the following two approaches, one a menu-driven approach in SPSS and another programming approach in R.


- SPSS

```
Click File
Open
hunt for file on computer
wait a few seconds for SPSS to open its own file type
Click Analyze
Click/hover Regression
Click Linear Regression
Search for and Click your dependent variable (or click, start typing the name, and hope)
Search for and Click each 'independent' variable
Click on various options.
Click ok
```

- R

```{r lm_vs_spss, eval=F}
d = haven::read_spss('filelocation/file.spss')
model = lm(DV ~ x1 + x2 + x3, data=d)
```



There is no debate here, even givng SPSS the head start of having the data in its own format, one approach is vastly more efficient, and simply easier.  In fact, with RStudio's autocompletion, the odds are that the first line probably only required a few keystrokes. You may have to learn that you can use the <span class="pack">haven</span> package for importing SPSS files, or the <span class="func">lm</span> function to do regression, but that's very quickly picked up, and less than you would have had to learn regarding the SPSS menus, which have remained mostly unchanged and in their disorganized state for over 20 years.  The other nice thing is that you can redo the above in two steps with Ctrl+A (select all) and Ctrl+Enter (run)[^pyspss].




### R & Python

<div class='col2'>
<img src="img/Rlogo.svg" style="display:block; margin: 0 auto;" width='50%'>
<br>
<img src="img/py.svg" style="display:block; margin: 0 auto;" width='75%'>
</div>
<br>
When it comes to statistical programming and computation, there are two primary choices these days, but you should be flexible, because the situation will likely change at some point in your career.  <span style="color:#1f65b7">**R**</span> and <span style="color:#FFDC52">**Python**</span> are the primary tools of statistics/data science, and have pretty much overtaken both enterprise and academia.  They are both easier to program for data processing and analysis than they were even just a few years ago, and notably more so than they were in the early days of their use for modeling.  It doesn't really matter which you use.  Python will have more power when it comes to machine learning, potential enterprise applications, natural language processing, and, in general, speed.  R is still notably easier to use for applied interactive analysis, and has far more developed packages for a far wider range of techniques.  However, these distinctions melt by the wayside more and more everyday. So you're fine with either.



#### Alternatives

##### Other Programming languages and related

Lower level programming languages such as Java, Fortran, and C++ are not required knowledge for the vast majority of applied statistical and related application.  They are primarily used when you need to get the most speed you can, and you'll be able to find someone on campus to help you there, for example, in your research support group.

Matlab, Julia, and Mathematica are other tools or languages you might come across that are highly capable, but are either proprietary (i.e. require a renewable license, cost money, and don't share source code), or don't yet offer much beyond what you can get with Python and R, and usually far less.  Of these I'd recommend Julia as its use appears to be on the rise and is freely available.

You will also find languages and frameworks within languages such as Scala, Spark, Hadoop, Keras, Torch, TensorFlow, SQL, Lua and so forth. Most of these have specific purposes, e.g. databases, or don't need to come into play unless you have massive data (e.g. TensorFlow).  Don't bother unless needed, but don't be surprised if you do.

In addition, people are writing analytical functionality in things like javascript, Ruby, Haskell and other things that weren't meant for it nor are all that good at it. I don't know why.


##### Statistical or similar 

The primary statistical programs in academia are SAS, SPSS and Stata.  Unlike the others, Stata use actually appears to be growing, and they also have a great community. The other two are dwindling fast in academia, especially, and thankfully, SPSS.  Aside from those, there are relics, money-grabs, and WTFs of all kinds. Seriously, there is some ridiculous software out there that some poor folks in some specific niche of a discipline got suckered into using a long time ago, and for whatever reason still use it.  If you're using non-open source software whose functionality is completely matched and bettered by others that are freely available, you'd better be able to justify it in some way, and I can't think of anything you could come up with.

If you use something like SAS or Stata, you still can't use the menus for analysis.  Aside from being inefficient, it's not reproducible, and thus not a way to do science.

Excel is not an option for scientific work, ever.  I would say you could use it for data entry, but it's very problematic even then. Avoid at all costs.

If you don't want to take my word for it here are some other things to peruse.

- [Link 1](http://spectrum.ieee.org/computing/software/the-2017-top-programming-languages)
- [Link 2](https://www.tiobe.com/tiobe-index/)
- [Link 3](http://r4stats.com/articles/popularity/)
- [Link 4](http://www.kdnuggets.com/2017/05/poll-analytics-data-science-machine-learning-software-leaders.html)

### IDEs

After installing the basic components for R or Python, you can use them straight away. However, this would be silly, as they don't come with a way to use them that enables basic functionality or is easy.  If your R console comes with retina-bleeding red-colored font, or you're using the clown-colored IDLE from Python, just shut them down and walk slowly away.

You want to use what's called an <span class="emph">integreted development environment</span>, or IDE.  For R, the far and away most popular one is RStudio. For Python, I would suggest Anaconda/Spyder for scientific work.  There are others, but unless you have a lot more experience, I'd suggest sticking with those[^atom].

What these do, practically speaking, is make your coding efforts a lot more easy and efficient. Syntax highlighting, code completion, autospacing, interactivity... this is only the beginning.  I've written this document and many other things *entirely* with RStudio.  A good IDE is essential.


#### customize it

After you initially start to get the hang of your IDE, customize it. I'm always struck by the fact that people don't change the basic settings in their software, which are often chosen by people who should not have any say in design whatsoever[^baddesign].


### coding <span class="font-variant:small-caps; font-family:'Risque'">style</span>



<img src="img/code_quality.png" style="display:block; margin: 0 auto;" width=200%>
<br>
<img src="img/code_quality_2.png" style="display:block; margin: 0 auto;" width=50%>
<br>
<img src="img/code_quality_3.png" style="display:block; margin: 0 auto;" width=150%>
<br>


Coding style matters, but perhaps not in the way that many think.  


<span class="" style="size:125%; font-variant:small-caps">The only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code.<br> ~ Hadley Wickham</span>

Wickham is right on the first part, but I'm not so much in agreeement with the second.  No one writes perfect code, *no one*.  It takes effort just to write halfway decent code.  That's fine.  As you start out, you won't be writing good code for quite some time. That's okay too.

However, assuming you want others to be able to use your code, you should feel bad giving them shitty code, because you'll be wasting their time.  Going back to one of our initial suggestions, *Don't be a jerk*. Write better code, always. Time and other resources will always be working against you, but do the best you can.

The first step is to start with a coding style.  For example, both [Google](https://google.github.io/styleguide/Rguide.xml) and [Hadley](http://adv-r.had.co.nz/Style.html) have provided an R coding style for others to follow, and there is [PEP8](https://www.python.org/dev/peps/pep-0008/) for Python.  If you only followed those, your code will be great relative to what you would have done.  But realize they are only suggestions, some are based on decades of coding practice, others are based on whim, and they don't tell you which.

For example, the Google says using underscores for variable names, e.g. `this_is_a_variable`, is 'bad', while capital first camel casing is 'good', e.g. `ThisIsAVariable`.  However, it's very clear which is more readable, and just as easy to type. Furthermore, with modern IDEs, once the object is created, you only have to type a couple letters to complete the rest.  Both Wickham's and Google's style guide consistently contradict one another and themseleves, and yet they're both based on more experience than you'll ever have, so that should give you some sense that it isn't exactly easy to figure out what works best.

The point is, there is no authority here, but there are better approaches than your nospace, semi;coloned, naming-things-'data', everything-in-a-single-script attempts will come up with, I guarantee you.  In coding, just trying to do better will make your code better[^github].



- errors as calls for help

- I/O

- basic object types
    - boolean
    - numeric of a kind
    - factor/categorical

- data structures
    - vectors
    - matrices/arrays
    - data frames
  
- indexing

- iterative programming
    - loops/while

- aggregation
  


## Reproducibility revisited

### Data 

(SHOULD THIS BE IN NEXT SECTION?)

Before we get to analysis, one needs to understand that 

> *everything begins and ends with the data*.  

If you don't collect the data carefully, it doesn't matter what else you do, despite what the folks in the Engineering department tell you. You've completely wasted your own time and resources, along with that of everyone else who is affiliated with the project.  If you're not willing to get your hands dirty with the data and make sure it's collected properly, you simply shouldn't be doing research.

Naming, labels, missing data etc.

### Analysis

For yourself, you should be able to load model-ready data, run the analysis on it, and reproduce the same result you intend to publish.  In addition, you might apply the idea of <span class="emph">convergent validity</span> to the process. Similar analytical techniques should come to the same conclusions.  And finally, give someone an outline of the necessary steps to take, and see if they can reproduce the same results.

### Programming

A starting point in your programming is to comment everything.  The comments should not be about what the code is doing. This is because if you know the language at all you'll know what it's doing, or if you forget, it will be easy to look up what the function or command.  Instead, comment about *why* you're doing it.

- Naming things
    - files
    - variables
    - data
    - objects
    
    "There are only two hard things in Computer Science: cache invalidation, naming things, and off-by-one errors."

## Other



### Other



  
- Documents
    - Latex or markdown

- Version control basics


## Summary of Part II


[^deeplearnworks]: In other words, a ridiculous amount of data, with 'clean' data, and a whole host of other things in place.

[^phack]: I'm not even going into the fact that they probably finagled the data 20 different ways with umpteen models to get that p-value slightly less than .05, a practice called <span class="emph">p-hacking</span>.  People who do this engage in the worst type of anti-science.

[^bob]: Should you need assistance with knowing whether you have leveled up or not, there is someone in the IT department at LSU named Robert Gill III Esq. that can help you.  You cannot provide specifics, only ask, 'Have I leveled?'. He will then respond with a series of questions that will, in one way or another, probe the essence of your being, and provide the only answer you will ever need. Sprinkled with quotes from Buckaroo Bonzai, Goonies, and for matters of import, Forrest Gump.


[^simplerelative]: If you're just starting out, it probably isn't to you, but we're talking *relatively*. It'll be old hat to you as well at some point.

[^margins]: Sometimes called marginal effects.

[^notstartrek]: At least until the ideas of Star Trek become a reality.  And don't let the folks in the Bay Area fool you, it's still a very long way off.  For some perspective, at the turn of the 20^th^ century, there were no cars as we now know them other than demonstrations and prototypes. By World War I, less than 20 years later, cars were a common sight in cities, and airplanes were being used in combat. We've now had more than four decades of combined internet and personal computing capabilities, and probably the biggest difference between now and then is that people spend a lot of time staring at tiny screens in their hand.  Most of the stuff we're just now getting to was promised to be right around the corner during the space age of the 1950s and 60s.  The point is, a lot of this stuff will come, but don't hold your breath.

[^baddesign]: Just look at gmail, Facebook, Amazon.  They are essentially the anti-Tron of user esteem.  Amazon in particular seems to simply vomit out the return of a search that mildly references some of the letters you actually typed into its engine, if they used Google translate on the query first, while assuming the native language was an interpretive dance in Kabuki theater.  The 'art' of basic drilldown is a completely lost enterprise.  Using that website almost makes you want to ask Alexa for the best form of suicide.

[^github]: My main motivation for getting on GitHub was making my code better.  If the only reason you use it is that the thought of others looking at your code will make you write better code, then definitely do so.

[^bestquote]: You may try, but you will never, *ever* find a better quote in all of statistics.  Also, it's generally expected that when uttered, those in earshot must snap at least twice and say 'Right on, daddy-o'.

[^pyspss]: I was going to do a Python example too, but I couldn't find an example that wouldn't take 5 or more lines not counting the import lines, which would be several more.  

[^repcrisis]: It is *current* if you're not aware of statistical history of the past 60+ years.  The only thing that's different from past calls to do better work is people's ability to complain on social media.  Applied researchers will do no more methodologically than what is expected, which seems largely defined by whatever they come across in the few journals they spend time with.  This is not necessarily right or wrong, just the way it is.

[^atom]: I do like Atom for Julia and Python, but it's a general text editor, not a program specific IDE.