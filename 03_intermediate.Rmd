# Part III: One Last Caress

This section is for those who get the bug, and actually find they enjoy programming and analysis, or simply want to continue expanding their toolbox to do better analysis.  It'll outline some good things to be aware of, or skills to start obtaining.


## Analytical

### Beyond GLM 

After standard regression and classification techniques, there are some common extensions to consider.  However, I would suggest you first learn an advanced technique that is commonly used in your discipline.  Beyond that, some more common statistical models that take a next step in complexity are those that deal with clustered data (mixed models), nonlinear relationships (e.g. additive models), and those that deal with space or time (possibly both).  Be aware that the additional complexity brings with it a fuzziness, such that it becomes difficult to use standard null hypothesis testing.  This is not a drawback by any stretch, but it will be difficult for you if you've come to rely on p-values as your primary means of interpreting models.


### Dimension Reduction/Latent variable models

In the previous section we noted that one should at least be familiar with why you would use things like PCA and factor analysis.  For the more motivated, these should be standard tools in your toolbox.  I would specifically suggest PCA, standard factor analysis, latent dirichlet allocation, and model-based clustering methods (i.e. mixture models).

### Matrix basics

There are two reasons to know just a smidge of matrix operations- speeding up your code and understanding methodological presentations.  As far as the amount, you probably don't need any more than is commonly found in the appendices of some statistical texts, and being familiar with those operations in your programming language of choice.  With programming, while others are writing verbose loops, you'll be speedily one-lining the same operations in <span class="emph">vectorized</span> fashion.  Furthermore, when you're wanting to learn new techniques, such knowledge will allow for more rapid acquisition.

### The Bayes Way

Bayesian analysis is commonplace, and requires no more justification than standard techniques. If you don't see it regularly applied in your field, it is your discipline that hasn't been keeping up with things. *You*, on the other hand, should feel comfortable using it in lieu of any other analysis you would, starting with regression.  A Bayesian regression requires no more effort to explain than a standard regression, *because it is just a standard regression*, albeit estimated differently.  

The benefits of the Bayesian approach are many.  To begin, you get probabilities and intervals that make sense, and don't require some notion of 'infinitely repeated experiments' and backward logic.  Any statistic you can calculate from a model will automatically have an interval estimate. For example, in a standard regression you'd not only get an 'adjusted' R^2^ by default[^bayesianr2], you'd also have an interval estimate for it.  The Bayesian approach works well in cases where you have small samples, possessing built-in <span class="emph">regularization</span> to guard against overfitting, allowing you to explore more fully your data even when you don't have much of it.  For more complicated models, the p-values and intervals are no more difficult to acquire than usual, unlike the traditional setting where it seems that for many models beyond the GLM setting approximations are required (e.g. mixed models, generalized additive models).

In general the Bayesian approach will give you more flexibility and better assessment of uncertainty, and can definitely be your default modeling approach. For an introduction to Bayesian analysis, see my document [here](https://m-clark.github.io/bayesian-basics).



### Machine Learning

In this day and age, any applied discipline that teaches a semester course in statistical analysis should have a week of it devoted to an overview of machine learning.  The only reason you may not see it practiced much in your discipline is due to 1. your discipline never teaches it, or 2. people outside of your discipline are doing the interesting things for your discipline, and publishing elsewhere.  I can tell you however, that it is useful, you may need to employ it some day, or at the very least, you shouldn't ignore articles using it because you don't know what they're talking about conceptually.

Data can be big, complex, or both, and there are techniques to handle it if so.  If you don't know about them, you may be inclined to do some very silly things to coerce the situation to one you know, which is precisely the wrong thing to do.  If your discipline doesn't cover it in any fashion, do a workshop, look online for some exercises, or simply schedule some time to chat with someone who does know about them.  With tools like R and Python, the difficulty will *not* be running the models.




## Programming

You'll always be able to go further in programming, if for no other reason than the fact that you likely weren't taught basic programming skills in the first place.  The more you know, the easier things will continue to come, the more data you can explore, the better visualizations you can produce, the speed at which you can do practically everything will increase, etc.

### Keyboard

There are things you can do.  And there are things you can do because you finished that other stuff an hour ago.  One of the main benefits of an IDE are all the keyboard shortcuts that can save you a lot of time compared to point and clicking.  Unfortunately, keyboards on many laptops don't have the type of keyboard those shortcuts assume, and some operatings systems seem to not be aware that keyboards can have over 100 keys.

### Writing functions

One of the first things you can do to take your programming further is to write your own functions.  One of the tenets of programming is <span class="emph">DRY</span>, or *don't repeat yourself*. Anytime you're copying what is essentially the same code violates that principle.  Writing your own functions can help you avoid doing so, and can be extremely simple.  For example, the following takes two variables, divides one by the other and multiplies by 100 (i.e. creates a percentage).

R

```{r eval=FALSE}
percent <- function(x, y) {
  100*x/y
}
```

Python

```{python eval=FALSE}
def percent(x, y):
  return 100*x/y
```


If you want, you can now add additional complexity to make it do more (e.g. add rounding, formatting).  Furthermore, you can reuse the function anywhere you like.

Once you get the hang of writing functions, you'll find yourself doing so regularly, because it simply makes things much easier than constantly repeating yourself.  In addition, you may find other functions available in packages do *almost* everything you want, and you'll be able to tweak their code to create a function that does *exactly* what you want.


### Debugging

<span class="emph">Debugging</span> is the process of finding errors or issues in code.  If you are writing your own functions, debugging will allow you to course through the function line by line, inspecting the output created, and fix any errors.  Even if it's not your own function, debugging can allow you to find out why some function is spitting an esoteric error message at you.

More generally, the *concept* of debugging is essential to scientific endeavor.  Data, models, and theories come with bugs too, and you should have a similar mindset when approaching them as you would problematic code, inspecting them piece by piece and improving them whenever the opportunity arises.

### Iterative Programming & Vectorization

One of the more popular blogs on Rbloggers for a long time was 'Writing your first <span class="emph">for loop</span>'.  The reason is that iterative programming is a core programming technique, particularly in data science.  It is so common that you'll want to apply some function to, for example, every row or column, that not knowing how to do so in an efficient fashion will severly hamper your coding practice.

Some programming languages have even more efficiency to be gained through <span class="emph">vectorization</span>, where operations are done on vectors rather than proceeding element by element.   For example, in R, between core functions being vectorized, matrix operations, the <span class="func">apply</span> family of functions, and packages like <span class="pack">purrr</span>, I almost never have to write an explicit loop, and code runs much faster.  With some languages you may find some that suggest vectorization will not produce much speed gain.  They are wrong, if only because it will take you less time to write the code.  Cleaner and clearer code is always desirable too.


### High Performance Computing

At some point your data or models may require more resources than your computer can provide.  Research universities provide cluster computing for such cases, but now anyone can use services such as those provided by Amazon or Microsoft as well.   Knowing the basics of <span class="emph">parallelization</span>, the cluster computing environment, and related tools will be necessary in such cases.  However, even your own computer comes with multiple cores that you can take advantage, so you should be able to use the tools of your language to do that.  *Any* iterative process in which the iterations are independent can potentially benefit from parallelization.

When speed becomes an issue, you'll want to also be familiar with code <span class="emph">profiling</span>.  Profiling allows you to assess where in your code bottlenecks occur, which you may then rework to produce faster results.  However, this comes with the following caveat: premature optimization is the root of all evil.  Code can be relatively slow but still good enough, and it's important to balance computing vs. programming time.


## Visualization & Presentation

In a [previous section][visualization], we talked about how a good visualization will take complex relationships and make them easier to understand, and in general, it will require some effort to pull off a good one.  Beyond that, you also want to think interactively.  Interactivity is key to visualization in this day and age, and should be a fundamental part of your visualization arsenal, especially since interactive forms of traditional plots are as easy to produce as the traditional ones. It probably won't be long before it's an expected part of scientific visualization, assuming the journals get out of the way.  The nice thing is, you can always start with an interactive plot, and take a snapshot for the publication, keeping the interactive plot as supplementary material. Adding interactivity can add even more dimensions to a single visualization.


For those that really want to go down the rabbit hole regarding visualization, there's always more to consider, and this document can perhaps serve as a starting point. The text is of a contrast ratio relative to the background that would satisfy some of the more strict [web accessibility standards](http://www.webaxe.org/color-contrast-tools/).  This includes the color coded text, though those are not quite as stringent. I use my own css files to implement these throughout the document.  In addition, the colors in the plots are evenly spaced and would be distinguishable for those with even rare forms of color-blindness.  You'll also notice there are no gridlines, which are completely unnecessary when you can obtain the specific values by hovering over the plot[^gridlines].

## Other

### Engaging the web

If you do enough data analysis you'll eventually need to acquire some data directly from the web, possibly in some raw form (e.g. text) that will require additional processing.  Whether this will be easy or difficult unfortunately will not be up to you, but rather to those developing the website(s) that house the data, and many actively make the data difficult to obtain[^webdev]. However, you can make it easier on yourself by knowing some basic HTML and other aspects of server-client communication, which will also help your web-based presentations.  Your browser even comes with tools to help you.  

Scraping the web will involve using tools in ways you never have, and thus often take a lot of effort. However, the payoff is often worth it, especially when you obtain a unique data set that no one previously had access to.

### Version control revisited
    - Git and related

### Publishing
    - Markdown, css, html etc.
    
### Collaboration
    - Notebooks
    - GitHub
    
### Sharing
  - Packages
  - Notebooks
  
  
[^gridlines]: Gridlines are unnecessary anyway in my opinion. First, can you really not tell roughly where the values are without them?  Second, they often convey a level of precision that is not inherent in the data.  Third, and most important, they're ugly.  This goes for their use in tables as well.

[^webdev]: By intention or incompetence.